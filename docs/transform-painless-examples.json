{
    "meta": {
        "timestamp": "2024-11-01T02:49:25.833066",
        "size": 24416,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/transform-painless-examples.html",
        "type": "documentation",
        "role": [
            "xpack"
        ],
        "has_code": true,
        "title": "transform-painless-examples",
        "version": "8.15"
    },
    "doc": "[role=\"xpack\"]\n[[transform-painless-examples]]\n= Painless examples for {transforms}\n++++\n<titleabbrev>Painless examples</titleabbrev>\n++++\n\n\nIMPORTANT: The examples that use the `scripted_metric` aggregation are not supported on {es} Serverless.\n\nThese examples demonstrate how to use Painless in {transforms}. You can learn \nmore about the Painless scripting language in the \n{painless}/painless-guide.html[Painless guide].\n\n* <<painless-top-hits>>\n* <<painless-time-features>>\n// * <<painless-group-by>>\n* <<painless-bucket-script>>\n* <<painless-count-http>>\n* <<painless-compare>>\n* <<painless-web-session>>\n\n[NOTE] \n--\n* While the context of the following examples is the {transform} use case, \nthe Painless scripts in the snippets below can be used in other {es} search \naggregations, too.\n* All the following examples use scripts, {transforms} cannot deduce mappings of \noutput fields when the fields are created by a script. {transforms-cap} don't \ncreate any mappings in the destination index for these fields, which means they \nget dynamically mapped. Create the destination index prior to starting the \n{transform} in case you want explicit mappings.\n--\n\n[[painless-top-hits]]\n== Getting top hits by using scripted metric aggregation\n\nThis snippet shows how to find the latest document, in other words the document \nwith the latest timestamp. From a technical perspective, it helps to achieve \nthe function of a <<search-aggregations-metrics-top-hits-aggregation>> by using \nscripted metric aggregation in a {transform}, which provides a metric output.\n\nIMPORTANT: This example uses a `scripted_metric` aggregation which is not supported on {es} Serverless.\n\n[source,js]\n--------------------------------------------------\n\"aggregations\": {\n  \"latest_doc\": { \n    \"scripted_metric\": {\n      \"init_script\": \"state.timestamp_latest = 0L; state.last_doc = ''\", <1>\n      \"map_script\": \"\"\" <2>\n        def current_date = doc['@timestamp'].getValue().toInstant().toEpochMilli(); \n        if (current_date > state.timestamp_latest) \n        {state.timestamp_latest = current_date;\n        state.last_doc = new HashMap(params['_source']);}\n      \"\"\",\n      \"combine_script\": \"return state\", <3>\n      \"reduce_script\": \"\"\" <4>\n        def last_doc = '';\n        def timestamp_latest = 0L;\n        for (s in states) {if (s.timestamp_latest > (timestamp_latest))\n        {timestamp_latest = s.timestamp_latest; last_doc = s.last_doc;}} \n        return last_doc\n      \"\"\"\n    }\n  }\n}\n--------------------------------------------------\n// NOTCONSOLE\n\n<1> The `init_script` creates a long type `timestamp_latest` and a string type \n`last_doc` in the `state` object.\n<2> The `map_script` defines `current_date` based on the timestamp of the \ndocument, then compares `current_date` with `state.timestamp_latest`, finally \nreturns `state.last_doc` from the shard. By using `new HashMap(...)` you copy \nthe source document, this is important whenever you want to pass the full source \nobject from one phase to the next.\n<3> The `combine_script` returns `state` from each shard.\n<4> The `reduce_script` iterates through the value of `s.timestamp_latest` \nreturned by each shard and returns the document with the latest timestamp \n(`last_doc`). In the response, the top hit (in other words, the `latest_doc`) is \nnested below the `latest_doc` field.\n\nCheck the <<scripted-metric-aggregation-scope,scope of scripts>> for detailed \nexplanation on the respective scripts.\n\nYou can retrieve the last value in a similar way: \n\n[source,js]\n--------------------------------------------------\n\"aggregations\": {\n  \"latest_value\": {\n    \"scripted_metric\": {\n      \"init_script\": \"state.timestamp_latest = 0L; state.last_value = ''\",\n      \"map_script\": \"\"\"\n        def current_date = doc['@timestamp'].getValue().toInstant().toEpochMilli(); \n        if (current_date > state.timestamp_latest) \n        {state.timestamp_latest = current_date;\n        state.last_value = params['_source']['value'];}\n      \"\"\",\n      \"combine_script\": \"return state\",\n      \"reduce_script\": \"\"\"\n        def last_value = '';\n        def timestamp_latest = 0L; \n        for (s in states) {if (s.timestamp_latest > (timestamp_latest)) \n        {timestamp_latest = s.timestamp_latest; last_value = s.last_value;}} \n        return last_value\n      \"\"\"\n    }\n  }\n}\n--------------------------------------------------\n// NOTCONSOLE\n\n\n[discrete]\n[[top-hits-stored-scripts]]\n=== Getting top hits by using stored scripts\n\nYou can also use the power of \n{ref}/create-stored-script-api.html[stored scripts] to get the latest value. \nStored scripts reduce compilation time,  make searches faster, and are \nupdatable. \n\n1. Create the stored scripts:\n+\n--\n[source,js]\n--------------------------------------------------\nPOST _scripts/last-value-map-init\n{\n  \"script\": {\n    \"lang\": \"painless\",\n    \"source\": \"\"\"\n        state.timestamp_latest = 0L; state.last_value = ''\n    \"\"\"\n  }\n}\n\nPOST _scripts/last-value-map\n{\n  \"script\": {\n    \"lang\": \"painless\",\n    \"source\": \"\"\"\n      def current_date = doc['@timestamp'].getValue().toInstant().toEpochMilli();\n        if (current_date > state.timestamp_latest)\n        {state.timestamp_latest = current_date;\n        state.last_value = doc[params['key']].value;}\n    \"\"\"\n  }\n}\n\nPOST _scripts/last-value-combine\n{\n  \"script\": {\n    \"lang\": \"painless\",\n    \"source\": \"\"\"\n        return state\n    \"\"\"\n  }\n}\n\nPOST _scripts/last-value-reduce\n{\n  \"script\": {\n    \"lang\": \"painless\",\n    \"source\": \"\"\"\n        def last_value = '';\n        def timestamp_latest = 0L;\n        for (s in states) {if (s.timestamp_latest > (timestamp_latest))\n        {timestamp_latest = s.timestamp_latest; last_value = s.last_value;}}\n        return last_value\n    \"\"\"\n  }\n}\n--------------------------------------------------\n// NOTCONSOLE\n--\n\n2. Use the stored scripts in a scripted metric aggregation.\n+\n--\n[source,js]\n--------------------------------------------------\n\"aggregations\":{\n   \"latest_value\":{\n      \"scripted_metric\":{\n         \"init_script\":{\n            \"id\":\"last-value-map-init\"\n         },\n         \"map_script\":{\n            \"id\":\"last-value-map\",\n            \"params\":{\n               \"key\":\"field_with_last_value\" <1>\n            }\n         },\n         \"combine_script\":{\n            \"id\":\"last-value-combine\"\n         },\n         \"reduce_script\":{\n            \"id\":\"last-value-reduce\"\n         }\n--------------------------------------------------\n// NOTCONSOLE\n<1> The parameter `field_with_last_value` can be set any field that you want the \nlatest value for.\n--\n\n\n[[painless-time-features]]\n== Getting time features by using aggregations\n\nThis snippet shows how to extract time based features by using Painless in a \n{transform}. The snippet uses an index where `@timestamp` is defined as a `date` \ntype field.\n\n[source,js]\n--------------------------------------------------\n\"aggregations\": {\n  \"avg_hour_of_day\": { <1>\n    \"avg\":{\n      \"script\": { <2>\n        \"source\": \"\"\"\n          ZonedDateTime date =  doc['@timestamp'].value; <3>\n          return date.getHour(); <4>\n        \"\"\"\n      }\n    }  \n  },\n  \"avg_month_of_year\": { <5>\n    \"avg\":{\n      \"script\": { <6> \n        \"source\": \"\"\"\n          ZonedDateTime date =  doc['@timestamp'].value; <7>\n          return date.getMonthValue(); <8>\n        \"\"\"\n      }\n    }\n  },\n ...\n}\n--------------------------------------------------\n// NOTCONSOLE\n\n<1> Name of the aggregation.\n<2> Contains the Painless script that returns the hour of the day.\n<3> Sets `date` based on the timestamp of the document.\n<4> Returns the hour value from `date`.\n<5> Name of the aggregation.\n<6> Contains the Painless script that returns the month of the year.\n<7> Sets `date` based on the timestamp of the document.\n<8> Returns the month value from `date`.\n\n////\n[[painless-group-by]]\n== Using Painless in `group_by`\n\nIt is possible to base the `group_by` property of a {transform} on the output of \na script. The following example uses the {kib} sample web logs dataset. The goal \nhere is to make the {transform} output easier to understand through normalizing \nthe value of the fields that the data is grouped by.\n\n[source,console]\n--------------------------------------------------\nPOST _transform/_preview\n{\n  \"source\": {\n    \"index\": [ <1>\n      \"kibana_sample_data_logs\"\n    ]\n  },\n  \"pivot\": {\n    \"group_by\": {\n      \"agent\": {\n        \"terms\": {\n          \"script\": { <2>\n            \"source\": \"\"\"String agent = doc['agent.keyword'].value; \n            if (agent.contains(\"MSIE\")) { \n              return \"internet explorer\";\n            } else if (agent.contains(\"AppleWebKit\")) { \n              return \"safari\"; \n            } else if (agent.contains('Firefox')) { \n              return \"firefox\";\n            } else { return agent }\"\"\",\n            \"lang\": \"painless\"\n          }\n        }\n      }\n    },\n    \"aggregations\": { <3>\n      \"200\": {\n        \"filter\": {\n          \"term\": {\n            \"response\": \"200\"\n          }\n        }\n      },\n      \"404\": {\n        \"filter\": {\n          \"term\": {\n            \"response\": \"404\"\n          }\n        }\n      },\n      \"503\": {\n        \"filter\": {\n          \"term\": {\n            \"response\": \"503\"\n          }\n        }\n      }\n    }\n  },\n  \"dest\": { <4>\n    \"index\": \"pivot_logs\"\n  }\n} \n--------------------------------------------------\n// TEST[skip:setup kibana sample data]\n\n<1> Specifies the source index or indices.\n<2> The script defines an `agent` string based on the `agent` field of the \ndocuments, then iterates through the values. If an `agent` field contains \n\"MSIE\", than the script returns \"Internet Explorer\". If it contains \n`AppleWebKit`, it returns \"safari\". It returns \"firefox\" if the field value \ncontains \"Firefox\". Finally, in every other case, the value of the field is \nreturned.\n<3> The aggregations object contains filters that narrow down the results to \ndocuments that contains `200`, `404`, or `503` values in the `response` field.\n<4> Specifies the destination index of the {transform}.\n\nThe API returns the following result:\n\n[source,js]\n--------------------------------------------------\n{\n  \"preview\" : [\n    {\n      \"agent\" : \"firefox\",\n      \"200\" : 4931,\n      \"404\" : 259,\n      \"503\" : 172\n    },\n    {\n      \"agent\" : \"internet explorer\",\n      \"200\" : 3674,\n      \"404\" : 210,\n      \"503\" : 126\n    },\n    {\n      \"agent\" : \"safari\",\n      \"200\" : 4227,\n      \"404\" : 332,\n      \"503\" : 143\n    }\n  ],\n  \"mappings\" : {\n    \"properties\" : {\n      \"200\" : {\n        \"type\" : \"long\"\n      },\n      \"agent\" : {\n        \"type\" : \"keyword\"\n      },\n      \"404\" : {\n        \"type\" : \"long\"\n      },\n      \"503\" : {\n        \"type\" : \"long\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n// NOTCONSOLE\n\nYou can see that the `agent` values are simplified so it is easier to interpret \nthem. The table below shows how normalization modifies the output of the \n{transform} in our example compared to the non-normalized values.\n\n[width=\"50%\"]\n\n|===\n| Non-normalized `agent` value                                                 | Normalized `agent` value \n\n| \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\" | \"internet explorer\"\n| \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.50 Safari/534.24\" | \"safari\"\n| \"Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1\" | \"firefox\"\n|===\n////\n\n\n[[painless-bucket-script]]\n== Getting duration by using bucket script\n\nThis example shows you how to get the duration of a session by client IP from a \ndata log by using \n<<search-aggregations-pipeline-bucket-script-aggregation,bucket script>>. \nThe example uses the {kib} sample web logs dataset.\n\n[source,console]\n--------------------------------------------------\nPUT _transform/data_log\n{\n  \"source\": {\n    \"index\": \"kibana_sample_data_logs\"\n  },\n  \"dest\": {\n    \"index\": \"data-logs-by-client\"\n  },\n  \"pivot\": {\n    \"group_by\": {\n      \"machine.os\": {\"terms\": {\"field\": \"machine.os.keyword\"}},\n      \"machine.ip\": {\"terms\": {\"field\": \"clientip\"}}\n    },\n    \"aggregations\": {\n      \"time_frame.lte\": {\n        \"max\": {\n          \"field\": \"timestamp\"\n        }\n      },\n      \"time_frame.gte\": {\n        \"min\": {\n          \"field\": \"timestamp\"\n        }\n      },\n      \"time_length\": { <1>\n        \"bucket_script\": {\n          \"buckets_path\": { <2>\n            \"min\": \"time_frame.gte.value\",\n            \"max\": \"time_frame.lte.value\"\n          },\n          \"script\": \"params.max - params.min\" <3>\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[skip:setup kibana sample data]\n\n<1> To define the length of the sessions, we use a bucket script.\n<2> The bucket path is a map of script variables and their associated path to \nthe buckets you want to use for the variable. In this particular case, `min` and \n`max` are variables mapped to `time_frame.gte.value` and `time_frame.lte.value`.\n<3> Finally, the script substracts the start date of the session from the end \ndate which results in the duration of the session.\n\n[[painless-count-http]]\n== Counting HTTP responses by using scripted metric aggregation\n\nYou can count the different HTTP response types in a web log data set by using \nscripted metric aggregation as part of the {transform}. You can achieve a \nsimilar function with filter aggregations, check the \n{ref}/transform-examples.html#example-clientips[Finding suspicious client IPs] \nexample for details.\n\nThe example below assumes that the HTTP response codes are stored as keywords in \nthe `response` field of the documents.\n\nIMPORTANT: This example uses a `scripted_metric` aggregation which is not supported on {es} Serverless.\n\n[source,js]\n--------------------------------------------------\n\"aggregations\": { <1>\n  \"responses.counts\": { <2>\n    \"scripted_metric\": { <3>\n      \"init_script\": \"state.responses = ['error':0L,'success':0L,'other':0L]\", <4>\n      \"map_script\": \"\"\" <5>\n        def code = doc['response.keyword'].value;\n        if (code.startsWith('5') || code.startsWith('4')) {\n          state.responses.error += 1 ;\n        } else if(code.startsWith('2')) {\n          state.responses.success += 1;\n        } else {\n          state.responses.other += 1;\n        }\n        \"\"\",\n      \"combine_script\": \"state.responses\", <6>\n      \"reduce_script\": \"\"\" <7>\n        def counts = ['error': 0L, 'success': 0L, 'other': 0L];\n        for (responses in states) {\n          counts.error += responses['error'];\n          counts.success += responses['success'];\n          counts.other += responses['other'];\n        }\n        return counts;\n        \"\"\"\n      }\n    },\n  ...  \n}\n--------------------------------------------------\n// NOTCONSOLE\n\n<1> The `aggregations` object of the {transform} that contains all aggregations.\n<2> Object of the `scripted_metric` aggregation.\n<3> This `scripted_metric` performs a distributed operation on the web log data \nto count specific types of HTTP responses (error, success, and other).\n<4> The `init_script` creates a `responses` array in the `state` object with \nthree properties (`error`, `success`, `other`) with long data type.\n<5> The `map_script` defines `code` based on the `response.keyword` value of the \ndocument, then it counts the errors, successes, and other responses based on the \nfirst digit of the responses.\n<6> The `combine_script` returns `state.responses` from each shard.\n<7> The `reduce_script` creates a `counts` array with the `error`, `success`, \nand `other` properties, then iterates through the value of `responses` returned \nby each shard and assigns the different response types to the appropriate \nproperties of the `counts` object; error responses to the error counts, success \nresponses to the success counts, and other responses to the other counts. \nFinally, returns the `counts` array with the response counts.\n\n[[painless-compare]]\n== Comparing indices by using scripted metric aggregations\n\nThis example shows how to compare the content of two indices by a {transform} \nthat uses a scripted metric aggregation.\n\nIMPORTANT: This example uses a `scripted_metric` aggregation which is not supported on {es} Serverless.\n\n[source,console]\n--------------------------------------------------\nPOST _transform/_preview\n{\n  \"id\" : \"index_compare\",\n  \"source\" : { <1>\n    \"index\" : [\n      \"index1\",\n      \"index2\"\n    ],\n    \"query\" : {\n      \"match_all\" : { }\n    }\n  },\n  \"dest\" : { <2>\n    \"index\" : \"compare\"\n  },\n  \"pivot\" : {\n    \"group_by\" : {\n      \"unique-id\" : {\n        \"terms\" : {\n          \"field\" : \"<unique-id-field>\" <3>\n        }\n      }\n    },\n    \"aggregations\" : {\n      \"compare\" : { <4>\n        \"scripted_metric\" : {\n          \"map_script\" : \"state.doc = new HashMap(params['_source'])\", <5>\n          \"combine_script\" : \"return state\", <6>\n          \"reduce_script\" : \"\"\" <7>\n            if (states.size() != 2) {\n              return \"count_mismatch\"\n            }\n            if (states.get(0).equals(states.get(1))) {\n              return \"match\"\n            } else {\n              return \"mismatch\"\n            }\n            \"\"\"\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[skip:setup kibana sample data]\n\n<1> The indices referenced in the `source` object are compared to each other.\n<2> The `dest` index contains the results of the comparison.\n<3> The `group_by` field needs to be a unique identifier for each document.\n<4> Object of the `scripted_metric` aggregation.\n<5> The `map_script` defines `doc` in the state object. By using \n`new HashMap(...)` you copy the source document, this is important whenever you \nwant to pass the full source object from one phase to the next.\n<6> The `combine_script` returns `state` from each shard.\n<7> The `reduce_script` checks if the size of the indices are equal. If they are \nnot equal, than it reports back a `count_mismatch`. Then it iterates through all \nthe values of the two indices and compare them. If the values are equal, then it \nreturns a `match`, otherwise returns a `mismatch`.\n\n[[painless-web-session]]\n== Getting web session details by using scripted metric aggregation\n\nThis example shows how to derive multiple features from a single transaction. \nLet's take a look on the example source document from the data:\n\n.Source document\n[%collapsible%open]\n=====\n[source,js]\n--------------------------------------------------\n{\n  \"_index\":\"apache-sessions\",\n  \"_type\":\"_doc\",\n  \"_id\":\"KvzSeGoB4bgw0KGbE3wP\",\n  \"_score\":1.0,\n  \"_source\":{\n    \"@timestamp\":1484053499256,\n    \"apache\":{\n      \"access\":{\n        \"sessionid\":\"571604f2b2b0c7b346dc685eeb0e2306774a63c2\",\n        \"url\":\"http://www.leroymerlin.fr/v3/search/search.do?keyword=Carrelage%20salle%20de%20bain\",\n        \"path\":\"/v3/search/search.do\",\n        \"query\":\"keyword=Carrelage%20salle%20de%20bain\",\n        \"referrer\":\"http://www.leroymerlin.fr/v3/p/produits/carrelage-parquet-sol-souple/carrelage-sol-et-mur/decor-listel-et-accessoires-carrelage-mural-l1308217717?resultOffset=0&resultLimit=51&resultListShape=MOSAIC&priceStyle=SALEUNIT_PRICE\",\n        \"user_agent\":{\n          \"original\":\"Mobile Safari 10.0 Mac OS X (iPad) Apple Inc.\",\n          \"os_name\":\"Mac OS X (iPad)\"\n        },\n        \"remote_ip\":\"0337b1fa-5ed4-af81-9ef4-0ec53be0f45d\",\n        \"geoip\":{\n          \"country_iso_code\":\"FR\",\n          \"location\":{\n            \"lat\":48.86,\n            \"lon\":2.35\n          }\n        },\n        \"response_code\":200,\n        \"method\":\"GET\"\n      }\n    }\n  }\n}\n...\n--------------------------------------------------\n// NOTCONSOLE\n=====\n\n\nBy using the `sessionid` as a group-by field, you are able to enumerate events \nthrough the session and get more details of the session by using scripted metric \naggregation.\n\nIMPORTANT: This example uses a `scripted_metric` aggregation which is not supported on {es} Serverless.\n\n[source,js]\n--------------------------------------------------\nPOST _transform/_preview\n{\n  \"source\": {\n    \"index\": \"apache-sessions\"\n  },\n  \"pivot\": {\n    \"group_by\": {\n      \"sessionid\": { <1>\n        \"terms\": {\n          \"field\": \"apache.access.sessionid\"\n        }\n      }\n    },\n    \"aggregations\": { <2>\n      \"distinct_paths\": { \n        \"cardinality\": {\n          \"field\": \"apache.access.path\"\n        }\n      },\n      \"num_pages_viewed\": {\n        \"value_count\": {\n          \"field\": \"apache.access.url\"\n        }\n      },\n      \"session_details\": {\n        \"scripted_metric\": {\n          \"init_script\": \"state.docs = []\", <3>\n          \"map_script\": \"\"\" <4>\n            Map span = [\n              '@timestamp':doc['@timestamp'].value, \n              'url':doc['apache.access.url'].value,\n              'referrer':doc['apache.access.referrer'].value\n            ]; \n            state.docs.add(span)\n          \"\"\",\n          \"combine_script\": \"return state.docs;\", <5>\n          \"reduce_script\": \"\"\" <6>\n            def all_docs = []; \n            for (s in states) { \n              for (span in s) { \n                all_docs.add(span); \n              }\n            }\n            all_docs.sort((HashMap o1, HashMap o2)->o1['@timestamp'].toEpochMilli().compareTo(o2['@timestamp'].toEpochMilli())); \n            def size = all_docs.size();\n            def min_time = all_docs[0]['@timestamp'];\n            def max_time = all_docs[size-1]['@timestamp'];\n            def duration = max_time.toEpochMilli() - min_time.toEpochMilli();\n            def entry_page = all_docs[0]['url'];\n            def exit_path = all_docs[size-1]['url'];\n            def first_referrer = all_docs[0]['referrer'];\n            def ret = new HashMap();\n            ret['first_time'] = min_time;\n            ret['last_time'] = max_time;\n            ret['duration'] = duration;\n            ret['entry_page'] = entry_page;\n            ret['exit_path'] = exit_path;\n            ret['first_referrer'] = first_referrer;\n            return ret;\n          \"\"\"\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n// NOTCONSOLE\n\n<1> The data is grouped by `sessionid`.\n<2> The aggregations counts the number of paths and enumerate the viewed pages \nduring the session.\n<3> The `init_script` creates an array type `doc` in the `state` object.\n<4> The `map_script` defines a `span` array with a timestamp, a URL, and a \nreferrer value which are based on the corresponding values of the document, then \nadds the value of the `span` array to the `doc` object.\n<5> The `combine_script` returns `state.docs` from each shard.\n<6> The `reduce_script` defines various objects like `min_time`, `max_time`, and \n`duration` based on the document fields, then declares a `ret` object, and \ncopies the source document by using `new HashMap ()`. Next, the script defines \n`first_time`, `last_time`, `duration` and other fields inside the `ret` object \nbased on the corresponding object defined earlier, finally returns `ret`.\n\nThe API call results in a similar response:\n\n[source,js]\n--------------------------------------------------\n{\n  \"num_pages_viewed\" : 2.0,\n  \"session_details\" : {\n    \"duration\" : 100300001,\n    \"first_referrer\" : \"https://www.bing.com/\",\n    \"entry_page\" : \"http://www.leroymerlin.fr/v3/p/produits/materiaux-menuiserie/porte-coulissante-porte-interieure-escalier-et-rambarde/barriere-de-securite-l1308218463\",\n    \"first_time\" : \"2017-01-10T21:22:52.982Z\",\n    \"last_time\" : \"2017-01-10T21:25:04.356Z\",\n    \"exit_path\" : \"http://www.leroymerlin.fr/v3/p/produits/materiaux-menuiserie/porte-coulissante-porte-interieure-escalier-et-rambarde/barriere-de-securite-l1308218463?__result-wrapper?pageTemplate=Famille%2FMat%C3%A9riaux+et+menuiserie&resultOffset=0&resultLimit=50&resultListShape=PLAIN&nomenclatureId=17942&priceStyle=SALEUNIT_PRICE&fcr=1&*4294718806=4294718806&*14072=14072&*4294718593=4294718593&*17942=17942\"\n  },\n  \"distinct_paths\" : 1.0,\n  \"sessionid\" : \"000046f8154a80fd89849369c984b8cc9d795814\"\n},\n{\n  \"num_pages_viewed\" : 10.0,\n  \"session_details\" : {\n    \"duration\" : 343100405,\n    \"first_referrer\" : \"https://www.google.fr/\",\n    \"entry_page\" : \"http://www.leroymerlin.fr/\",\n    \"first_time\" : \"2017-01-10T16:57:39.937Z\",\n    \"last_time\" : \"2017-01-10T17:03:23.049Z\",\n    \"exit_path\" : \"http://www.leroymerlin.fr/v3/p/produits/porte-de-douche-coulissante-adena-e168578\"\n  },\n  \"distinct_paths\" : 8.0,\n  \"sessionid\" : \"000087e825da1d87a332b8f15fa76116c7467da6\"\n}\n...\n--------------------------------------------------\n// NOTCONSOLE\n"
}