{
    "meta": {
        "size": 8430,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-edgengram-tokenizer.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "analysis-edgengram-tokenizer",
        "version": "8.15"
    },
    "doc": "[[analysis-edgengram-tokenizer]]\n=== Edge n-gram tokenizer\n++++\n<titleabbrev>Edge n-gram</titleabbrev>\n++++\n\nThe `edge_ngram` tokenizer first breaks text down into words whenever it\nencounters one of a list of specified characters, then it emits\n{wikipedia}/N-gram[N-grams] of each word where the start of\nthe N-gram is anchored to the beginning of the word.\n\nEdge N-Grams are useful for _search-as-you-type_ queries.\n\nTIP: When you need _search-as-you-type_ for text which has a widely known\norder, such as movie or song titles, the\n<<completion-suggester,completion suggester>> is a much more efficient\nchoice than edge N-grams. Edge N-grams have the advantage when trying to\nautocomplete words that can appear in any order.\n\n[discrete]\n=== Example output\n\nWith the default settings, the `edge_ngram` tokenizer treats the initial text as a\nsingle token and produces N-grams with minimum length `1` and maximum length\n`2`:\n\n[source,console]\n---------------------------\nPOST _analyze\n{\n  \"tokenizer\": \"edge_ngram\",\n  \"text\": \"Quick Fox\"\n}\n---------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"Q\",\n      \"start_offset\": 0,\n      \"end_offset\": 1,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"Qu\",\n      \"start_offset\": 0,\n      \"end_offset\": 2,\n      \"type\": \"word\",\n      \"position\": 1\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe above sentence would produce the following terms:\n\n[source,text]\n---------------------------\n[ Q, Qu ]\n---------------------------\n\nNOTE: These default gram lengths are almost entirely useless. You need to\nconfigure the `edge_ngram` before using it.\n\n[discrete]\n=== Configuration\n\nThe `edge_ngram` tokenizer accepts the following parameters:\n\n`min_gram`::\n    Minimum length of characters in a gram. Defaults to `1`.\n\n`max_gram`::\n+\n--\nMaximum length of characters in a gram. Defaults to `2`.\n\nSee <<max-gram-limits>>.\n--\n\n`token_chars`::\n\n    Character classes that should be included in a token. Elasticsearch\n    will split on characters that don't belong to the classes specified.\n    Defaults to `[]` (keep all characters).\n+\nCharacter classes may be any of the following:\n+\n* `letter` --      for example `a`, `b`, `\u00ef` or `\u4eac`\n* `digit` --       for example `3` or `7`\n* `whitespace` --  for example `\" \"` or `\"\\n\"`\n* `punctuation` -- for example `!` or `\"`\n* `symbol` --      for example `$` or `\u221a`\n* `custom` --      custom characters which need to be set using the\n`custom_token_chars` setting.\n\n`custom_token_chars`::\n\n    Custom characters that should be treated as part of a token. For example,\n    setting this to `+-_` will make the tokenizer treat the plus, minus and\n    underscore sign as part of a token.\n\n[discrete]\n[[max-gram-limits]]\n=== Limitations of the `max_gram` parameter\n\nThe `edge_ngram` tokenizer's `max_gram` value limits the character length of\ntokens. When the `edge_ngram` tokenizer is used with an index analyzer, this\nmeans search terms longer than the `max_gram` length may not match any indexed\nterms.\n\nFor example, if the `max_gram` is `3`, searches for `apple` won't match the\nindexed term `app`.\n\nTo account for this, you can use the\n<<analysis-truncate-tokenfilter,`truncate`>> token filter with a search analyzer\nto shorten search terms to the `max_gram` character length. However, this could\nreturn irrelevant results.\n\nFor example, if the `max_gram` is `3` and search terms are truncated to three\ncharacters, the search term `apple` is shortened to `app`. This means searches\nfor `apple` return any indexed terms matching `app`, such as `apply`, `approximate`\nand `apple`.\n\nWe recommend testing both approaches to see which best fits your\nuse case and desired search experience.\n\n[discrete]\n=== Example configuration\n\nIn this example, we configure the `edge_ngram` tokenizer to treat letters and\ndigits as tokens, and to produce grams with minimum length `2` and maximum\nlength `10`:\n\n[source,console]\n----------------------------\nPUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"my_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"edge_ngram\",\n          \"min_gram\": 2,\n          \"max_gram\": 10,\n          \"token_chars\": [\n            \"letter\",\n            \"digit\"\n          ]\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"2 Quick Foxes.\"\n}\n----------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"Qu\",\n      \"start_offset\": 2,\n      \"end_offset\": 4,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"Qui\",\n      \"start_offset\": 2,\n      \"end_offset\": 5,\n      \"type\": \"word\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"Quic\",\n      \"start_offset\": 2,\n      \"end_offset\": 6,\n      \"type\": \"word\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"Quick\",\n      \"start_offset\": 2,\n      \"end_offset\": 7,\n      \"type\": \"word\",\n      \"position\": 3\n    },\n    {\n      \"token\": \"Fo\",\n      \"start_offset\": 8,\n      \"end_offset\": 10,\n      \"type\": \"word\",\n      \"position\": 4\n    },\n    {\n      \"token\": \"Fox\",\n      \"start_offset\": 8,\n      \"end_offset\": 11,\n      \"type\": \"word\",\n      \"position\": 5\n    },\n    {\n      \"token\": \"Foxe\",\n      \"start_offset\": 8,\n      \"end_offset\": 12,\n      \"type\": \"word\",\n      \"position\": 6\n    },\n    {\n      \"token\": \"Foxes\",\n      \"start_offset\": 8,\n      \"end_offset\": 13,\n      \"type\": \"word\",\n      \"position\": 7\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\nThe above example produces the following terms:\n\n[source,text]\n---------------------------\n[ Qu, Qui, Quic, Quick, Fo, Fox, Foxe, Foxes ]\n---------------------------\n\nUsually we recommend using the same `analyzer` at index time and at search\ntime. In the case of the `edge_ngram` tokenizer, the advice is different. It\nonly makes sense to use the `edge_ngram` tokenizer at index time, to ensure\nthat partial words are available for matching in the index. At search time,\njust search for the terms the user has typed in, for instance: `Quick Fo`.\n\nBelow is an example of how to set up a field for _search-as-you-type_.\n\nNote that the `max_gram` value for the index analyzer is `10`, which limits\nindexed terms to 10 characters. Search terms are not truncated, meaning that\nsearch terms longer than 10 characters may not match any indexed terms.\n\n[source,console]\n-----------------------------------\nPUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"autocomplete\": {\n          \"tokenizer\": \"autocomplete\",\n          \"filter\": [\n            \"lowercase\"\n          ]\n        },\n        \"autocomplete_search\": {\n          \"tokenizer\": \"lowercase\"\n        }\n      },\n      \"tokenizer\": {\n        \"autocomplete\": {\n          \"type\": \"edge_ngram\",\n          \"min_gram\": 2,\n          \"max_gram\": 10,\n          \"token_chars\": [\n            \"letter\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\",\n        \"analyzer\": \"autocomplete\",\n        \"search_analyzer\": \"autocomplete_search\"\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"title\": \"Quick Foxes\" <1>\n}\n\nPOST my-index-000001/_refresh\n\nGET my-index-000001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"title\": {\n        \"query\": \"Quick Fo\", <2>\n        \"operator\": \"and\"\n      }\n    }\n  }\n}\n-----------------------------------\n\n<1> The `autocomplete` analyzer indexes the terms `[qu, qui, quic, quick, fo, fox, foxe, foxes]`.\n<2> The `autocomplete_search` analyzer searches for the terms `[quick, fo]`, both of which appear in the index.\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"took\": $body.took,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\" : 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\" : {\n        \"value\": 1,\n        \"relation\": \"eq\"\n    },\n    \"max_score\": 0.5753642,\n    \"hits\": [\n      {\n        \"_index\": \"my-index-000001\",\n        \"_id\": \"1\",\n        \"_score\": 0.5753642,\n        \"_source\": {\n          \"title\": \"Quick Foxes\"\n        }\n      }\n    ]\n  }\n}\n----------------------------\n// TESTRESPONSE[s/\"took\".*/\"took\": \"$body.took\",/]\n/////////////////////\n"
}