{
    "meta": {
        "timestamp": "2024-11-01T03:02:53.041579",
        "size": 6558,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-ngram-tokenizer.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "analysis-ngram-tokenizer",
        "version": "8.15"
    },
    "doc": "[[analysis-ngram-tokenizer]]\n=== N-gram tokenizer\n++++\n<titleabbrev>N-gram</titleabbrev>\n++++\n\nThe `ngram` tokenizer first breaks text down into words whenever it encounters\none of a list of specified characters, then it emits\n{wikipedia}/N-gram[N-grams] of each word of the specified\nlength.\n\nN-grams are like a sliding window that moves across the word - a continuous\nsequence of characters of the specified length. They are useful for querying\nlanguages that don't use spaces or that have long compound words, like German.\n\n[discrete]\n=== Example output\n\nWith the default settings, the `ngram` tokenizer treats the initial text as a\nsingle token and produces N-grams with minimum length `1` and maximum length\n`2`:\n\n[source,console]\n---------------------------\nPOST _analyze\n{\n  \"tokenizer\": \"ngram\",\n  \"text\": \"Quick Fox\"\n}\n---------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"Q\",\n      \"start_offset\": 0,\n      \"end_offset\": 1,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"Qu\",\n      \"start_offset\": 0,\n      \"end_offset\": 2,\n      \"type\": \"word\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"u\",\n      \"start_offset\": 1,\n      \"end_offset\": 2,\n      \"type\": \"word\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"ui\",\n      \"start_offset\": 1,\n      \"end_offset\": 3,\n      \"type\": \"word\",\n      \"position\": 3\n    },\n    {\n      \"token\": \"i\",\n      \"start_offset\": 2,\n      \"end_offset\": 3,\n      \"type\": \"word\",\n      \"position\": 4\n    },\n    {\n      \"token\": \"ic\",\n      \"start_offset\": 2,\n      \"end_offset\": 4,\n      \"type\": \"word\",\n      \"position\": 5\n    },\n    {\n      \"token\": \"c\",\n      \"start_offset\": 3,\n      \"end_offset\": 4,\n      \"type\": \"word\",\n      \"position\": 6\n    },\n    {\n      \"token\": \"ck\",\n      \"start_offset\": 3,\n      \"end_offset\": 5,\n      \"type\": \"word\",\n      \"position\": 7\n    },\n    {\n      \"token\": \"k\",\n      \"start_offset\": 4,\n      \"end_offset\": 5,\n      \"type\": \"word\",\n      \"position\": 8\n    },\n    {\n      \"token\": \"k \",\n      \"start_offset\": 4,\n      \"end_offset\": 6,\n      \"type\": \"word\",\n      \"position\": 9\n    },\n    {\n      \"token\": \" \",\n      \"start_offset\": 5,\n      \"end_offset\": 6,\n      \"type\": \"word\",\n      \"position\": 10\n    },\n    {\n      \"token\": \" F\",\n      \"start_offset\": 5,\n      \"end_offset\": 7,\n      \"type\": \"word\",\n      \"position\": 11\n    },\n    {\n      \"token\": \"F\",\n      \"start_offset\": 6,\n      \"end_offset\": 7,\n      \"type\": \"word\",\n      \"position\": 12\n    },\n    {\n      \"token\": \"Fo\",\n      \"start_offset\": 6,\n      \"end_offset\": 8,\n      \"type\": \"word\",\n      \"position\": 13\n    },\n    {\n      \"token\": \"o\",\n      \"start_offset\": 7,\n      \"end_offset\": 8,\n      \"type\": \"word\",\n      \"position\": 14\n    },\n    {\n      \"token\": \"ox\",\n      \"start_offset\": 7,\n      \"end_offset\": 9,\n      \"type\": \"word\",\n      \"position\": 15\n    },\n    {\n      \"token\": \"x\",\n      \"start_offset\": 8,\n      \"end_offset\": 9,\n      \"type\": \"word\",\n      \"position\": 16\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe above sentence would produce the following terms:\n\n[source,text]\n---------------------------\n[ Q, Qu, u, ui, i, ic, c, ck, k, \"k \", \" \", \" F\", F, Fo, o, ox, x ]\n---------------------------\n\n[discrete]\n=== Configuration\n\nThe `ngram` tokenizer accepts the following parameters:\n\n[horizontal]\n`min_gram`::\n    Minimum length of characters in a gram. Defaults to `1`.\n\n`max_gram`::\n    Maximum length of characters in a gram. Defaults to `2`.\n\n`token_chars`::\n\n    Character classes that should be included in a token. Elasticsearch\n    will split on characters that don't belong to the classes specified.\n    Defaults to `[]` (keep all characters).\n+\nCharacter classes may be any of the following:\n+\n* `letter` --      for example `a`, `b`, `\u00ef` or `\u4eac`\n* `digit` --       for example `3` or `7`\n* `whitespace` --  for example `\" \"` or `\"\\n\"`\n* `punctuation` -- for example `!` or `\"`\n* `symbol` --      for example `$` or `\u221a`\n* `custom` --      custom characters which need to be set using the\n`custom_token_chars` setting.\n\n`custom_token_chars`::\n\n    Custom characters that should be treated as part of a token. For example,\n    setting this to `+-_` will make the tokenizer treat the plus, minus and\n    underscore sign as part of a token.\n\nTIP:  It usually makes sense to set `min_gram` and `max_gram` to the same\nvalue. The smaller the length, the more documents will match but the lower\nthe quality of the matches. The longer the length, the more specific the\nmatches. A tri-gram (length `3`) is a good place to start.\n\nThe index level setting `index.max_ngram_diff` controls the maximum allowed\ndifference between `max_gram` and `min_gram`.\n\n[discrete]\n=== Example configuration\n\nIn this example, we configure the `ngram` tokenizer to treat letters and\ndigits as tokens, and to produce tri-grams (grams of length `3`):\n\n[source,console]\n----------------------------\nPUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"my_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"ngram\",\n          \"min_gram\": 3,\n          \"max_gram\": 3,\n          \"token_chars\": [\n            \"letter\",\n            \"digit\"\n          ]\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"2 Quick Foxes.\"\n}\n----------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"Qui\",\n      \"start_offset\": 2,\n      \"end_offset\": 5,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"uic\",\n      \"start_offset\": 3,\n      \"end_offset\": 6,\n      \"type\": \"word\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"ick\",\n      \"start_offset\": 4,\n      \"end_offset\": 7,\n      \"type\": \"word\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"Fox\",\n      \"start_offset\": 8,\n      \"end_offset\": 11,\n      \"type\": \"word\",\n      \"position\": 3\n    },\n    {\n      \"token\": \"oxe\",\n      \"start_offset\": 9,\n      \"end_offset\": 12,\n      \"type\": \"word\",\n      \"position\": 4\n    },\n    {\n      \"token\": \"xes\",\n      \"start_offset\": 10,\n      \"end_offset\": 13,\n      \"type\": \"word\",\n      \"position\": 5\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe above example produces the following terms:\n\n[source,text]\n---------------------------\n[ Qui, uic, ick, Fox, oxe, xes ]\n---------------------------\n"
}