{
    "meta": {
        "timestamp": "2024-11-01T02:49:26.387081",
        "size": 9039,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/infer-service-elasticsearch.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "infer-service-elasticsearch",
        "version": "8.15"
    },
    "doc": "[[infer-service-elasticsearch]]\n=== Elasticsearch {infer} service\n\nCreates an {infer} endpoint to perform an {infer} task with the `elasticsearch` service.\n\nNOTE: If you use the ELSER or the E5 model through the `elasticsearch` service, the API request will automatically download and deploy the model if it isn't downloaded yet.\n\n\n[discrete]\n[[infer-service-elasticsearch-api-request]]\n==== {api-request-title}\n\n`PUT /_inference/<task_type>/<inference_id>`\n\n[discrete]\n[[infer-service-elasticsearch-api-path-params]]\n==== {api-path-parms-title}\n\n`<inference_id>`::\n(Required, string)\ninclude::inference-shared.asciidoc[tag=inference-id]\n\n`<task_type>`::\n(Required, string)\ninclude::inference-shared.asciidoc[tag=task-type]\n+\n--\nAvailable task types:\n\n* `rerank`,\n* `sparse_embedding`,\n* `text_embedding`.\n--\n\n[discrete]\n[[infer-service-elasticsearch-api-request-body]]\n==== {api-request-body-title}\n\n`chunking_settings`::\n(Optional, object)\ninclude::inference-shared.asciidoc[tag=chunking-settings]\n\n`max_chunking_size`:::\n(Optional, integer)\ninclude::inference-shared.asciidoc[tag=chunking-settings-max-chunking-size]\n\n`overlap`:::\n(Optional, integer)\ninclude::inference-shared.asciidoc[tag=chunking-settings-overlap]\n\n`sentence_overlap`:::\n(Optional, integer)\ninclude::inference-shared.asciidoc[tag=chunking-settings-sentence-overlap]\n\n`strategy`:::\n(Optional, string)\ninclude::inference-shared.asciidoc[tag=chunking-settings-strategy]\n\n`service`::\n(Required, string)\nThe type of service supported for the specified task type. In this case,\n`elasticsearch`.\n\n`service_settings`::\n(Required, object)\ninclude::inference-shared.asciidoc[tag=service-settings]\n+\n--\nThese settings are specific to the `elasticsearch` service.\n--\n\n`adaptive_allocations`:::\n(Optional, object)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation]\n\n`deployment_id`:::\n(Optional, string)\nThe `deployment_id` of an existing trained model deployment.\nWhen `deployment_id` is used the `model_id` is optional.\n\n`enabled`::::\n(Optional, Boolean)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation-enabled]\n\n`max_number_of_allocations`::::\n(Optional, integer)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation-max-number]\n\n`min_number_of_allocations`::::\n(Optional, integer)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation-min-number]\n\n`model_id`:::\n(Required, string)\nThe name of the model to use for the {infer} task.\nIt can be the ID of either a built-in model (for example, `.multilingual-e5-small` for E5), a text embedding model already\n{ml-docs}/ml-nlp-import-model.html#ml-nlp-import-script[uploaded through Eland].\n\n`num_allocations`:::\n(Required, integer)\nThe total number of allocations this model is assigned across machine learning nodes.\nIncreasing this value generally increases the throughput.\nIf `adaptive_allocations` is enabled, do not set this value, because it's automatically set.\n\n`num_threads`:::\n(Required, integer)\nSets the number of threads used by each model allocation during inference. This generally increases the speed per inference request. The inference process is a compute-bound process; `threads_per_allocations` must not exceed the number of available allocated processors per node.\nMust be a power of 2. Max allowed value is 32.\n\n`task_settings`::\n(Optional, object)\ninclude::inference-shared.asciidoc[tag=task-settings]\n+\n.`task_settings` for the `rerank` task type\n[%collapsible%closed]\n=====\n`return_documents`:::\n(Optional, Boolean)\nReturns the document instead of only the index. Defaults to `true`.\n=====\n\n\n[discrete]\n[[inference-example-elasticsearch-elser]]\n==== ELSER via the `elasticsearch` service\n\nThe following example shows how to create an {infer} endpoint called `my-elser-model` to perform a `sparse_embedding` task type.\n\nThe API request below will automatically download the ELSER model if it isn't already downloaded and then deploy the model.\n\n[source,console]\n------------------------------------------------------------\nPUT _inference/sparse_embedding/my-elser-model\n{\n  \"service\": \"elasticsearch\",\n  \"service_settings\": {\n    \"adaptive_allocations\": { <1>\n      \"enabled\": true,\n      \"min_number_of_allocations\": 1,\n      \"max_number_of_allocations\": 10\n    },\n    \"num_threads\": 1,\n    \"model_id\": \".elser_model_2\" <2>\n  }\n}\n------------------------------------------------------------\n// TEST[skip:TBD]\n<1> Adaptive allocations will be enabled with the minimum of 1 and the maximum of 10 allocations.\n<2> The `model_id` must be the ID of one of the built-in ELSER models.\nValid values are `.elser_model_2` and `.elser_model_2_linux-x86_64`.\nFor further details, refer to the {ml-docs}/ml-nlp-elser.html[ELSER model documentation].\n\n\n[discrete]\n[[inference-example-elasticsearch]]\n==== E5 via the `elasticsearch` service\n\nThe following example shows how to create an {infer} endpoint called `my-e5-model` to perform a `text_embedding` task type.\n\nThe API request below will automatically download the E5 model if it isn't already downloaded and then deploy the model.\n\n[source,console]\n------------------------------------------------------------\nPUT _inference/text_embedding/my-e5-model\n{\n  \"service\": \"elasticsearch\",\n  \"service_settings\": {\n    \"num_allocations\": 1,\n    \"num_threads\": 1,\n    \"model_id\": \".multilingual-e5-small\" <1>\n  }\n}\n------------------------------------------------------------\n// TEST[skip:TBD]\n<1> The `model_id` must be the ID of one of the built-in E5 models.\nValid values are `.multilingual-e5-small` and `.multilingual-e5-small_linux-x86_64`.\nFor further details, refer to the {ml-docs}/ml-nlp-e5.html[E5 model documentation].\n\n[NOTE]\n====\nYou might see a 502 bad gateway error in the response when using the {kib} Console.\nThis error usually just reflects a timeout, while the model downloads in the background.\nYou can check the download progress in the {ml-app} UI.\nIf using the Python client, you can set the `timeout` parameter to a higher value.\n====\n\n[discrete]\n[[inference-example-eland]]\n==== Models uploaded by Eland via the elasticsearch service\n\nThe following example shows how to create an {infer} endpoint called\n`my-msmarco-minilm-model` to perform a `text_embedding` task type.\n\n[source,console]\n------------------------------------------------------------\nPUT _inference/text_embedding/my-msmarco-minilm-model <1>\n{\n  \"service\": \"elasticsearch\",\n  \"service_settings\": {\n    \"num_allocations\": 1,\n    \"num_threads\": 1,\n    \"model_id\": \"msmarco-MiniLM-L12-cos-v5\" <2>\n  }\n}\n------------------------------------------------------------\n// TEST[skip:TBD]\n<1> Provide an unique identifier for the inference endpoint. The `inference_id` must be unique and must not match the `model_id`.\n<2> The `model_id` must be the ID of a text embedding model which has already been\n{ml-docs}/ml-nlp-import-model.html#ml-nlp-import-script[uploaded through Eland].\n\n[discrete]\n[[inference-example-adaptive-allocation]]\n==== Setting adaptive allocation for E5 via the `elasticsearch` service\n\nThe following example shows how to create an {infer} endpoint called\n`my-e5-model` to perform a `text_embedding` task type and configure adaptive\nallocations.\n\nThe API request below will automatically download the E5 model if it isn't\nalready downloaded and then deploy the model.\n\n[source,console]\n------------------------------------------------------------\nPUT _inference/text_embedding/my-e5-model\n{\n  \"service\": \"elasticsearch\",\n  \"service_settings\": {\n    \"adaptive_allocations\": {\n      \"enabled\": true,\n      \"min_number_of_allocations\": 3,\n      \"max_number_of_allocations\": 10\n    },\n    \"num_threads\": 1,\n    \"model_id\": \".multilingual-e5-small\"\n  }\n}\n------------------------------------------------------------\n// TEST[skip:TBD]\n\n\n[discrete]\n[[inference-example-existing-deployment]]\n==== Using an existing model deployment with the `elasticsearch` service\n\nThe following example shows how to use an already existing model deployment when creating an {infer} endpoint.\n\n[source,console]\n------------------------------------------------------------\nPUT _inference/sparse_embedding/use_existing_deployment\n{\n  \"service\": \"elasticsearch\",\n  \"service_settings\": {\n    \"deployment_id\": \".elser_model_2\" <1>\n  }\n}\n------------------------------------------------------------\n// TEST[skip:TBD]\n<1> The `deployment_id` of the already existing model deployment.\n\nThe API response contains the `model_id`, and the threads and allocations settings from the model deployment:\n\n[source,console-result]\n------------------------------------------------------------\n{\n  \"inference_id\": \"use_existing_deployment\",\n  \"task_type\": \"sparse_embedding\",\n  \"service\": \"elasticsearch\",\n  \"service_settings\": {\n    \"num_allocations\": 2,\n    \"num_threads\": 1,\n    \"model_id\": \".elser_model_2\",\n    \"deployment_id\": \".elser_model_2\"\n  },\n  \"chunking_settings\": {\n    \"strategy\": \"sentence\",\n    \"max_chunk_size\": 250,\n    \"sentence_overlap\": 1\n  }\n}\n------------------------------------------------------------\n// NOTCONSOLE"
}