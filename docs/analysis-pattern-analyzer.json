{
    "meta": {
        "timestamp": "2024-11-01T02:49:25.904085",
        "size": 9165,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pattern-analyzer.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "analysis-pattern-analyzer",
        "version": "8.15"
    },
    "doc": "[[analysis-pattern-analyzer]]\n=== Pattern analyzer\n++++\n<titleabbrev>Pattern</titleabbrev>\n++++\n\nThe `pattern` analyzer uses a regular expression to split the text into terms.\nThe regular expression should match the *token separators*  not the tokens\nthemselves. The regular expression defaults to `\\W+` (or all non-word characters).\n\n[WARNING]\n.Beware of Pathological Regular Expressions\n========================================\n\nThe pattern analyzer uses\nhttps://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html[Java Regular Expressions].\n\nA badly written regular expression could run very slowly or even throw a\nStackOverflowError and cause the node it is running on to exit suddenly.\n\nRead more about https://www.regular-expressions.info/catastrophic.html[pathological regular expressions and how to avoid them].\n\n========================================\n\n[discrete]\n=== Example output\n\n[source,console]\n---------------------------\nPOST _analyze\n{\n  \"analyzer\": \"pattern\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n---------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"the\",\n      \"start_offset\": 0,\n      \"end_offset\": 3,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"2\",\n      \"start_offset\": 4,\n      \"end_offset\": 5,\n      \"type\": \"word\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"quick\",\n      \"start_offset\": 6,\n      \"end_offset\": 11,\n      \"type\": \"word\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"brown\",\n      \"start_offset\": 12,\n      \"end_offset\": 17,\n      \"type\": \"word\",\n      \"position\": 3\n    },\n    {\n      \"token\": \"foxes\",\n      \"start_offset\": 18,\n      \"end_offset\": 23,\n      \"type\": \"word\",\n      \"position\": 4\n    },\n    {\n      \"token\": \"jumped\",\n      \"start_offset\": 24,\n      \"end_offset\": 30,\n      \"type\": \"word\",\n      \"position\": 5\n    },\n    {\n      \"token\": \"over\",\n      \"start_offset\": 31,\n      \"end_offset\": 35,\n      \"type\": \"word\",\n      \"position\": 6\n    },\n    {\n      \"token\": \"the\",\n      \"start_offset\": 36,\n      \"end_offset\": 39,\n      \"type\": \"word\",\n      \"position\": 7\n    },\n    {\n      \"token\": \"lazy\",\n      \"start_offset\": 40,\n      \"end_offset\": 44,\n      \"type\": \"word\",\n      \"position\": 8\n    },\n    {\n      \"token\": \"dog\",\n      \"start_offset\": 45,\n      \"end_offset\": 48,\n      \"type\": \"word\",\n      \"position\": 9\n    },\n    {\n      \"token\": \"s\",\n      \"start_offset\": 49,\n      \"end_offset\": 50,\n      \"type\": \"word\",\n      \"position\": 10\n    },\n    {\n      \"token\": \"bone\",\n      \"start_offset\": 51,\n      \"end_offset\": 55,\n      \"type\": \"word\",\n      \"position\": 11\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe above sentence would produce the following terms:\n\n[source,text]\n---------------------------\n[ the, 2, quick, brown, foxes, jumped, over, the, lazy, dog, s, bone ]\n---------------------------\n\n[discrete]\n=== Configuration\n\nThe `pattern` analyzer accepts the following parameters:\n\n[horizontal]\n`pattern`::\n\n    A https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html[Java regular expression], defaults to `\\W+`.\n\n`flags`::\n\n    Java regular expression https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html#field.summary[flags].\n    Flags should be pipe-separated, eg `\"CASE_INSENSITIVE|COMMENTS\"`.\n\n`lowercase`::\n\n    Should terms be lowercased or not. Defaults to `true`.\n\n`stopwords`::\n\n    A pre-defined stop words list like `_english_` or an array containing a\n    list of stop words. Defaults to `_none_`.\n\n`stopwords_path`::\n\n    The path to a file containing stop words.\n\nSee the <<analysis-stop-tokenfilter,Stop Token Filter>> for more information\nabout stop word configuration.\n\n\n[discrete]\n=== Example configuration\n\nIn this example, we configure the `pattern` analyzer to split email addresses\non non-word characters or on underscores (`\\W|_`), and to lower-case the result:\n\n[source,console]\n----------------------------\nPUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_email_analyzer\": {\n          \"type\":      \"pattern\",\n          \"pattern\":   \"\\\\W|_\", <1>\n          \"lowercase\": true\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_email_analyzer\",\n  \"text\": \"John_Smith@foo-bar.com\"\n}\n----------------------------\n\n<1> The backslashes in the pattern need to be escaped when specifying the\n    pattern as a JSON string.\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"john\",\n      \"start_offset\": 0,\n      \"end_offset\": 4,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"smith\",\n      \"start_offset\": 5,\n      \"end_offset\": 10,\n      \"type\": \"word\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"foo\",\n      \"start_offset\": 11,\n      \"end_offset\": 14,\n      \"type\": \"word\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"bar\",\n      \"start_offset\": 15,\n      \"end_offset\": 18,\n      \"type\": \"word\",\n      \"position\": 3\n    },\n    {\n      \"token\": \"com\",\n      \"start_offset\": 19,\n      \"end_offset\": 22,\n      \"type\": \"word\",\n      \"position\": 4\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe above example produces the following terms:\n\n[source,text]\n---------------------------\n[ john, smith, foo, bar, com ]\n---------------------------\n\n[discrete]\n==== CamelCase tokenizer\n\nThe following more complicated example splits CamelCase text into tokens:\n\n[source,console]\n--------------------------------------------------\nPUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"camel\": {\n          \"type\": \"pattern\",\n          \"pattern\": \"([^\\\\p{L}\\\\d]+)|(?<=\\\\D)(?=\\\\d)|(?<=\\\\d)(?=\\\\D)|(?<=[\\\\p{L}&&[^\\\\p{Lu}]])(?=\\\\p{Lu})|(?<=\\\\p{Lu})(?=\\\\p{Lu}[\\\\p{L}&&[^\\\\p{Lu}]])\"\n        }\n      }\n    }\n  }\n}\n\nGET my-index-000001/_analyze\n{\n  \"analyzer\": \"camel\",\n  \"text\": \"MooseX::FTPClass2_beta\"\n}\n--------------------------------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"moose\",\n      \"start_offset\": 0,\n      \"end_offset\": 5,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"x\",\n      \"start_offset\": 5,\n      \"end_offset\": 6,\n      \"type\": \"word\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"ftp\",\n      \"start_offset\": 8,\n      \"end_offset\": 11,\n      \"type\": \"word\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"class\",\n      \"start_offset\": 11,\n      \"end_offset\": 16,\n      \"type\": \"word\",\n      \"position\": 3\n    },\n    {\n      \"token\": \"2\",\n      \"start_offset\": 16,\n      \"end_offset\": 17,\n      \"type\": \"word\",\n      \"position\": 4\n    },\n    {\n      \"token\": \"beta\",\n      \"start_offset\": 18,\n      \"end_offset\": 22,\n      \"type\": \"word\",\n      \"position\": 5\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe above example produces the following terms:\n\n[source,text]\n---------------------------\n[ moose, x, ftp, class, 2, beta ]\n---------------------------\n\nThe regex above is easier to understand as:\n\n[source,regex]\n--------------------------------------------------\n  ([^\\p{L}\\d]+)                 # swallow non letters and numbers,\n| (?<=\\D)(?=\\d)                 # or non-number followed by number,\n| (?<=\\d)(?=\\D)                 # or number followed by non-number,\n| (?<=[ \\p{L} && [^\\p{Lu}]])    # or lower case\n  (?=\\p{Lu})                    #   followed by upper case,\n| (?<=\\p{Lu})                   # or upper case\n  (?=\\p{Lu}                     #   followed by upper case\n    [\\p{L}&&[^\\p{Lu}]]          #   then lower case\n  )\n--------------------------------------------------\n\n[discrete]\n=== Definition\n\nThe `pattern` analyzer consists of:\n\nTokenizer::\n* <<analysis-pattern-tokenizer,Pattern Tokenizer>>\n\nToken Filters::\n*  <<analysis-lowercase-tokenfilter,Lower Case Token Filter>>\n*  <<analysis-stop-tokenfilter,Stop Token Filter>> (disabled by default)\n\nIf you need to customize the `pattern` analyzer beyond the configuration\nparameters then you need to recreate it as a `custom` analyzer and modify\nit, usually by adding token filters. This would recreate the built-in\n`pattern` analyzer and you can use it as a starting point for further\ncustomization:\n\n[source,console]\n----------------------------------------------------\nPUT /pattern_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"tokenizer\": {\n        \"split_on_non_word\": {\n          \"type\":       \"pattern\",\n          \"pattern\":    \"\\\\W+\" <1>\n        }\n      },\n      \"analyzer\": {\n        \"rebuilt_pattern\": {\n          \"tokenizer\": \"split_on_non_word\",\n          \"filter\": [\n            \"lowercase\"       <2>\n          ]\n        }\n      }\n    }\n  }\n}\n----------------------------------------------------\n// TEST[s/\\n$/\\nstartyaml\\n  - compare_analyzers: {index: pattern_example, first: pattern, second: rebuilt_pattern}\\nendyaml\\n/]\n<1> The default pattern is `\\W+` which splits on non-word characters\nand this is where you'd change it.\n<2> You'd add other token filters after `lowercase`.\n"
}