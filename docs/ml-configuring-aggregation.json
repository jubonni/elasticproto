{
    "meta": {
        "size": 15370,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-configuring-aggregation.html",
        "type": "documentation",
        "role": [
            "xpack"
        ],
        "has_code": true,
        "title": "ml-configuring-aggregation",
        "version": "8.15"
    },
    "doc": "[role=\"xpack\"]\n[[ml-configuring-aggregation]]\n= Aggregating data for faster performance\n\nWhen you aggregate data, {es} automatically distributes the calculations across \nyour cluster. Then you can feed this aggregated data into the {ml-features} \ninstead of raw results. It reduces the volume of data that must be analyzed.\n\n\n[discrete]\n[[aggs-requs-dfeeds]]\n== Requirements\n\nThere are a number of requirements for using aggregations in {dfeeds}.\n\n[discrete]\n[[aggs-aggs]]\n=== Aggregations\n\n* Your aggregation must include a `date_histogram` aggregation or a top level \n`composite` aggregation, which in turn must contain a `max` aggregation on the \ntime field. It ensures that the aggregated data is a time series and the \ntimestamp of each bucket is the time of the last record in the bucket.\n\n* The `time_zone` parameter in the date histogram aggregation must be set to\n`UTC`, which is the default value.\n\n* The name of the aggregation and the name of the field that it operates on need \nto match. For example, if you use a `max` aggregation on a time field called \n`responsetime`, the name of the aggregation must also be `responsetime`.\n\n* For `composite` aggregation support, there must be exactly one \n`date_histogram` value source. That value source must not be sorted in \ndescending order. Additional `composite` aggregation value sources are allowed, \nsuch as `terms`.\n\n* The `size` parameter of the non-composite aggregations must match the \ncardinality of your data. A greater value of the `size` parameter increases the \nmemory requirement of the aggregation.\n\n* If you set the `summary_count_field_name` property to a non-null value, the \n{anomaly-job} expects to receive aggregated input. The property must be set to \nthe name of the field that contains the count of raw data points that have been \naggregated. It applies to all detectors in the job.\n\n* The influencers or the partition fields must be included in the aggregation of \nyour {dfeed}, otherwise they are not included in the job analysis. For more \ninformation on influencers, refer to <<ml-ad-influencers>>.\n\n\n[discrete]\n[[aggs-interval]]\n=== Intervals\n\n* The bucket span of your {anomaly-job} must be divisible by the value of the \n`calendar_interval` or `fixed_interval` in your aggregation (with no remainder).\n\n* If you specify a `frequency` for your {dfeed}, it must be divisible by the \n`calendar_interval` or the `fixed_interval`.\n\n* {anomaly-jobs-cap} cannot use `date_histogram` or `composite` aggregations \nwith an interval measured in months because the length of the month is not \nfixed; they can use weeks or smaller units.\n\n\n[discrete]\n[[aggs-limits-dfeeds]]\n== Limitations\n\n* If your <<aggs-dfeeds,{dfeed} uses aggregations with nested `terms` aggs>> and\nmodel plot is not enabled for the {anomaly-job}, neither the \n**Single Metric Viewer** nor the **Anomaly Explorer** can plot and display an \nanomaly chart. In these cases, an explanatory message is shown instead of the \nchart.\n\n* Your {dfeed} can contain multiple aggregations, but only the ones with names\nthat match values in the job configuration are fed to the job.\n\n* Using \n{ref}/search-aggregations-metrics-scripted-metric-aggregation.html[scripted metric]\naggregations is not supported in {dfeeds}.\n\n\n[discrete]\n[[aggs-recommendations-dfeeds]]\n== Recommendations\n\n* When your detectors use <<ml-metric-functions,metric>> or \n<<ml-sum-functions,sum>> analytical functions, it's recommended to set the \n`date_histogram` or `composite` aggregation interval to a tenth of the bucket \nspan. This creates finer, more granular time buckets, which are ideal for this \ntype of analysis.\n\n* When your detectors use <<ml-count-functions,count>> or \n<<ml-rare-functions,rare>> functions, set the interval to the same value as the \nbucket span.\n\n* If you have multiple influencers or partition fields or if your field \ncardinality is more than 1000, use \n{ref}/search-aggregations-bucket-composite-aggregation.html[composite aggregations].\n+\n--\nTo determine the cardinality of your data, you can run searches such as:\n\n[source,js]\n--------------------------------------------------\nGET .../_search\n{\n  \"aggs\": {\n    \"service_cardinality\": {\n      \"cardinality\": {\n        \"field\": \"service\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n// NOTCONSOLE\n--\n\n\n[discrete]\n[[aggs-using-date-histogram]]\n== Including aggregations in {anomaly-jobs}\n\nWhen you create or update an {anomaly-job}, you can include aggregated fields in \nthe analysis configuration. In the {dfeed} configuration object, you can define \nthe aggregations.\n\n[source,console]\n----------------------------------\nPUT _ml/anomaly_detectors/kibana-sample-data-flights\n{\n  \"analysis_config\": {\n    \"bucket_span\": \"60m\",\n    \"detectors\": [{\n      \"function\": \"mean\",\n      \"field_name\": \"responsetime\",  <1>\n      \"by_field_name\": \"airline\"  <1>\n    }],\n    \"summary_count_field_name\": \"doc_count\" <2>\n  },\n  \"data_description\": {\n    \"time_field\":\"time\"  <1>\n  },\n  \"datafeed_config\":{\n    \"indices\": [\"kibana-sample-data-flights\"],\n    \"aggregations\": {\n      \"buckets\": {\n        \"date_histogram\": {\n          \"field\": \"time\",\n          \"fixed_interval\": \"360s\",\n          \"time_zone\": \"UTC\"\n        },\n        \"aggregations\": {\n          \"time\": {  <3>\n            \"max\": {\"field\": \"time\"}\n          },\n          \"airline\": {  <4>\n            \"terms\": {\n             \"field\": \"airline\",\n              \"size\": 100\n            },\n            \"aggregations\": {\n              \"responsetime\": {  <5>\n                \"avg\": {\n                  \"field\": \"responsetime\"\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n----------------------------------\n// TEST[skip:setup:farequote_data]\n\n<1> The `airline`, `responsetime`, and `time` fields are aggregations. Only the\naggregated fields defined in the `analysis_config` object are analyzed by the\n{anomaly-job}.\n<2> The `summary_count_field_name` property is set to the `doc_count` field that \nis an aggregated field and contains the count of the aggregated data points.\n<3> The aggregations have names that match the fields that they operate on. The\n`max` aggregation is named `time` and its field also needs to be `time`.\n<4> The `term` aggregation is named `airline` and its field is also named\n`airline`.\n<5> The `avg` aggregation is named `responsetime` and its field is also named\n`responsetime`.\n\nUse the following format to define a `date_histogram` aggregation to bucket by \ntime in your {dfeed}:\n\n[source,js]\n----------------------------------\n\"aggregations\": {\n  [\"bucketing_aggregation\": {\n    \"bucket_agg\": {\n      ...\n    },\n    \"aggregations\": {\n      \"data_histogram_aggregation\": {\n        \"date_histogram\": {\n          \"field\": \"time\",\n        },\n        \"aggregations\": {\n          \"timestamp\": {\n            \"max\": {\n              \"field\": \"time\"\n            }\n          },\n          [,\"<first_term>\": {\n            \"terms\":{...\n            }\n            [,\"aggregations\" : {\n              [<sub_aggregation>]+\n            } ]\n          }]\n        }\n      }\n    }\n  }\n}\n----------------------------------\n// NOTCONSOLE\n\n\n[discrete]\n[[aggs-using-composite]]\n== Composite aggregations\n\nComposite aggregations are optimized for queries that are either `match_all` or \n`range` filters. Use composite aggregations in your {dfeeds} for these cases. \nOther types of queries may cause the `composite` aggregation to be inefficient.\n\nThe following is an example of a job with a {dfeed} that uses a `composite` \naggregation to bucket the metrics based on time and terms:\n\n[source,console]\n----------------------------------\nPUT _ml/anomaly_detectors/kibana-sample-data-flights-composite\n{\n  \"analysis_config\": {\n    \"bucket_span\": \"60m\",\n    \"detectors\": [{\n      \"function\": \"mean\",\n      \"field_name\": \"responsetime\",\n      \"by_field_name\": \"airline\"\n    }],\n    \"summary_count_field_name\": \"doc_count\"\n  },\n  \"data_description\": {\n    \"time_field\":\"time\"\n  },\n  \"datafeed_config\":{\n    \"indices\": [\"kibana-sample-data-flights\"],\n    \"aggregations\": {\n      \"buckets\": {\n        \"composite\": {\n          \"size\": 1000,  <1>\n          \"sources\": [\n            {\n              \"time_bucket\": {  <2>\n                \"date_histogram\": {\n                  \"field\": \"time\",\n                  \"fixed_interval\": \"360s\",\n                  \"time_zone\": \"UTC\"\n                }\n              }\n            },\n            {\n              \"airline\": {  <3>\n                \"terms\": {\n                  \"field\": \"airline\"\n                }\n              }\n            }\n          ]\n        },\n        \"aggregations\": {\n          \"time\": {  <4>\n            \"max\": {\n              \"field\": \"time\"\n            }\n          },\n          \"responsetime\": { <5>\n            \"avg\": {\n              \"field\": \"responsetime\"\n            }\n          }\n        }\n      }\n    }\n  }\n}\n----------------------------------\n<1> The number of resources to use when aggregating the data. A larger `size` \nmeans a faster {dfeed} but more cluster resources are used when searching.\n<2> The required `date_histogram` composite aggregation source. Make sure it\nis named differently than your desired time field.\n<3> Instead of using a regular `term` aggregation, adding a composite\naggregation `term` source with the name `airline` works. Note its name\nis the same as the field.\n<4> The required `max` aggregation whose name is the time field in the\njob analysis config.\n<5> The `avg` aggregation is named `responsetime` and its field is also named\n`responsetime`.\n\n\nUse the following format to define a composite aggregation in your {dfeed}:\n\n[source,js]\n----------------------------------\n\"aggregations\": {\n  \"composite_agg\": {\n    \"sources\": [\n      {\n        \"date_histogram_agg\": {\n          \"field\": \"time\",\n          ...settings...\n        }\n      },\n      ...other valid sources...\n      ],\n      ...composite agg settings...,\n      \"aggregations\": {\n        \"timestamp\": {\n            \"max\": {\n              \"field\": \"time\"\n            }\n          },\n          ...other aggregations...\n          [\n            [,\"aggregations\" : {\n              [<sub_aggregation>]+\n            } ]\n          }]\n      }\n   }\n}\n----------------------------------\n// NOTCONSOLE\n\n\n[discrete]\n[[aggs-dfeeds]]\n== Nested aggregations\n\nYou can also use complex nested aggregations in {dfeeds}.\n\nThe next example uses the\n{ref}/search-aggregations-pipeline-derivative-aggregation.html[`derivative` pipeline aggregation] \nto find the first order derivative of the counter `system.network.out.bytes` for \neach value of the field `beat.name`.\n\nNOTE: `derivative` or other pipeline aggregations may not work within \n`composite` aggregations. See\n{ref}/search-aggregations-bucket-composite-aggregation.html#search-aggregations-bucket-composite-aggregation-pipeline-aggregations[composite aggregations and pipeline aggregations].\n\n[source,js]\n----------------------------------\n\"aggregations\": {\n  \"beat.name\": {\n    \"terms\": {\n      \"field\": \"beat.name\"\n    },\n    \"aggregations\": {\n      \"buckets\": {\n        \"date_histogram\": {\n          \"field\": \"@timestamp\",\n          \"fixed_interval\": \"5m\"\n        },\n        \"aggregations\": {\n          \"@timestamp\": {\n            \"max\": {\n              \"field\": \"@timestamp\"\n            }\n          },\n          \"bytes_out_average\": {\n            \"avg\": {\n              \"field\": \"system.network.out.bytes\"\n            }\n          },\n          \"bytes_out_derivative\": {\n            \"derivative\": {\n              \"buckets_path\": \"bytes_out_average\"\n            }\n          }\n        }\n      }\n    }\n  }\n}\n----------------------------------\n// NOTCONSOLE\n\n\n[discrete]\n[[aggs-single-dfeeds]]\n== Single bucket aggregations\n\nYou can also use single bucket aggregations in {dfeeds}. The following example \nshows two `filter` aggregations, each gathering the number of unique entries for \nthe `error` field.\n\n[source,js]\n----------------------------------\n{\n  \"job_id\":\"servers-unique-errors\",\n  \"indices\": [\"logs-*\"],\n  \"aggregations\": {\n    \"buckets\": {\n      \"date_histogram\": {\n        \"field\": \"time\",\n        \"interval\": \"360s\",\n        \"time_zone\": \"UTC\"\n      },\n      \"aggregations\": {\n        \"time\": {\n          \"max\": {\"field\": \"time\"}\n        }\n        \"server1\": {\n          \"filter\": {\"term\": {\"source\": \"server-name-1\"}},\n          \"aggregations\": {\n            \"server1_error_count\": {\n              \"value_count\": {\n                \"field\": \"error\"\n              }\n            }\n          }\n        },\n        \"server2\": {\n          \"filter\": {\"term\": {\"source\": \"server-name-2\"}},\n          \"aggregations\": {\n            \"server2_error_count\": {\n              \"value_count\": {\n                \"field\": \"error\"\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n----------------------------------\n// NOTCONSOLE\n\n\n[discrete]\n[[aggs-amd-dfeeds]]\n== Using `aggregate_metric_double` field type in {dfeeds}\n\n\nNOTE: It is not currently possible to use `aggregate_metric_double` type fields \nin {dfeeds} without aggregations. \n\nYou can use fields with the \n{ref}/aggregate-metric-double.html[`aggregate_metric_double`] field type in a \n{dfeed} with aggregations. It is required to retrieve the `value_count` of the \n`aggregate_metric_double` filed in an aggregation and then use it as the \n`summary_count_field_name` to provide the correct count that represents the \naggregation value.\n\nIn the following example, `presum` is an `aggregate_metric_double` type field \nthat has all the possible metrics: `[ min, max, sum, value_count ]`. To use an \n`avg` aggregation on this field, you need to perform a `value_count` aggregation \non `presum` and then set the field that contains the aggregated values \n`my_count` as the `summary_count_field_name`: \n\n\n[source,js]\n----------------------------------\n{\n  \"analysis_config\": {\n    \"bucket_span\": \"1h\",\n    \"detectors\": [\n      {\n        \"function\": \"avg\",\n        \"field_name\": \"my_avg\"\n      }\n    ],\n    \"summary_count_field_name\": \"my_count\" <1>\n  },\n  \"data_description\": {\n    \"time_field\": \"timestamp\"\n  },\n  \"datafeed_config\": {\n    \"indices\": [\n      \"my_index\"\n    ],\n    \"datafeed_id\": \"datafeed-id\",\n    \"aggregations\": {\n      \"buckets\": {\n        \"date_histogram\": {\n          \"field\": \"time\",\n          \"fixed_interval\": \"360s\",\n          \"time_zone\": \"UTC\"\n        },\n        \"aggregations\": {\n            \"timestamp\": {  \n                \"max\": {\"field\": \"timestamp\"}\n            },\n            \"my_avg\": {  <2>\n                \"avg\": {\n                    \"field\": \"presum\" \n                }\n             },\n             \"my_count\": { <3>\n                 \"value_count\": {\n                     \"field\": \"presum\" \n                 }\n             }\n          }\n        }\n     }\n  }\n}\n----------------------------------\n// NOTCONSOLE\n\n<1> The field `my_count` is set as the `summary_count_field_name`. This field \ncontains aggregated values from the `presum` `aggregate_metric_double` type \nfield (refer to footnote 3). \n<2> The `avg` aggregation to use on the `presum` `aggregate_metric_double` type \nfield.\n<3> The `value_count` aggregation on the `presum` `aggregate_metric_double` type \nfield. This aggregated field must be set as the `summary_count_field_name` \n(refer to footnote 1) to make it possible to use the `aggregate_metric_double` \ntype field in another aggregation."
}