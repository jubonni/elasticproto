{
    "meta": {
        "timestamp": "2024-11-01T03:07:09.497270",
        "size": 6611,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/inference-apis.html",
        "type": "documentation",
        "role": [
            "xpack"
        ],
        "has_code": true,
        "title": "inference-apis",
        "version": "8.15"
    },
    "doc": "[role=\"xpack\"]\n[[inference-apis]]\n== {infer-cap} APIs\n\nIMPORTANT: The {infer} APIs enable you to use certain services, such as built-in\n{ml} models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure,\nGoogle AI Studio or Hugging Face. For built-in models and models uploaded\nthrough Eland, the {infer} APIs offer an alternative way to use and manage\ntrained models. However, if you do not plan to use the {infer} APIs to use these\nmodels or if you want to use non-NLP models, use the\n<<ml-df-trained-models-apis>>.\n\nThe {infer} APIs enable you to create {infer} endpoints and use {ml} models of\ndifferent providers - such as Amazon Bedrock, Anthropic, Azure AI Studio,\nCohere, Google AI, Mistral, OpenAI, or HuggingFace - as a service. Use\nthe following APIs to manage {infer} models and perform {infer}:\n\n* <<delete-inference-api>>\n* <<get-inference-api>>\n* <<post-inference-api>>\n* <<put-inference-api>>\n* <<stream-inference-api>>\n* <<update-inference-api>>\n\n[[inference-landscape]]\n.A representation of the Elastic inference landscape\nimage::images/inference-landscape.jpg[A representation of the Elastic inference landscape,align=\"center\"]\n\nAn {infer} endpoint enables you to use the corresponding {ml} model without\nmanual deployment and apply it to your data at ingestion time through\n<<semantic-search-semantic-text, semantic text>>. \n\nChoose a model from your provider or use ELSER \u2013 a retrieval model trained by \nElastic \u2013, then create an {infer} endpoint by the <<put-inference-api>>.\nNow use <<semantic-search-semantic-text, semantic text>> to perform\n<<semantic-search, semantic search>> on your data.\n\n//[discrete]\n//[[default-enpoints]]\n//=== Default {infer} endpoints\n\n//Your {es} deployment contains some preconfigured {infer} endpoints that makes it easier for you to use them when defining `semantic_text` fields or {infer} processors.\n//The following list contains the default {infer} endpoints listed by `inference_id`:\n\n//* `.elser-2-elasticsearch`: uses the {ml-docs}/ml-nlp-elser.html[ELSER] built-in trained model for `sparse_embedding` tasks (recommended for English language texts)\n//* `.multilingual-e5-small-elasticsearch`: uses the {ml-docs}/ml-nlp-e5.html[E5] built-in trained model for `text_embedding` tasks (recommended for non-English language texts)\n\n//Use the `inference_id` of the endpoint in a <<semantic-text,`semantic_text`>> field definition or when creating an <<inference-processor,{infer} processor>>.\n//The API call will automatically download and deploy the model which might take a couple of minutes.\n//Default {infer} enpoints have {ml-docs}/ml-nlp-auto-scale.html#nlp-model-adaptive-allocations[adaptive allocations] enabled.\n//For these models, the minimum number of allocations is `0`. \n//If there is no {infer} activity that uses the endpoint, the number of allocations will scale down to `0` automatically after 15 minutes.\n\n\n[discrete]\n[[infer-chunking-config]]\n=== Configuring chunking\n\n{infer-cap} endpoints have a limit on the amount of text they can process at once, determined by the model's input capacity.\nChunking is the process of splitting the input text into pieces that remain within these limits.\nIt occurs when ingesting documents into <<semantic-text,`semantic_text` fields>>.\nChunking also helps produce sections that are digestible for humans.\nReturning a long document in search results is less useful than providing the most relevant chunk of text.\n\nEach chunk will include the text subpassage and the corresponding embedding generated from it.\n\nBy default, documents are split into sentences and grouped in sections up to 250 words with 1 sentence overlap so that each chunk shares a sentence with the previous chunk.\nOverlapping ensures continuity and prevents vital contextual information in the input text from being lost by a hard break. \n\n{es} uses the https://unicode-org.github.io/icu-docs/[ICU4J] library to detect word and sentence boundaries for chunking.\nhttps://unicode-org.github.io/icu/userguide/boundaryanalysis/#word-boundary[Word boundaries] are identified by following a series of rules, not just the presence of a whitespace character.\nFor written languages that do use whitespace such as Chinese or Japanese dictionary lookups are used to detect word boundaries.\n\n\n[discrete]\n==== Chunking strategies\n\nTwo strategies are available for chunking: `sentence` and `word`.\n\nThe `sentence` strategy splits the input text at sentence boundaries.\nEach chunk contains one or more complete sentences ensuring that the integrity of sentence-level context is preserved, except when a sentence causes a chunk to exceed a word count of `max_chunk_size`, in which case it will be split across chunks.\nThe `sentence_overlap` option defines the number of sentences from the previous chunk to include in the current chunk which is either `0` or `1`.\n\nThe `word` strategy splits the input text on individual words up to the `max_chunk_size` limit.\nThe `overlap` option is the number of words from the previous chunk to include in the current chunk.\n\nThe default chunking strategy is `sentence`.\n\nNOTE: The default chunking strategy for {infer} endpoints created before 8.16 is `word`.\n\n\n[discrete]\n==== Example of configuring the chunking behavior\n\nThe following example creates an {infer} endpoint with the `elasticsearch` service that deploys the ELSER model by default and configures the chunking behavior.\n\n[source,console]\n------------------------------------------------------------\nPUT _inference/sparse_embedding/small_chunk_size\n{\n  \"service\": \"elasticsearch\",\n  \"service_settings\": {\n    \"num_allocations\": 1,\n    \"num_threads\": 1\n  },\n  \"chunking_settings\": {\n    \"strategy\": \"sentence\",\n    \"max_chunk_size\": 100,\n    \"sentence_overlap\": 0\n  }\n}\n------------------------------------------------------------\n// TEST[skip:TBD]\n\n\ninclude::delete-inference.asciidoc[]\ninclude::get-inference.asciidoc[]\ninclude::post-inference.asciidoc[]\ninclude::put-inference.asciidoc[]\ninclude::stream-inference.asciidoc[]\ninclude::update-inference.asciidoc[]\ninclude::service-alibabacloud-ai-search.asciidoc[]\ninclude::service-amazon-bedrock.asciidoc[]\ninclude::service-anthropic.asciidoc[]\ninclude::service-azure-ai-studio.asciidoc[]\ninclude::service-azure-openai.asciidoc[]\ninclude::service-cohere.asciidoc[]\ninclude::service-elasticsearch.asciidoc[]\ninclude::service-elser.asciidoc[]\ninclude::service-google-ai-studio.asciidoc[]\ninclude::service-google-vertex-ai.asciidoc[]\ninclude::service-hugging-face.asciidoc[]\ninclude::service-mistral.asciidoc[]\ninclude::service-openai.asciidoc[]\ninclude::service-watsonx-ai.asciidoc[]\n"
}