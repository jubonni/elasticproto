{
    "meta": {
        "timestamp": "2024-11-01T03:07:09.409271",
        "size": 11752,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/hotspotting.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "hotspotting",
        "version": "8.15"
    },
    "doc": "[[hotspotting]]\n=== Hot spotting\n++++\n<titleabbrev>Hot spotting</titleabbrev>\n++++\n:keywords: hot-spotting, hotspot, hot-spot, hot spot, hotspots, hotspotting\n\nComputer link:{wikipedia}/Hot_spot_(computer_programming)[hot spotting] \nmay occur in {es} when resource utilizations are unevenly distributed across \n<<modules-node,nodes>>. Temporary spikes are not usually considered problematic, but \nongoing significantly unique utilization may lead to cluster bottlenecks \nand should be reviewed. \n\nSee link:https://www.youtube.com/watch?v=Q5ODJ5nIKAM[this video] for a walkthrough of troubleshooting a hot spotting issue.\n\n[discrete]\n[[detect]]\n==== Detect hot spotting\n\nHot spotting most commonly surfaces as significantly elevated \nresource utilization (of `disk.percent`, `heap.percent`, or `cpu`) among a \nsubset of nodes as reported via <<cat-nodes,cat nodes>>. Individual spikes aren't \nnecessarily problematic, but if utilization repeatedly spikes or consistently remains \nhigh over time (for example longer than 30 seconds), the resource may be experiencing problematic \nhot spotting. \n\nFor example, let's show case two separate plausible issues using cat nodes:\n\n[source,console]\n----\nGET _cat/nodes?v&s=master,name&h=name,master,node.role,heap.percent,disk.used_percent,cpu\n----\nPretend this same output pulled twice across five minutes:\n\n[source,console-result]\n----\nname   master node.role heap.percent disk.used_percent cpu\nnode_1 *      hirstm              24                20  95\nnode_2 -      hirstm              23                18  18\nnode_3 -      hirstmv             25                90  10\n----\n// TEST[skip:illustrative response only]\n\nHere we see two significantly unique utilizations: where the master node is at \n`cpu: 95` and a hot node is at `disk.used_percent: 90%`. This would indicate \nhot spotting was occurring on these two nodes, and not necessarily from the same\nroot cause. \n\n[discrete]\n[[causes]]\n==== Causes\n\nHistorically, clusters experience hot spotting mainly as an effect of hardware, \nshard distributions, and/or task load. We'll review these sequentially in order \nof their potentially impacting scope.\n\n[discrete]\n[[causes-hardware]]\n==== Hardware\n\nHere are some common improper hardware setups which may contribute to hot \nspotting:\n\n* Resources are allocated non-uniformly. For example, if one hot node is \ngiven half the CPU of its peers. {es} expects all nodes on a \n<<data-tiers,data tier>> to share the same hardware profiles or \nspecifications.\n\n* Resources are consumed by another service on the host, including other \n{es} nodes. Refer to our <<dedicated-host,dedicated host>> recommendation.\n\n* Resources experience different network or disk throughputs. For example, if one \nnode's I/O is lower than its peers. Refer to \n<<tune-for-indexing-speed,Use faster hardware>> for more information.\n\n* A JVM that has been configured with a heap larger than 31GB. Refer to <<set-jvm-heap-size>> \nfor more information.\n\n* Problematic resources uniquely report <<setup-configuration-memory,memory swapping>>. \n\n[discrete]\n[[causes-shards]]\n==== Shard distributions\n\n{es} indices are divided into one or more link:{wikipedia}/Shard_(database_architecture)[shards] \nwhich can sometimes be poorly distributed. {es} accounts for this by <<modules-cluster,balancing shard counts>> \nacross data nodes. As link:{blog-ref}whats-new-elasticsearch-kibana-cloud-8-6-0[introduced in version 8.6], \n{es} by default also enables <<modules-cluster,desired balancing>> to account for ingest load.\nA node may still experience hot spotting either due to write-heavy indices or by the \noverall shards it's hosting.\n\n[discrete]\n[[causes-shards-nodes]]\n===== Node level\n\nYou can check for shard balancing via <<cat-allocation,cat allocation>>, though as of version \n8.6, <<modules-cluster,desired balancing>> may no longer fully expect to \nbalance shards. Kindly note, both methods may temporarily show problematic imbalance during \n<<cluster-fault-detection,cluster stability issues>>.\n\nFor example, let's showcase two separate plausible issues using cat allocation:\n\n[source,console]\n----\nGET _cat/allocation?v&s=node&h=node,shards,disk.percent,disk.indices,disk.used\n----\n\nWhich could return:\n\n[source,console-result]\n----\nnode   shards disk.percent disk.indices disk.used\nnode_1    446           19      154.8gb   173.1gb\nnode_2     31           52       44.6gb   372.7gb\nnode_3    445           43      271.5gb   289.4gb\n----\n// TEST[skip:illustrative response only]\n\nHere we see two significantly unique situations. `node_2` has recently\nrestarted, so it has a much lower number of shards than all other nodes. This\nalso relates to `disk.indices` being much smaller than `disk.used` while shards\nare recovering as seen via <<cat-recovery,cat recovery>>. While `node_2`'s shard\ncount is low, it may become a write hot spot due to ongoing <<ilm-rollover,ILM\nrollovers>>. This is a common root cause of write hot spots covered in the next\nsection.\n\nThe second situation is that `node_3` has a higher `disk.percent` than `node_1`,\neven though they hold roughly the same number of shards. This occurs when either\nshards are not evenly sized (refer to <<shard-size-recommendation>>) or when\nthere are a lot of empty indices.\n\nCluster rebalancing based on desired balance does much of the heavy lifting \nof keeping nodes from hot spotting. It can be limited by either nodes hitting \n<<disk-based-shard-allocation,watermarks>> (refer to <<fix-watermark-errors,fixing disk watermark errors>>) or by a \nwrite-heavy index's total shards being much lower than the written-to nodes. \n\nYou can confirm hot spotted nodes via <<cluster-nodes-stats,the nodes stats API>>, \npotentially polling twice over time to only checking for the stats differences \nbetween them rather than polling once giving you stats for the node's \nfull <<cluster-nodes-usage,node uptime>>. For example, to check all nodes \nindexing stats:\n\n[source,console]\n----\nGET _nodes/stats?human&filter_path=nodes.*.name,nodes.*.indices.indexing\n----\n\n[discrete]\n[[causes-shards-index]]\n===== Index level\n\nHot spotted nodes frequently surface via <<cat-thread-pool,cat thread pool>>'s \n`write` and `search` queue backups. For example:\n\n[source,console]\n----\nGET _cat/thread_pool/write,search?v=true&s=n,nn&h=n,nn,q,a,r,c\n----\n\nWhich could return:\n\n[source,console-result]\n----\nn      nn       q a r    c\nsearch node_1   3 1 0 1287\nsearch node_2   0 2 0 1159\nsearch node_3   0 1 0 1302\nwrite  node_1 100 3 0 4259\nwrite  node_2   0 4 0  980\nwrite  node_3   1 5 0 8714\n----\n// TEST[skip:illustrative response only]\n\nHere you can see two significantly unique situations. Firstly, `node_1` has a\nseverely backed up write queue compared to other nodes. Secondly, `node_3` shows\nhistorically completed writes that are double any other node. These are both\nprobably due to either poorly distributed write-heavy indices, or to multiple\nwrite-heavy indices allocated to the same node. Since primary and replica writes\nare majorly the same amount of cluster work, we usually recommend setting\n<<total-shards-per-node,`index.routing.allocation.total_shards_per_node`>> to\nforce index spreading after lining up index shard counts to total nodes. \n\nWe normally recommend heavy-write indices have sufficient primary\n`number_of_shards` and replica `number_of_replicas` to evenly spread across\nindexing nodes. Alternatively, you can <<cluster-reroute,reroute>> shards to\nmore quiet nodes to alleviate the nodes with write hot spotting. \n\nIf it's non-obvious what indices are problematic, you can introspect further via \n<<indices-stats,the index stats API>> by running:\n\n[source,console]\n----\nGET _stats?level=shards&human&expand_wildcards=all&filter_path=indices.*.total.indexing.index_total\n----\n\nFor more advanced analysis, you can poll for shard-level stats, \nwhich lets you compare joint index-level and node-level stats. This analysis \nwouldn't account for node restarts and/or shards rerouting, but serves as \noverview:\n\n[source,console]\n----\nGET _stats/indexing,search?level=shards&human&expand_wildcards=all\n----\n\nYou can for example use the link:https://stedolan.github.io/jq[third-party JQ tool], \nto process the output saved as `indices_stats.json`:\n\n[source,sh]\n----\ncat indices_stats.json | jq -rc ['.indices|to_entries[]|.key as $i|.value.shards|to_entries[]|.key as $s|.value[]|{node:.routing.node[:4], index:$i, shard:$s, primary:.routing.primary, size:.store.size, total_indexing:.indexing.index_total, time_indexing:.indexing.index_time_in_millis, total_query:.search.query_total, time_query:.search.query_time_in_millis } | .+{ avg_indexing: (if .total_indexing>0 then (.time_indexing/.total_indexing|round) else 0 end), avg_search: (if .total_search>0 then (.time_search/.total_search|round) else 0 end) }'] > shard_stats.json\n\n# show top written-to shard simplified stats which contain their index and node references\ncat shard_stats.json | jq -rc 'sort_by(-.avg_indexing)[]' | head\n----\n\n[discrete]\n[[causes-tasks]]\n==== Task loads\n\nShard distribution problems will most-likely surface as task load as seen \nabove in the <<cat-thread-pool,cat thread pool>> example. It is also\npossible for tasks to hot spot a node either due to \nindividual qualitative expensiveness or overall quantitative traffic loads. \n\nFor example, if <<cat-thread-pool,cat thread pool>> reported a high \nqueue on the `warmer` <<modules-threadpool,thread pool>>, you would \nlook-up the effected node's <<cluster-nodes-hot-threads,hot threads>>. \nLet's say it reported `warmer` threads at `100% cpu` related to \n`GlobalOrdinalsBuilder`. This would let you know to inspect  \n<<eager-global-ordinals,field data's global ordinals>>. \n\nAlternatively, let's say <<cat-nodes,cat nodes>> shows a hot spotted master node\nand <<cat-thread-pool,cat thread pool>> shows general queuing across nodes. \nThis would suggest the master node is overwhelmed. To resolve \nthis, first ensure <<high-availability-cluster-small-clusters,hardware high availability>> \nsetup and then look to ephemeral causes. In this example, \n<<cluster-nodes-hot-threads,the nodes hot threads API>> reports multiple threads in \n`other` which indicates they're waiting on or blocked by either garbage collection \nor I/O.\n\nFor either of these example situations, a good way to confirm the problematic tasks \nis to look at longest running non-continuous (designated `[c]`) tasks via \n<<cat-tasks,cat task management>>. This can be supplemented checking longest \nrunning cluster sync tasks via <<cat-pending-tasks,cat pending tasks>>. Using  \na third example,\n\n[source,console]\n----\nGET _cat/tasks?v&s=time:desc&h=type,action,running_time,node,cancellable\n----\n\nThis could return:\n\n[source,console-result]\n----\ntype   action                running_time  node    cancellable\ndirect indices:data/read/eql 10m           node_1  true\n...\n----\n// TEST[skip:illustrative response only]\n\nThis surfaces a problematic <<eql-search-api,EQL query>>. We can gain \nfurther insight on it via <<tasks,the task management API>>,\n\n[source,console]\n----\nGET _tasks?human&detailed\n----\n\nIts response contains a `description` that reports this query:\n\n[source,eql]\n----\nindices[winlogbeat-*,logs-window*], sequence by winlog.computer_name with maxspan=1m\\n\\n[authentication where host.os.type == \"windows\" and event.action:\"logged-in\" and\\n event.outcome == \"success\" and process.name == \"svchost.exe\" ] by winlog.event_data.TargetLogonId\n----\n\nThis lets you know which indices to check (`winlogbeat-*,logs-window*`), as well \nas the <<eql-search-api,EQL search>> request body. Most likely this is \nlink:{security-guide}/es-overview.html[SIEM related]. \nYou can combine this with <<enable-audit-logging,audit logging>> as needed to \ntrace the request source.\n"
}