{
    "meta": {
        "timestamp": "2024-11-01T03:07:10.324272",
        "size": 8443,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/start-trained-model-deployment.html",
        "type": "documentation",
        "role": [
            "xpack"
        ],
        "has_code": true,
        "title": "start-trained-model-deployment",
        "version": "8.15"
    },
    "doc": "[role=\"xpack\"]\n[[start-trained-model-deployment]]\n= Start trained model deployment API\n[subs=\"attributes\"]\n++++\n<titleabbrev>Start trained model deployment</titleabbrev>\n++++\n\nStarts a new trained model deployment.\n\n[[start-trained-model-deployment-request]]\n== {api-request-title}\n\n`POST _ml/trained_models/<model_id>/deployment/_start`\n\n[[start-trained-model-deployment-prereq]]\n== {api-prereq-title}\nRequires the `manage_ml` cluster privilege. This privilege is included in the\n`machine_learning_admin` built-in role.\n\n[[start-trained-model-deployment-desc]]\n== {api-description-title}\n\nCurrently only `pytorch` models are supported for deployment. Once deployed\nthe model can be used by the <<inference-processor,{infer-cap} processor>>\nin an ingest pipeline or directly in the <<infer-trained-model>> API.\n\nA model can be deployed multiple times by using deployment IDs. A deployment ID\nmust be unique and should not match any other deployment ID or model ID, unless\nit is the same as the ID of the model being deployed. If `deployment_id` is not\nset, it defaults to the `model_id`.\n\nYou can enable adaptive allocations to automatically scale model allocations up\nand down based on the actual resource requirement of the processes.\n\nManually scaling inference performance can be achieved by setting the parameters\n`number_of_allocations` and `threads_per_allocation`.\n\nIncreasing `threads_per_allocation` means more threads are used when an\ninference request is processed on a node. This can improve inference speed for\ncertain models. It may also result in improvement to throughput.\n\nIncreasing `number_of_allocations` means more threads are used to process\nmultiple inference requests in parallel resulting in throughput improvement.\nEach model allocation uses a number of threads defined by\n`threads_per_allocation`.\n\nModel allocations are distributed across {ml} nodes. All allocations assigned to\na node share the same copy of the model in memory. To avoid thread\noversubscription which is detrimental to performance, model allocations are\ndistributed in such a way that the total number of used threads does not surpass\nthe node's allocated processors.\n\n[[start-trained-model-deployment-path-params]]\n== {api-path-parms-title}\n\n`<model_id>`::\n(Required, string)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=model-id]\n\n[[start-trained-model-deployment-query-params]]\n== {api-query-parms-title}\n\n`deployment_id`::\n(Optional, string)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=deployment-id]\n+\n--\nDefaults to `model_id`.\n--\n\n`timeout`::\n(Optional, time)\nControls the amount of time to wait for the model to deploy. Defaults to 30\nseconds.\n\n`wait_for`::\n(Optional, string)\nSpecifies the allocation status to wait for before returning. Defaults to\n`started`. The value `starting` indicates deployment is starting but not yet on\nany node. The value `started` indicates the model has started on at least one\nnode. The value `fully_allocated` indicates the deployment has started on all\nvalid nodes.\n\n[[start-trained-model-deployment-request-body]]\n== {api-request-body-title}\n\n`adaptive_allocations`::\n(Optional, object)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation]\n\n`enabled`:::\n(Optional, Boolean)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation-enabled]\n\n`max_number_of_allocations`:::\n(Optional, integer)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation-max-number]\n\n`min_number_of_allocations`:::\n(Optional, integer)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation-min-number]\n\n`cache_size`::\n(Optional, <<byte-units,byte value>>)\nThe inference cache size (in memory outside the JVM heap) per node for the\nmodel. In serverless, the cache is disabled by default. Otherwise, the default value is the size of the model as reported by the\n`model_size_bytes` field in the <<get-trained-models-stats>>. To disable the\ncache, `0b` can be provided.\n\n`number_of_allocations`::\n(Optional, integer)\nThe total number of allocations this model is assigned across {ml} nodes.\nIncreasing this value generally increases the throughput. Defaults to `1`.\nIf `adaptive_allocations` is enabled, do not set this value, because it's automatically set.\n\n`priority`::\n(Optional, string)\nThe priority of the deployment. The default value is `normal`.\nThere are two priority settings:\n+\n--\n* `normal`: Use this for deployments in production. The deployment allocations\nare distributed so that node processors are not oversubscribed.\n* `low`: Use this for testing model functionality. The intention is that these\ndeployments are not sent a high volume of input. The deployment is required to\nhave a single allocation with just one thread. Low priority deployments may be\nassigned on nodes that already utilize all their processors but will be given a\nlower CPU priority than normal deployments. Low priority deployments may be\nunassigned in order to satisfy more allocations of normal priority deployments.\n--\n\nWARNING: Heavy usage of low priority deployments may impact performance of\nnormal priority deployments.\n\n`queue_capacity`::\n(Optional, integer)\nControls how many inference requests are allowed in the queue at a time.\nEvery machine learning node in the cluster where the model can be allocated\nhas a queue of this size; when the number of requests exceeds the total value,\nnew requests are rejected with a 429 error. Defaults to 1024. Max allowed value\nis 1000000.\n\n`threads_per_allocation`::\n(Optional, integer)\nSets the number of threads used by each model allocation during inference. This\ngenerally increases the speed per inference request. The inference process is a\ncompute-bound process; `threads_per_allocations` must not exceed the number of\navailable allocated processors per node. Defaults to 1. Must be a power of 2.\nMax allowed value is 32.\n\n\n[[start-trained-model-deployment-example]]\n== {api-examples-title}\n\nThe following example starts a new deployment for a\n`elastic__distilbert-base-uncased-finetuned-conll03-english` trained model:\n\n[source,console]\n--------------------------------------------------\nPOST _ml/trained_models/elastic__distilbert-base-uncased-finetuned-conll03-english/deployment/_start?wait_for=started&timeout=1m\n--------------------------------------------------\n// TEST[skip:TBD]\n\nThe API returns the following results:\n\n[source,console-result]\n----\n{\n    \"assignment\": {\n        \"task_parameters\": {\n            \"model_id\": \"elastic__distilbert-base-uncased-finetuned-conll03-english\",\n            \"model_bytes\": 265632637,\n            \"threads_per_allocation\" : 1,\n            \"number_of_allocations\" : 1,\n            \"queue_capacity\" : 1024,\n            \"priority\": \"normal\"\n        },\n        \"routing_table\": {\n            \"uckeG3R8TLe2MMNBQ6AGrw\": {\n                \"routing_state\": \"started\",\n                \"reason\": \"\"\n            }\n        },\n        \"assignment_state\": \"started\",\n        \"start_time\": \"2022-11-02T11:50:34.766591Z\"\n    }\n}\n----\n\n\n[[start-trained-model-deployment-deployment-id-example]]\n=== Using deployment IDs\n\nThe following example starts a new deployment for the `my_model` trained model\nwith the ID `my_model_for_ingest`. The deployment ID an be used in {infer} API\ncalls or in {infer} processors.\n\n[source,console]\n--------------------------------------------------\nPOST _ml/trained_models/my_model/deployment/_start?deployment_id=my_model_for_ingest\n--------------------------------------------------\n// TEST[skip:TBD]\n\nThe `my_model` trained model can be deployed again with a different ID:\n\n[source,console]\n--------------------------------------------------\nPOST _ml/trained_models/my_model/deployment/_start?deployment_id=my_model_for_search\n--------------------------------------------------\n// TEST[skip:TBD]\n\n\n[[start-trained-model-deployment-adaptive-allocation-example]]\n=== Setting adaptive allocations\n\nThe following example starts a new deployment of the `my_model` trained model\nwith the ID `my_model_for_search` and enables adaptive allocations with the\nminimum number of 3 allocations and the maximum number of 10. \n\n[source,console]\n--------------------------------------------------\nPOST _ml/trained_models/my_model/deployment/_start?deployment_id=my_model_for_search\n{\n  \"adaptive_allocations\": {\n    \"enabled\": true,\n    \"min_number_of_allocations\": 3,\n    \"max_number_of_allocations\": 10\n  }\n}\n--------------------------------------------------\n// TEST[skip:TBD]"
}