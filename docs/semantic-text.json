{
    "meta": {
        "size": 8845,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/semantic-text.html",
        "type": "documentation",
        "role": [
            "xpack"
        ],
        "has_code": true,
        "title": "semantic-text",
        "version": "8.15"
    },
    "doc": "[role=\"xpack\"]\n[[semantic-text]]\n=== Semantic text field type\n++++\n<titleabbrev>Semantic text</titleabbrev>\n++++\n\nbeta[]\n\nThe `semantic_text` field type automatically generates embeddings for text content using an inference endpoint.\nLong passages are <<auto-text-chunking, automatically chunked>> to smaller sections to enable the processing of larger corpuses of text.\n\nThe `semantic_text` field type specifies an inference endpoint identifier that will be used to generate embeddings.\nYou can create the inference endpoint by using the <<put-inference-api>>.\nThis field type and the <<query-dsl-semantic-query,`semantic` query>> type make it simpler to perform semantic search on your data.\n\nUsing `semantic_text`, you won't need to specify how to generate embeddings for your data, or how to index it.\nThe {infer} endpoint automatically determines the embedding generation, indexing, and query to use.\n\n[source,console]\n------------------------------------------------------------\nPUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"inference_field\": {\n        \"type\": \"semantic_text\",\n        \"inference_id\": \"my-elser-endpoint\"\n      }\n    }\n  }\n}\n------------------------------------------------------------\n// TEST[skip:Requires inference endpoint]\n\n\nThe recommended way to use semantic_text is by having dedicated {infer} endpoints for ingestion and search.\nThis ensures that search speed remains unaffected by ingestion workloads, and vice versa.\nAfter creating dedicated {infer} endpoints for both, you can reference them using the `inference_id` and `search_inference_id` parameters when setting up the index mapping for an index that uses the `semantic_text` field.\n\n[source,console]\n------------------------------------------------------------\nPUT my-index-000002\n{\n  \"mappings\": {\n    \"properties\": {\n      \"inference_field\": {\n        \"type\": \"semantic_text\",\n        \"inference_id\": \"my-elser-endpoint-for-ingest\",\n        \"search_inference_id\": \"my-elser-endpoint-for-search\"\n      }\n    }\n  }\n}\n------------------------------------------------------------\n// TEST[skip:Requires inference endpoint]\n\n\n[discrete]\n[[semantic-text-params]]\n==== Parameters for `semantic_text` fields\n\n`inference_id`::\n(Required, string)\n{infer-cap} endpoint that will be used to generate the embeddings for the field.\nThis parameter cannot be updated.\nUse the <<put-inference-api>> to create the endpoint.\nIf `search_inference_id` is specified, the {infer} endpoint defined by `inference_id` will only be used at index time.\n\n`search_inference_id`::\n(Optional, string)\n{infer-cap} endpoint that will be used to generate embeddings at query time.\nYou can update this parameter by using the <<indices-put-mapping, Update mapping API>>.\nUse the <<put-inference-api>> to create the endpoint.\nIf not specified, the {infer} endpoint defined by `inference_id` will be used at both index and query time.\n\n[discrete]\n[[infer-endpoint-validation]]\n==== {infer-cap} endpoint validation\n\nThe `inference_id` will not be validated when the mapping is created, but when documents are ingested into the index.\nWhen the first document is indexed, the `inference_id` will be used to generate underlying indexing structures for the field.\n\nWARNING: Removing an {infer} endpoint will cause ingestion of documents and semantic queries to fail on indices that define `semantic_text` fields with that {infer} endpoint as their `inference_id`.\nTrying to <<delete-inference-api,delete an {infer} endpoint>> that is used on a `semantic_text` field will result in an error.\n\n\n[discrete]\n[[auto-text-chunking]]\n==== Automatic text chunking\n\n{infer-cap} endpoints have a limit on the amount of text they can process.\nTo allow for large amounts of text to be used in semantic search, `semantic_text` automatically generates smaller passages if needed, called _chunks_.\n\nEach chunk will include the text subpassage and the corresponding embedding generated from it.\nWhen querying, the individual passages will be automatically searched for each document, and the most relevant passage will be used to compute a score.\n\nDocuments are split into 250-word sections with a 100-word overlap so that each section shares 100 words with the previous section.\nThis overlap ensures continuity and prevents vital contextual information in the input text from being lost by a hard break.\n\n\n[discrete]\n[[semantic-text-structure]]\n==== `semantic_text` structure\n\nOnce a document is ingested, a `semantic_text` field will have the following structure:\n\n[source,console-result]\n------------------------------------------------------------\n\"inference_field\": {\n  \"text\": \"these are not the droids you're looking for\", <1>\n  \"inference\": {\n    \"inference_id\": \"my-elser-endpoint\", <2>\n    \"model_settings\": { <3>\n      \"task_type\": \"sparse_embedding\"\n    },\n    \"chunks\": [ <4>\n      {\n        \"text\": \"these are not the droids you're looking for\",\n        \"embeddings\": {\n          (...)\n        }\n      }\n    ]\n  }\n}\n------------------------------------------------------------\n// TEST[skip:TBD]\n<1> The field will become an object structure to accommodate both the original\ntext and the inference results.\n<2> The `inference_id` used to generate the embeddings.\n<3> Model settings, including the task type and dimensions/similarity if\napplicable.\n<4> Inference results will be grouped in chunks, each with its corresponding\ntext and embeddings.\n\nRefer to <<semantic-search-semantic-text,this tutorial>> to learn more about\nsemantic search using `semantic_text` and the `semantic` query.\n\n\n[discrete]\n[[custom-indexing]]\n==== Customizing `semantic_text` indexing\n\n`semantic_text` uses defaults for indexing data based on the {infer} endpoint\nspecified. It enables you to quickstart your semantic search by providing\nautomatic {infer} and a dedicated query so you don't need to provide further\ndetails.\n\nIn case you want to customize data indexing, use the\n<<sparse-vector,`sparse_vector`>> or <<dense-vector,`dense_vector`>> field\ntypes and create an ingest pipeline with an\n<<inference-processor, {infer} processor>> to generate the embeddings.\n<<semantic-search-inference,This tutorial>> walks you through the process. In\nthese cases - when you use `sparse_vector` or `dense_vector` field types instead\nof the `semantic_text` field type to customize indexing - using the \n<<query-dsl-semantic-query,`semantic_query`>> is not supported for querying the \nfield data.\n\n\n[discrete]\n[[update-script]]\n==== Updates to `semantic_text` fields\n\nUpdates that use scripts are not supported for an index contains a `semantic_text` field.\nEven if the script targets non-`semantic_text` fields, the update will fail when the index contains a `semantic_text` field.\n\n\n[discrete]\n[[copy-to-support]]\n==== `copy_to` support\n\nThe `semantic_text` field type can be the target of\n<<copy-to,`copy_to` fields>>. This means you can use a single `semantic_text`\nfield to collect the values of other fields for semantic search. Each value has\nits embeddings calculated separately; each field value is a separate set of chunk(s) in\nthe resulting embeddings.\n\nThis imposes a restriction on bulk requests and ingestion pipelines that update documents with `semantic_text` fields.\nIn these cases, all fields that are copied to a `semantic_text` field, including the `semantic_text` field value, must have a value to ensure every embedding is calculated correctly.\n\nFor example, the following mapping:\n\n[source,console]\n------------------------------------------------------------\nPUT test-index\n{\n    \"mappings\": {\n        \"properties\": {\n            \"infer_field\": {\n                \"type\": \"semantic_text\",\n                \"inference_id\": \"my-elser-endpoint\"\n            },\n            \"source_field\": {\n                \"type\": \"text\",\n                \"copy_to\": \"infer_field\"\n            }\n        }\n    }\n}\n------------------------------------------------------------\n// TEST[skip:TBD]\n\nWill need the following bulk update request to ensure that `infer_field` is updated correctly:\n\n[source,console]\n------------------------------------------------------------\nPUT test-index/_bulk\n{\"update\": {\"_id\": \"1\"}}\n{\"doc\": {\"infer_field\": \"updated inference field\", \"source_field\": \"updated source field\"}}\n------------------------------------------------------------\n// TEST[skip:TBD]\n\nNotice that both the `semantic_text` field and the source field are updated in the bulk request.\n\n\n[discrete]\n[[limitations]]\n==== Limitations\n\n`semantic_text` field types have the following limitations:\n\n* `semantic_text` fields are not currently supported as elements of <<nested,nested fields>>.\n* `semantic_text` fields can't currently be set as part of <<dynamic-templates>>.\n* `semantic_text` fields can't be defined as <<multi-fields,multi-fields>> of another field, nor can they contain other fields as multi-fields.\n"
}