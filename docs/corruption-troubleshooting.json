{
    "meta": {
        "timestamp": "2024-11-01T02:49:24.774065",
        "size": 5988,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/corruption-troubleshooting.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "corruption-troubleshooting",
        "version": "8.15"
    },
    "doc": "[[corruption-troubleshooting]]\n== Troubleshooting corruption\n\n{es} expects that the data it reads from disk is exactly the data it previously\nwrote. If it detects that the data on disk is different from what it wrote then\nit will report some kind of exception such as:\n\n- `org.apache.lucene.index.CorruptIndexException`\n- `org.elasticsearch.gateway.CorruptStateException`\n- `org.elasticsearch.index.translog.TranslogCorruptedException`\n\nTypically these exceptions happen due to a checksum mismatch. Most of the data\nthat {es} writes to disk is followed by a checksum using a simple algorithm\nknown as CRC32 which is fast to compute and good at detecting the kinds of\nrandom corruption that may happen when using faulty storage. A CRC32 checksum\nmismatch definitely indicates that something is faulty, although of course a\nmatching checksum doesn't prove the absence of corruption.\n\nVerifying a checksum is expensive since it involves reading every byte of the\nfile which takes significant effort and might evict more useful data from the\nfilesystem cache, so systems typically don't verify the checksum on a file very\noften. This is why you tend only to encounter a corruption exception when\nsomething unusual is happening. For instance, corruptions are often detected\nduring merges, shard movements, and snapshots. This does not mean that these\nprocesses are causing corruption: they are examples of the rare times where\nreading a whole file is necessary. {es} takes the opportunity to verify the\nchecksum at the same time, and this is when the corruption is detected and\nreported. It doesn't indicate the cause of the corruption or when it happened.\nCorruptions can remain undetected for many months.\n\nThe files that make up a Lucene index are written sequentially from start to\nend and then never modified or overwritten. This access pattern means the\nchecksum computation is very simple and can happen on-the-fly as the file is\ninitially written, and also makes it very unlikely that an incorrect checksum\nis due to a userspace bug at the time the file was written. The part of {es}\nthat computes the checksum is straightforward, widely used, and very\nwell-tested, so you can be very confident that a checksum mismatch really does\nindicate that the data read from disk is different from the data that {es}\npreviously wrote.\n\nIf a file header is corrupted then it's possible that {es} might not be able\nto work out how to even start reading the file which can lead to an exception\nsuch as:\n\n- `org.apache.lucene.index.IndexFormatTooOldException`\n- `org.apache.lucene.index.IndexFormatTooNewException`\n\nIt is also possible that {es} reports a corruption if a file it needs is\nentirely missing, with an exception such as:\n\n- `java.io.FileNotFoundException`\n- `java.nio.file.NoSuchFileException`\n\nThe files that make up a Lucene index are written in full before they are used.\nIf a file is needed to recover an index after a restart then your storage\nsystem previously confirmed to {es} that this file was durably synced to disk.\nOn Linux this means that the `fsync()` system call returned successfully. {es}\nsometimes reports that an index is corrupt because a file needed for recovery\nis missing, or it exists but has been truncated or is missing its footer. This\nmay indicate that your storage system acknowledges durable writes incorrectly.\n\nThere are many possible explanations for {es} detecting corruption in your\ncluster. Databases like {es} generate a challenging I/O workload that may find\nsubtle infrastructural problems which other tests may miss. {es} is known to\nexpose the following problems as file corruptions:\n\n- Filesystem bugs, especially in newer and nonstandard filesystems which might\n  not have seen enough real-world production usage to be confident that they\nwork correctly.\n\n- https://www.elastic.co/blog/canonical-elastic-and-google-team-up-to-prevent-data-corruption-in-linux[Kernel bugs].\n\n- Bugs in firmware running on the drive or RAID controller.\n\n- Incorrect configuration, for instance configuring `fsync()` to report success\n  before all durable writes have completed.\n\n- Faulty hardware, which may include the drive itself, the RAID controller,\n  your RAM or CPU.\n\n- Third-party software which modifies the files that {es} writes.\n\nData corruption typically doesn't result in other evidence of problems apart\nfrom the checksum mismatch. Do not interpret this as an indication that your\nstorage subsystem is working correctly and therefore that {es} itself caused\nthe corruption. It is rare for faulty storage to show any evidence of problems\napart from the data corruption, but data corruption itself is a very strong\nindicator that your storage subsystem is not working correctly.\n\nTo rule out {es} as the source of data corruption, generate an I/O workload\nusing something other than {es} and look for data integrity errors. On Linux\nthe `fio` and `stress-ng` tools can both generate challenging I/O workloads and\nverify the integrity of the data they write. Use version 0.12.01 or newer of\n`stress-ng` since earlier versions do not have strong enough integrity checks.\nVerify that durable writes persist across power outages using a script such as\nhttps://gist.github.com/bradfitz/3172656[`diskchecker.pl`]. Alternatively, use\na tool such as `strace` to observe the sequence of syscalls that {es} makes\nwhen writing data and confirm that this sequence does not explain the reported\ncorruption.\n\nTo narrow down the source of the corruptions, systematically change components\nin your cluster's environment until the corruptions stop. The details will\ndepend on the exact configuration of your environment, but may include the\nfollowing:\n\n- Try a different filesystem or a different kernel.\n\n- Try changing each hardware component in turn, ideally changing to a different\n  model or manufacturer.\n\n- Try different firmware versions for each hardware component.\n\n- Remove any third-party software which may modify the contents of the {es}\n  data path.\n"
}