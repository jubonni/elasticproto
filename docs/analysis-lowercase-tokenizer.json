{
    "meta": {
        "size": 2588,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lowercase-tokenizer.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "analysis-lowercase-tokenizer",
        "version": "8.15"
    },
    "doc": "[[analysis-lowercase-tokenizer]]\n=== Lowercase tokenizer\n++++\n<titleabbrev>Lowercase</titleabbrev>\n++++\n\nThe `lowercase` tokenizer, like the\n<<analysis-letter-tokenizer, `letter` tokenizer>> breaks text into terms\nwhenever it encounters a character which is not a letter, but it also\nlowercases all terms. It is functionally equivalent to the\n<<analysis-letter-tokenizer, `letter` tokenizer>> combined with the\n<<analysis-lowercase-tokenfilter, `lowercase` token filter>>, but is more\nefficient as it performs both steps in a single pass.\n\n\n[discrete]\n=== Example output\n\n[source,console]\n---------------------------\nPOST _analyze\n{\n  \"tokenizer\": \"lowercase\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n---------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"the\",\n      \"start_offset\": 0,\n      \"end_offset\": 3,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"quick\",\n      \"start_offset\": 6,\n      \"end_offset\": 11,\n      \"type\": \"word\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"brown\",\n      \"start_offset\": 12,\n      \"end_offset\": 17,\n      \"type\": \"word\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"foxes\",\n      \"start_offset\": 18,\n      \"end_offset\": 23,\n      \"type\": \"word\",\n      \"position\": 3\n    },\n    {\n      \"token\": \"jumped\",\n      \"start_offset\": 24,\n      \"end_offset\": 30,\n      \"type\": \"word\",\n      \"position\": 4\n    },\n    {\n      \"token\": \"over\",\n      \"start_offset\": 31,\n      \"end_offset\": 35,\n      \"type\": \"word\",\n      \"position\": 5\n    },\n    {\n      \"token\": \"the\",\n      \"start_offset\": 36,\n      \"end_offset\": 39,\n      \"type\": \"word\",\n      \"position\": 6\n    },\n    {\n      \"token\": \"lazy\",\n      \"start_offset\": 40,\n      \"end_offset\": 44,\n      \"type\": \"word\",\n      \"position\": 7\n    },\n    {\n      \"token\": \"dog\",\n      \"start_offset\": 45,\n      \"end_offset\": 48,\n      \"type\": \"word\",\n      \"position\": 8\n    },\n    {\n      \"token\": \"s\",\n      \"start_offset\": 49,\n      \"end_offset\": 50,\n      \"type\": \"word\",\n      \"position\": 9\n    },\n    {\n      \"token\": \"bone\",\n      \"start_offset\": 51,\n      \"end_offset\": 55,\n      \"type\": \"word\",\n      \"position\": 10\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe above sentence would produce the following terms:\n\n[source,text]\n---------------------------\n[ the, quick, brown, foxes, jumped, over, the, lazy, dog, s, bone ]\n---------------------------\n\n[discrete]\n=== Configuration\n\nThe `lowercase` tokenizer is not configurable.\n"
}