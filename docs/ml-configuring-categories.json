{
    "meta": {
        "timestamp": "2024-11-01T03:07:09.684274",
        "size": 12203,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-configuring-categories.html",
        "type": "documentation",
        "role": [
            "xpack",
            "screenshot",
            "screenshot",
            "screenshot"
        ],
        "has_code": true,
        "title": "ml-configuring-categories",
        "version": "8.15"
    },
    "doc": "[role=\"xpack\"]\n[[ml-configuring-categories]]\n= Detecting anomalous categories of data\n\nCategorization is a {ml} process that tokenizes a text field, clusters similar\ndata together, and classifies it into categories. It works best on\nmachine-written messages and application output that typically consist of\nrepeated elements. For example, it works well on logs that contain a finite set\nof possible messages:\n\n//Obtained from it_ops_new_app_logs.json\n[source,js]\n----------------------------------\n{\"@timestamp\":1549596476000,\n\"message\":\"org.jdbi.v2.exceptions.UnableToExecuteStatementException: com.mysql.jdbc.exceptions.MySQLTimeoutException: Statement cancelled due to timeout or client request [statement:\\\"SELECT id, customer_id, name, force_disabled, enabled FROM customers\\\"]\",\n\"type\":\"logs\"}\n----------------------------------\n//NOTCONSOLE\n\nCategorization is tuned to work best on data like log messages by taking token\norder into account, including stop words, and not considering synonyms in its\nanalysis. Complete sentences in human communication or literary text (for\nexample email, wiki pages, prose, or other human-generated content) can be\nextremely diverse in structure. Since categorization is tuned for machine data,\nit gives poor results for human-generated data. It would create so many\ncategories that they couldn't be handled effectively. Categorization is _not_\nnatural language processing (NLP).\n\nWhen you create a categorization {anomaly-job}, the {ml} model learns what\nvolume and pattern is normal for each category over time. You can then detect\nanomalies and surface rare events or unusual types of messages by using\n<<ml-count-functions,count>> or <<ml-rare-functions,rare>> functions.\n\nIn {kib}, there is a categorization wizard to help you create this type of\n{anomaly-job}. For example, the following job generates categories from the\ncontents of the `message` field and uses the count function to determine when\ncertain categories are occurring at anomalous rates:\n\n[role=\"screenshot\"]\nimage::images/ml-category-wizard.jpg[\"Creating a categorization job in Kibana\"]\n\n[%collapsible]\n.API example\n====\n[source,console]\n----------------------------------\nPUT _ml/anomaly_detectors/it_ops_app_logs\n{\n  \"description\" : \"IT ops application logs\",\n  \"analysis_config\" : {\n    \"categorization_field_name\": \"message\",<1>\n    \"bucket_span\":\"30m\",\n    \"detectors\" :[{\n      \"function\":\"count\",\n      \"by_field_name\": \"mlcategory\"<2>\n    }]\n  },\n  \"data_description\" : {\n    \"time_field\":\"@timestamp\"\n  }\n}\n----------------------------------\n// TEST[skip:needs-licence]\n<1> This field is used to derive categories.\n<2> The categories are used in a detector by setting `by_field_name`,\n`over_field_name`, or `partition_field_name` to the keyword `mlcategory`. If you\ndo not specify this keyword in one of those properties, the API request fails.\n====\n\n\nYou can use the **Anomaly Explorer** in {kib} to view the analysis results:\n\n[role=\"screenshot\"]\nimage::images/ml-category-anomalies.jpg[\"Categorization results in the Anomaly Explorer\"]\n\nFor this type of job, the results contain extra information for each anomaly:\nthe name of the category (for example, `mlcategory 2`) and examples of the\nmessages in that category. You can use these details to investigate occurrences\nof unusually high message counts.\n\nIf you use the advanced {anomaly-job} wizard in {kib} or the\n{ref}/ml-put-job.html[create {anomaly-jobs} API], there are additional\nconfiguration options. For example, the optional `categorization_examples_limit`\nproperty specifies the maximum number of examples that are stored in memory and\nin the results data store for each category. The default value is `4`. Note that\nthis setting does not affect the categorization; it just affects the list of\nvisible examples. If you increase this value, more examples are available, but\nyou must have more storage available. If you set this value to `0`, no examples\nare stored.\n\nAnother advanced option is the `categorization_filters` property, which can\ncontain an array of regular expressions. If a categorization field value matches\nthe regular expression, the portion of the field that is matched is not taken\ninto consideration when defining categories. The categorization filters are\napplied in the order they are listed in the job configuration, which enables you\nto disregard multiple sections of the categorization field value. In this\nexample, you might create a filter like `[ \"\\\\[statement:.*\\\\]\"]` to remove the\nSQL statement from the categorization algorithm.\n\n[discrete]\n[[ml-per-partition-categorization]]\n== Per-partition categorization\n\nIf you enable per-partition categorization, categories are determined\nindependently for each partition. For example, if your data includes messages\nfrom multiple types of logs from different applications, you can use a field\nlike the ECS {ecs-ref}/ecs-event.html[`event.dataset` field] as the\n`partition_field_name` and categorize the messages for each type of log\nseparately.\n\nIf your job has multiple detectors, every detector that uses the `mlcategory`\nkeyword must also define a `partition_field_name`. You must use the same\n`partition_field_name` value in all of these detectors. Otherwise, when you\ncreate or update a job and enable per-partition categorization, it fails.\n\nWhen per-partition categorization is enabled, you can also take advantage of a\n`stop_on_warn` configuration option. If the categorization status for a\npartition changes to `warn`, it doesn't categorize well and can cause a lot of\nunnecessary resource usage. When you set `stop_on_warn` to `true`, the job stops\nanalyzing these problematic partitions. You can thus avoid an ongoing\nperformance cost for partitions that are unsuitable for categorization.\n\n[discrete]\n[[ml-configuring-analyzer]]\n== Customizing the categorization analyzer\n\nCategorization uses English dictionary words to identify log message categories.\nBy default, it also uses English tokenization rules. For this reason, if you use\nthe default categorization analyzer, only English language log messages are\nsupported, as described in the <<ml-limitations>>.\n\nIf you use the categorization wizard in {kib}, you can see which categorization\nanalyzer it uses and highlighted examples of the tokens that it identifies. You\ncan also change the tokenization rules by customizing the way the categorization\nfield values are interpreted:\n\n[role=\"screenshot\"]\nimage::images/ml-category-analyzer.jpg[\"Editing the categorization analyzer in Kibana\"]\n\nThe categorization analyzer can refer to a built-in {es} analyzer or a\ncombination of zero or more character filters, a tokenizer, and zero or more\ntoken filters. In this example, adding a\n{ref}/analysis-pattern-replace-charfilter.html[`pattern_replace` character filter]\nachieves exactly the same behavior as the `categorization_filters` job\nconfiguration option described earlier. For more details about these properties,\nsee the\n{ref}/ml-put-job.html#ml-put-job-request-body[`categorization_analyzer` API object].\n\nIf you use the default categorization analyzer in {kib} or omit the\n`categorization_analyzer` property from the API, the following default values\nare used:\n\n[source,console]\n--------------------------------------------------\nPOST _ml/anomaly_detectors/_validate\n{\n  \"analysis_config\" : {\n    \"categorization_analyzer\" : {\n      \"char_filter\" : [\n        \"first_line_with_letters\"\n      ],\n      \"tokenizer\" : \"ml_standard\",\n      \"filter\" : [\n        { \"type\" : \"stop\", \"stopwords\": [\n          \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\",\n          \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\",\n          \"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\",\n          \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\",\n          \"GMT\", \"UTC\"\n        ] }\n      ]\n    },\n    \"categorization_field_name\": \"message\",\n    \"detectors\" :[{\n      \"function\":\"count\",\n      \"by_field_name\": \"mlcategory\"\n    }]\n  },\n  \"data_description\" : {\n  }\n}\n--------------------------------------------------\n\nIf you specify any part of the `categorization_analyzer`, however, any omitted\nsub-properties are _not_ set to default values.\n\nThe `ml_standard` tokenizer and the day and month stopword filter are more or\nless equivalent to the following analyzer, which is defined using only built-in\n{es} {ref}/analysis-tokenizers.html[tokenizers] and\n{ref}/analysis-tokenfilters.html[token filters]:\n\n[source,console]\n----------------------------------\nPUT _ml/anomaly_detectors/it_ops_new_logs3\n{\n  \"description\" : \"IT Ops Application Logs\",\n  \"analysis_config\" : {\n    \"categorization_field_name\": \"message\",\n    \"bucket_span\":\"30m\",\n    \"detectors\" :[{\n      \"function\":\"count\",\n      \"by_field_name\": \"mlcategory\",\n      \"detector_description\": \"Unusual message counts\"\n    }],\n    \"categorization_analyzer\":{\n      \"char_filter\" : [\n        \"first_line_with_letters\" <1>\n      ],\n      \"tokenizer\": {\n        \"type\" : \"simple_pattern_split\",\n        \"pattern\" : \"[^-0-9A-Za-z_./]+\" <2>\n      },\n      \"filter\": [\n        { \"type\" : \"pattern_replace\", \"pattern\": \"^[0-9].*\" }, <3>\n        { \"type\" : \"pattern_replace\", \"pattern\": \"^[-0-9A-Fa-f.]+$\" }, <4>\n        { \"type\" : \"pattern_replace\", \"pattern\": \"^[^0-9A-Za-z]+\" }, <5>\n        { \"type\" : \"pattern_replace\", \"pattern\": \"[^0-9A-Za-z]+$\" }, <6>\n        { \"type\" : \"stop\", \"stopwords\": [\n          \"\",\n          \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\",\n          \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\",\n          \"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\",\n          \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\",\n          \"GMT\", \"UTC\"\n        ] }\n      ]\n    }\n  },\n  \"analysis_limits\":{\n    \"categorization_examples_limit\": 5\n  },\n  \"data_description\" : {\n    \"time_field\":\"time\",\n    \"time_format\": \"epoch_ms\"\n  }\n}\n----------------------------------\n// TEST[skip:needs-licence]\n\n<1> Only consider the first line of the message with letters for categorization purposes.\n<2> Tokens basically consist of hyphens, digits, letters, underscores, dots and slashes.\n<3> By default, categorization ignores tokens that begin with a digit.\n<4> By default, categorization also ignores tokens that are hexadecimal numbers.\n<5> Underscores, hyphens, and dots are removed from the beginning of tokens.\n<6> Underscores, hyphens, and dots are also removed from the end of tokens.\n\nThe key difference between the default `categorization_analyzer` and this\nexample analyzer is that using the `ml_standard` tokenizer is several times\nfaster. The `ml_standard` tokenizer also tries to preserve URLs, Windows paths\nand email addresses as single tokens. Another difference in behavior is that\nthis custom analyzer does not include accented letters in tokens whereas the\n`ml_standard` tokenizer does, although that could be fixed by using more complex\nregular expressions.\n\nIf you are categorizing non-English messages in a language where words are\nseparated by spaces, you might get better results if you change the day or month\nwords in the stop token filter to the appropriate words in your language. If you\nare categorizing messages in a language where words are not separated by spaces,\nyou must use a different tokenizer as well in order to get sensible\ncategorization results.\n\nIt is important to be aware that analyzing for categorization of machine\ngenerated log messages is a little different from tokenizing for search.\nFeatures that work well for search, such as stemming, synonym substitution, and\nlowercasing are likely to make the results of categorization worse. However, in\norder for drill down from {ml} results to work correctly, the tokens that the\ncategorization analyzer produces must be similar to those produced by the search\nanalyzer. If they are sufficiently similar, when you search for the tokens that\nthe categorization analyzer produces then you find the original document that\nthe categorization field value came from.\n"
}