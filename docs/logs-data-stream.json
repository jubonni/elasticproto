{
    "meta": {
        "timestamp": "2024-11-01T02:49:25.567067",
        "size": 11328,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/logs-data-stream.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "logs-data-stream",
        "version": "8.15"
    },
    "doc": "[[logs-data-stream]]\n== Logs data stream\n\npreview::[Logs data streams and the logsdb index mode are in tech preview and may be changed or removed in the future. Don't use logs data streams or logsdb index mode in production.]\n\nA logs data stream is a data stream type that stores log data more efficiently.\n\nIn benchmarks, log data stored in a logs data stream used ~2.5 times less disk space than a regular data\nstream. The exact impact will vary depending on your data set.\n\n[discrete]\n[[how-to-use-logsds]]\n=== Create a logs data stream\n\nTo create a logs data stream, set your index template  `index.mode` to `logsdb`:\n\n[source,console]\n----\nPUT _index_template/my-index-template\n{\n  \"index_patterns\": [\"logs-*\"],\n  \"data_stream\": { },\n  \"template\": {\n     \"settings\": {\n        \"index.mode\": \"logsdb\" <1>\n     }\n  },\n  \"priority\": 101 <2>\n}\n----\n// TEST\n\n<1> The index mode setting.\n<2> The index template priority. By default, Elasticsearch ships with an index template with a `logs-*-*` pattern with a priority of 100. You need to define a priority higher than 100 to ensure that this index template gets selected over the default index template for the `logs-*-*` pattern. See the <<avoid-index-pattern-collisions,avoid index pattern collision section>> for more information.\n\nAfter the index template is created, new indices that use the template will be configured as a logs data stream. You can start indexing data and <<use-a-data-stream,using the data stream>>.\n\n////\n[source,console]\n----\nDELETE _index_template/my-index-template\n----\n// TEST[continued]\n////\n\n[[logsdb-default-settings]]\n\n[discrete]\n[[logsdb-synthtic-source]]\n=== Synthetic source\n\nBy default, `logsdb` mode uses <<synthetic-source,synthetic source>>, which omits storing the original `_source`\nfield and synthesizes it from doc values or stored fields upon document retrieval. Synthetic source comes with a few\nrestrictions which you can read more about in the <<synthetic-source,documentation>> section dedicated to it.\n\nNOTE: When dealing with multi-value fields, the `index.mapping.synthetic_source_keep` setting controls how field values\nare preserved for <<synthetic-source,synthetic source>> reconstruction. In `logsdb`, the default value is `arrays`,\nwhich retains both duplicate values and the order of entries but not necessarily the exact structure when it comes to\narray elements or objects. Preserving duplicates and ordering could be critical for some log fields. This could be the\ncase, for instance, for DNS A records, HTTP headers, or log entries that represent sequential or repeated events.\n\nFor more details on this setting and ways to refine or bypass it, check out <<synthetic-source-keep, this section>>.\n\n[discrete]\n[[logsdb-sort-settings]]\n=== Index sort settings\n\nThe following settings are applied by default when using the `logsdb` mode for index sorting:\n\n* `index.sort.field`: `[\"host.name\", \"@timestamp\"]`\n  In `logsdb` mode, indices are sorted by `host.name` and `@timestamp` fields by default. For data streams, the\n  `@timestamp` field is automatically injected if it is not present.\n\n* `index.sort.order`: `[\"desc\", \"desc\"]`\n  The default sort order for both fields is descending (`desc`), prioritizing the latest data.\n\n* `index.sort.mode`: `[\"min\", \"min\"]`\n  The default sort mode is `min`, ensuring that indices are sorted by the minimum value of multi-value fields.\n\n* `index.sort.missing`: `[\"_first\", \"_first\"]`\n  Missing values are sorted to appear first (`_first`) in `logsdb` index mode.\n\n`logsdb` index mode allows users to override the default sort settings. For instance, users can specify their own fields\nand order for sorting by modifying the `index.sort.field` and `index.sort.order`.\n\nWhen using default sort settings, the `host.name` field is automatically injected into the mappings of the\nindex as a `keyword` field to ensure that sorting can be applied. This guarantees that logs are efficiently sorted and\nretrieved based on the `host.name` and `@timestamp` fields.\n\nNOTE: If `subobjects` is set to `true` (which is the default), the `host.name` field will be mapped as an object field\nnamed `host`, containing a `name` child field of type `keyword`. On the other hand, if `subobjects` is set to `false`,\na single `host.name` field will be mapped as a `keyword` field.\n\nOnce an index is created, the sort settings are immutable and cannot be modified. To apply different sort settings,\na new index must be created with the desired configuration. For data streams, this can be achieved by means of an index\nrollover after updating relevant (component) templates.\n\nIf the default sort settings are not suitable for your use case, consider modifying them. Keep in mind that sort\nsettings can influence indexing throughput, query latency, and may affect compression efficiency due to the way data\nis organized after sorting. For more details, refer to our documentation on\n<<index-modules-index-sorting,index sorting>>.\n\nNOTE: For <<data-streams, data streams>>, the `@timestamp` field is automatically injected if not already present.\nHowever, if custom sort settings are applied, the `@timestamp` field is injected into the mappings, but it is not\nautomatically added to the list of sort fields.\n\n[discrete]\n[[logsdb-specialized-codecs]]\n=== Specialized codecs\n\n`logsdb` index mode uses the `best_compression` <<index-codec,codec>> by default, which applies {wikipedia}/Zstd[ZSTD]\ncompression to stored fields. Users are allowed to override it and switch to the `default` codec for faster compression\nat the expense of slightly larger storage footprint.\n\n`logsdb` index mode also adopts specialized codecs for numeric doc values that are crafted to optimize storage usage.\nUsers can rely on these specialized codecs being applied by default when using `logsdb` index mode.\n\nDoc values encoding for numeric fields in `logsdb` follows a static sequence of codecs, applying each one in the\nfollowing order: delta encoding, offset encoding, Greatest Common Divisor GCD encoding, and finally Frame Of Reference\n(FOR) encoding. The decision to apply each encoding is based on heuristics determined by the data distribution.\nFor example, before applying delta encoding, the algorithm checks if the data is monotonically non-decreasing or\nnon-increasing. If the data fits this pattern, delta encoding is applied; otherwise, the next encoding is considered.\n\nThe encoding is specific to each Lucene segment and is also re-applied at segment merging time. The merged Lucene segment\nmay use a different encoding compared to the original Lucene segments, based on the characteristics of the merged data.\n\nThe following methods are applied sequentially:\n\n* **Delta encoding**:\n  a compression method that stores the difference between consecutive values instead of the actual values.\n\n* **Offset encoding**:\n  a compression method that stores the difference from a base value rather than between consecutive values.\n\n* **Greatest Common Divisor (GCD) encoding**:\n  a compression method that finds the greatest common divisor of a set of values and stores the differences\n  as multiples of the GCD.\n\n* **Frame Of Reference (FOR) encoding**:\n  a compression method that determines the smallest number of bits required to encode a block of values and uses\n  bit-packing to fit such values into larger 64-bit blocks.\n\nFor keyword fields, **Run Length Encoding (RLE)** is applied to the ordinals, which represent positions in the Lucene\nsegment-level keyword dictionary. This compression is used when multiple consecutive documents share the same keyword.\n\n[discrete]\n[[logsdb-ignored-settings]]\n=== `ignore_malformed`, `ignore_above`, `ignore_dynamic_beyond_limit`\n\nBy default, `logsdb` index mode sets `ignore_malformed` to `true`. This setting allows documents with malformed fields\nto be indexed without causing indexing failures, ensuring that log data ingestion continues smoothly even when some\nfields contain invalid or improperly formatted data.\n\nUsers can override this setting by setting `index.mapping.ignore_malformed` to `false`. However, this is not recommended\nas it might result in documents with malformed fields being rejected and not indexed at all.\n\nIn `logsdb` index mode, the `index.mapping.ignore_above` setting is applied by default at the index level to ensure\nefficient storage and indexing of large keyword fields.The index-level default for `ignore_above` is set to 8191\n**characters**. If using UTF-8 encoding, this results in a limit of 32764 bytes, depending on character encoding.\nThe mapping-level `ignore_above` setting still takes precedence. If a specific field has an `ignore_above` value\ndefined in its mapping, that value will override the index-level `index.mapping.ignore_above` value. This default\nbehavior helps to optimize indexing performance by preventing excessively large string values from being indexed, while\nstill allowing users to customize the limit, overriding it at the mapping level or changing the index level default\nsetting.\n\nIn `logsdb` index mode, the setting `index.mapping.total_fields.ignore_dynamic_beyond_limit` is set to `true` by\ndefault. This allows dynamically mapped fields to be added on top of statically defined fields without causing document\nrejection, even after the total number of fields exceeds the limit defined by `index.mapping.total_fields.limit`. The\n`index.mapping.total_fields.limit` setting specifies the maximum number of fields an index can have (static, dynamic\nand runtime). When the limit is reached, new dynamically mapped fields will be ignored instead of failing the document\nindexing, ensuring continued log ingestion without errors.\n\nNOTE: When automatically injected, `host.name` and `@timestamp` contribute to the limit of mapped fields. When\n`host.name` is mapped with `subobjects: true` it consists of two fields. When `host.name` is mapped with\n`subobjects: false` it only consists of one field.\n\n[discrete]\n[[logsdb-nodocvalue-fields]]\n=== Fields without doc values\n\nWhen `logsdb` index mode uses synthetic `_source`, and `doc_values` are disabled for a field in the mapping,\nElasticsearch may set the `store` setting to `true` for that field as a last resort option to ensure that the field's\ndata is still available for reconstructing the document\u2019s source when retrieving it via\n<<synthetic-source,synthetic source>>.\n\nFor example, this happens with text fields when `store` is `false` and there is no suitable multi-field available to\nreconstruct the original value in <<synthetic-source,synthetic source>>.\n\nThis automatic adjustment allows synthetic source to work correctly, even when doc values are not enabled for certain\nfields.\n\n[discrete]\n[[logsdb-settings-summary]]\n=== LogsDB settings summary\n\nThe following is a summary of key settings that apply when using `logsdb` index mode in Elasticsearch:\n\n* **`index.mode`**: `\"logsdb\"`\n\n* **`index.mapping.synthetic_source_keep`**: `\"arrays\"`\n\n* **`index.sort.field`**: `[\"host.name\", \"@timestamp\"]`\n\n* **`index.sort.order`**: `[\"desc\", \"desc\"]`\n\n* **`index.sort.mode`**: `[\"min\", \"min\"]`\n\n* **`index.sort.missing`**: `[\"_first\", \"_first\"]`\n\n* **`index.codec`**: `\"best_compression\"`\n\n* **`index.mapping.ignore_malformed`**: `true`\n\n* **`index.mapping.ignore_above`**: `8191`\n\n* **`index.mapping.total_fields.ignore_dynamic_beyond_limit`**: `true`\n"
}