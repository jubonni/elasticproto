{
    "meta": {
        "timestamp": "2024-11-01T03:07:09.948271",
        "size": 51737,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-profile.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "search-profile",
        "version": "8.15"
    },
    "doc": "[[search-profile]]\n=== Profile API\n++++\n<titleabbrev>Profile</titleabbrev>\n++++\n\nWARNING: The Profile API is a debugging tool and adds significant overhead to search execution.\n\nProvides detailed timing information about the execution of individual\ncomponents in a search request.\n\n\n[[search-profile-api-desc]]\n==== {api-description-title}\n\nThe Profile API gives the user insight into how search requests are executed at\na low level so that the user can understand why certain requests are slow, and\ntake steps to improve them. Note that the Profile API,\n<<profile-limitations, amongst other things>>, doesn't measure network latency,\ntime the requests spend in queues, or time spent merging shard\nresponses on the coordinating node.\n\nThe output from the Profile API is *very* verbose, especially for complicated\nrequests executed across many shards. Pretty-printing the response is\nrecommended to help understand the output.\n\n\n[[search-profile-api-example]]\n==== {api-examples-title}\n\n\nAny `_search` request can be profiled by adding a top-level `profile` parameter:\n\n[source,console]\n--------------------------------------------------\nGET /my-index-000001/_search\n{\n  \"profile\": true,<1>\n  \"query\" : {\n    \"match\" : { \"message\" : \"GET /search\" }\n  }\n}\n--------------------------------------------------\n// TEST[setup:my_index]\n\n<1> Setting the top-level `profile` parameter to `true` will enable profiling\nfor the search.\n\n\nThe API returns the following result:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"took\": 25,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": {\n      \"value\": 5,\n      \"relation\": \"eq\"\n    },\n    \"max_score\": 0.17402273,\n    \"hits\": [...] <1>\n  },\n  \"profile\": {\n    \"shards\": [\n      {\n        \"id\": \"[q2aE02wS1R8qQFnYu6vDVQ][my-index-000001][0]\",\n        \"node_id\": \"q2aE02wS1R8qQFnYu6vDVQ\",\n        \"shard_id\": 0,\n        \"index\": \"my-index-000001\",\n        \"cluster\": \"(local)\",\n        \"searches\": [\n          {\n            \"query\": [\n              {\n                \"type\": \"BooleanQuery\",\n                \"description\": \"message:get message:search\",\n                \"time_in_nanos\" : 11972972,\n                \"breakdown\" : {\n                  \"set_min_competitive_score_count\": 0,\n                  \"match_count\": 5,\n                  \"shallow_advance_count\": 0,\n                  \"set_min_competitive_score\": 0,\n                  \"next_doc\": 39022,\n                  \"match\": 4456,\n                  \"next_doc_count\": 5,\n                  \"score_count\": 5,\n                  \"compute_max_score_count\": 0,\n                  \"compute_max_score\": 0,\n                  \"advance\": 84525,\n                  \"advance_count\": 1,\n                  \"score\": 37779,\n                  \"build_scorer_count\": 2,\n                  \"create_weight\": 4694895,\n                  \"shallow_advance\": 0,\n                  \"create_weight_count\": 1,\n                  \"build_scorer\": 7112295,\n                  \"count_weight\": 0,\n                  \"count_weight_count\": 0\n                },\n                \"children\": [\n                  {\n                    \"type\": \"TermQuery\",\n                    \"description\": \"message:get\",\n                    \"time_in_nanos\": 3801935,\n                    \"breakdown\": {\n                      \"set_min_competitive_score_count\": 0,\n                      \"match_count\": 0,\n                      \"shallow_advance_count\": 3,\n                      \"set_min_competitive_score\": 0,\n                      \"next_doc\": 0,\n                      \"match\": 0,\n                      \"next_doc_count\": 0,\n                      \"score_count\": 5,\n                      \"compute_max_score_count\": 3,\n                      \"compute_max_score\": 32487,\n                      \"advance\": 5749,\n                      \"advance_count\": 6,\n                      \"score\": 16219,\n                      \"build_scorer_count\": 3,\n                      \"create_weight\": 2382719,\n                      \"shallow_advance\": 9754,\n                      \"create_weight_count\": 1,\n                      \"build_scorer\": 1355007,\n                      \"count_weight\": 0,\n                      \"count_weight_count\": 0\n                    }\n                  },\n                  {\n                    \"type\": \"TermQuery\",\n                    \"description\": \"message:search\",\n                    \"time_in_nanos\": 205654,\n                    \"breakdown\": {\n                      \"set_min_competitive_score_count\": 0,\n                      \"match_count\": 0,\n                      \"shallow_advance_count\": 3,\n                      \"set_min_competitive_score\": 0,\n                      \"next_doc\": 0,\n                      \"match\": 0,\n                      \"next_doc_count\": 0,\n                      \"score_count\": 5,\n                      \"compute_max_score_count\": 3,\n                      \"compute_max_score\": 6678,\n                      \"advance\": 12733,\n                      \"advance_count\": 6,\n                      \"score\": 6627,\n                      \"build_scorer_count\": 3,\n                      \"create_weight\": 130951,\n                      \"shallow_advance\": 2512,\n                      \"create_weight_count\": 1,\n                      \"build_scorer\": 46153,\n                      \"count_weight\": 0,\n                      \"count_weight_count\": 0\n                    }\n                  }\n                ]\n              }\n            ],\n            \"rewrite_time\": 451233,\n            \"collector\": [\n              {\n                \"name\": \"QueryPhaseCollector\",\n                \"reason\": \"search_query_phase\",\n                \"time_in_nanos\": 775274,\n                \"children\" : [\n                  {\n                    \"name\": \"SimpleTopScoreDocCollector\",\n                    \"reason\": \"search_top_hits\",\n                    \"time_in_nanos\": 775274\n                  }\n                ]\n              }\n            ]\n          }\n        ],\n        \"aggregations\": [],\n        \"fetch\": {\n          \"type\": \"fetch\",\n          \"description\": \"\",\n          \"time_in_nanos\": 660555,\n          \"breakdown\": {\n            \"next_reader\": 7292,\n            \"next_reader_count\": 1,\n            \"load_stored_fields\": 299325,\n            \"load_stored_fields_count\": 5,\n            \"load_source\": 3863,\n            \"load_source_count\": 5\n          },\n          \"debug\": {\n            \"stored_fields\": [\"_id\", \"_routing\", \"_source\"]\n          },\n          \"children\": [\n            {\n              \"type\" : \"FetchFieldsPhase\",\n              \"description\" : \"\",\n              \"time_in_nanos\" : 238762,\n              \"breakdown\" : {\n                \"process_count\" : 5,\n                \"process\" : 227914,\n                \"next_reader\" : 10848,\n                \"next_reader_count\" : 1\n              }\n            },\n            {\n              \"type\": \"FetchSourcePhase\",\n              \"description\": \"\",\n              \"time_in_nanos\": 20443,\n              \"breakdown\": {\n                \"next_reader\": 745,\n                \"next_reader_count\": 1,\n                \"process\": 19698,\n                \"process_count\": 5\n              },\n              \"debug\": {\n                \"fast_path\": 5\n              }\n            },\n            {\n              \"type\": \"StoredFieldsPhase\",\n              \"description\": \"\",\n              \"time_in_nanos\": 5310,\n              \"breakdown\": {\n                \"next_reader\": 745,\n                \"next_reader_count\": 1,\n                \"process\": 4445,\n                \"process_count\": 5\n              }\n            }\n          ]\n        }\n      }\n    ]\n  }\n}\n--------------------------------------------------\n// TESTRESPONSE[s/\"took\": 25/\"took\": $body.took/]\n// TESTRESPONSE[s/\"hits\": \\[...\\]/\"hits\": $body.$_path/]\n// TESTRESPONSE[s/(?<=[\" ])\\d+(\\.\\d+)?/$body.$_path/]\n// TESTRESPONSE[s/\"id\": \"\\[q2aE02wS1R8qQFnYu6vDVQ\\]\\[my-index-000001\\]\\[0\\]\"/\"id\": $body.profile.shards.0.id/]\n// TESTRESPONSE[s/\"node_id\": \"q2aE02wS1R8qQFnYu6vDVQ\",/\"node_id\": \"$body.profile.shards.0.node_id\",/]\n\n<1> Search results are returned, but were omitted here for brevity.\n\nEven for a simple query, the response is relatively complicated. Let's break it\ndown piece-by-piece before moving to more complex examples.\n\n\nThe overall structure of the profile response is as follows:\n\n[source,console-result]\n--------------------------------------------------\n{\n   \"profile\": {\n        \"shards\": [\n           {\n              \"id\": \"[q2aE02wS1R8qQFnYu6vDVQ][my-index-000001][0]\",  <1>\n              \"node_id\": \"q2aE02wS1R8qQFnYu6vDVQ\",\n              \"shard_id\": 0,\n              \"index\": \"my-index-000001\",\n              \"cluster\": \"(local)\",             <2>\n              \"searches\": [\n                 {\n                    \"query\": [...],             <3>\n                    \"rewrite_time\": 51443,      <4>\n                    \"collector\": [...]          <5>\n                 }\n              ],\n              \"aggregations\": [...],            <6>\n              \"fetch\": {...}                    <7>\n           }\n        ]\n     }\n}\n--------------------------------------------------\n// TESTRESPONSE[s/\"profile\": /\"took\": $body.took, \"timed_out\": $body.timed_out, \"_shards\": $body._shards, \"hits\": $body.hits, \"profile\": /]\n// TESTRESPONSE[s/(?<=[\" ])\\d+(\\.\\d+)?/$body.$_path/]\n// TESTRESPONSE[s/\"id\": \"\\[q2aE02wS1R8qQFnYu6vDVQ\\]\\[my-index-000001\\]\\[0\\]\"/\"id\": $body.profile.shards.0.id/]\n// TESTRESPONSE[s/\"node_id\": \"q2aE02wS1R8qQFnYu6vDVQ\",/\"node_id\": \"$body.profile.shards.0.node_id\",/]\n// TESTRESPONSE[s/\"query\": \\[...\\]/\"query\": $body.$_path/]\n// TESTRESPONSE[s/\"collector\": \\[...\\]/\"collector\": $body.$_path/]\n// TESTRESPONSE[s/\"aggregations\": \\[...\\]/\"aggregations\": []/]\n// TESTRESPONSE[s/\"fetch\": \\{...\\}/\"fetch\": $body.$_path/]\n<1> A profile is returned for each shard that participated in the response, and\nis identified by a unique ID.\n<2> If the query was run on the local cluster, the cluster name is left out of the\ncomposite id and is marked \"(local)\" here. For a profile running on a remote_cluster\nusing cross-cluster search, the \"id\" value would be something like\n`[q2aE02wS1R8qQFnYu6vDVQ][remote1:my-index-000001][0]` and the \"cluster\"\nvalue would be `remote1`.\n<3> Query timings and other debugging information.\n<4> The cumulative rewrite time.\n<5> Names and invocation timings for each collector.\n<6> Aggregation timings, invocation counts, and debug information.\n<7> Fetch timing and debug information.\n\nBecause a search request may be executed against one or more shards in an index,\nand a search may cover one or more indices, the top level element in the profile\nresponse is an array of `shard` objects. Each shard object lists its `id` which\nuniquely identifies the shard. The ID's format is\n`[nodeID][clusterName:indexName][shardID]`. If the search is run against the\nlocal cluster then the clusterName is not added and the format is\n`[nodeID][indexName][shardID]`.\n\nThe profile itself may consist of one or more \"searches\", where a search is a\nquery executed against the underlying Lucene index. Most search requests\nsubmitted by the user will only execute a single `search` against the Lucene\nindex. But occasionally multiple searches will be executed, such as including a\nglobal aggregation (which needs to execute a secondary \"match_all\" query for the\nglobal context).\n\nInside each `search` object there will be two arrays of profiled information:\na `query` array and a `collector` array. Alongside the `search` object is an\n`aggregations` object that contains the profile information for the\naggregations. In the future, more sections may be added, such as `suggest`,\n`highlight`, etc.\n\nThere will also be a `rewrite` metric showing the total time spent rewriting the\nquery (in nanoseconds).\n\nNOTE: As with other statistics apis, the Profile API supports human readable outputs. This can be turned on by adding\n`?human=true` to the query string. In this case, the output contains the additional `time` field containing rounded,\nhuman readable timing information (e.g. `\"time\": \"391,9ms\"`, `\"time\": \"123.3micros\"`).\n\n[[profiling-queries]]\n==== Profiling Queries\n\n[NOTE]\n=======================================\nThe details provided by the Profile API directly expose Lucene class names and concepts, which means\nthat complete interpretation of the results require fairly advanced knowledge of Lucene. This\npage attempts to give a crash-course in how Lucene executes queries so that you can use the Profile API to successfully\ndiagnose and debug queries, but it is only an overview. For complete understanding, please refer\nto Lucene's documentation and, in places, the code.\n\nWith that said, a complete understanding is often not required to fix a slow query. It is usually\nsufficient to see that a particular component of a query is slow, and not necessarily understand why\nthe `advance` phase of that query is the cause, for example.\n=======================================\n\n[[query-section]]\n===== `query` Section\n\nThe `query` section contains detailed timing of the query tree executed by\nLucene on a particular shard. The overall structure of this query tree will\nresemble your original Elasticsearch query, but may be slightly (or sometimes\nvery) different. It will also use similar but not always identical naming.\nUsing our previous `match` query example, let's analyze the `query` section:\n\n[source,console-result]\n--------------------------------------------------\n\"query\": [\n    {\n       \"type\": \"BooleanQuery\",\n       \"description\": \"message:get message:search\",\n       \"time_in_nanos\": \"11972972\",\n       \"breakdown\": {...},               <1>\n       \"children\": [\n          {\n             \"type\": \"TermQuery\",\n             \"description\": \"message:get\",\n             \"time_in_nanos\": \"3801935\",\n             \"breakdown\": {...}\n          },\n          {\n             \"type\": \"TermQuery\",\n             \"description\": \"message:search\",\n             \"time_in_nanos\": \"205654\",\n             \"breakdown\": {...}\n          }\n       ]\n    }\n]\n--------------------------------------------------\n// TESTRESPONSE[s/^/{\\n\"took\": $body.took,\\n\"timed_out\": $body.timed_out,\\n\"_shards\": $body._shards,\\n\"hits\": $body.hits,\\n\"profile\": {\\n\"shards\": [ {\\n\"id\": \"$body.profile.shards.0.id\",\\n\"node_id\": \"$body.profile.shards.0.node_id\",\\n\"shard_id\": $body.profile.shards.0.shard_id,\\n\"index\": \"$body.profile.shards.0.index\",\\n\"cluster\": \"(local)\",\\n\"searches\": [{\\n/]\n// TESTRESPONSE[s/]$/],\"rewrite_time\": $body.$_path, \"collector\": $body.$_path}], \"aggregations\": [], \"fetch\": $body.$_path}]}}/]\n// TESTRESPONSE[s/(?<=[\" ])\\d+(\\.\\d+)?/$body.$_path/]\n// TESTRESPONSE[s/\"breakdown\": \\{...\\}/\"breakdown\": $body.$_path/]\n<1> The breakdown timings are omitted for simplicity.\n\nBased on the profile structure, we can see that our `match` query was rewritten\nby Lucene into a BooleanQuery with two clauses (both holding a TermQuery). The\n`type` field displays the Lucene class name, and often aligns with the\nequivalent name in Elasticsearch. The `description` field displays the Lucene\nexplanation text for the query, and is made available to help differentiating\nbetween parts of your query (e.g. both `message:get` and `message:search` are\nTermQuery's and would appear identical otherwise.\n\nThe `time_in_nanos` field shows that this query took ~11.9ms for the entire\nBooleanQuery to execute. The recorded time is inclusive of all children.\n\nThe `breakdown` field will give detailed stats about how the time was spent,\nwe'll look at that in a moment. Finally, the `children` array lists any\nsub-queries that may be present. Because we searched for two values (\"get\nsearch\"), our BooleanQuery holds two children TermQueries. They have identical\ninformation (type, time, breakdown, etc). Children are allowed to have their\nown children.\n\n===== Timing Breakdown\n\nThe `breakdown` component lists detailed timing statistics about low-level\nLucene execution:\n\n[source,console-result]\n--------------------------------------------------\n\"breakdown\": {\n  \"set_min_competitive_score_count\": 0,\n  \"match_count\": 5,\n  \"shallow_advance_count\": 0,\n  \"set_min_competitive_score\": 0,\n  \"next_doc\": 39022,\n  \"match\": 4456,\n  \"next_doc_count\": 5,\n  \"score_count\": 5,\n  \"compute_max_score_count\": 0,\n  \"compute_max_score\": 0,\n  \"advance\": 84525,\n  \"advance_count\": 1,\n  \"score\": 37779,\n  \"build_scorer_count\": 2,\n  \"create_weight\": 4694895,\n  \"shallow_advance\": 0,\n  \"create_weight_count\": 1,\n  \"build_scorer\": 7112295,\n  \"count_weight\": 0,\n  \"count_weight_count\": 0\n}\n--------------------------------------------------\n// TESTRESPONSE[s/^/{\\n\"took\": $body.took,\\n\"timed_out\": $body.timed_out,\\n\"_shards\": $body._shards,\\n\"hits\": $body.hits,\\n\"profile\": {\\n\"shards\": [ {\\n\"id\": \"$body.profile.shards.0.id\",\\n\"node_id\": \"$body.profile.shards.0.node_id\",\\n\"shard_id\": $body.profile.shards.0.shard_id,\\n\"index\": \"$body.profile.shards.0.index\",\\n\"cluster\": \"(local)\",\\n\"searches\": [{\\n\"query\": [{\\n\"type\": \"BooleanQuery\",\\n\"description\": \"message:get message:search\",\\n\"time_in_nanos\": $body.$_path,/]\n// TESTRESPONSE[s/}$/},\\n\"children\": $body.$_path}],\\n\"rewrite_time\": $body.$_path, \"collector\": $body.$_path}], \"aggregations\": [], \"fetch\": $body.$_path}]}}/]\n// TESTRESPONSE[s/(?<=[\" ])\\d+(\\.\\d+)?/$body.$_path/]\n\nTimings are listed in wall-clock nanoseconds and are not normalized at all. All\ncaveats about the overall `time_in_nanos` apply here. The intention of the\nbreakdown is to give you a feel for A) what machinery in Lucene is actually\neating time, and B) the magnitude of differences in times between the various\ncomponents. Like the overall time, the breakdown is inclusive of all children\ntimes.\n\nThe meaning of the stats are as follows:\n\n[discrete]\n===== All parameters:\n\n[horizontal]\n`create_weight`::\n\n    A Query in Lucene must be capable of reuse across multiple IndexSearchers (think of it as the engine that\n    executes a search against a specific Lucene Index). This puts Lucene in a tricky spot, since many queries\n    need to accumulate temporary state/statistics associated with the index it is being used against, but the\n    Query contract mandates that it must be immutable.\n    {empty} +\n    {empty} +\n    To get around this, Lucene asks each query to generate a Weight object which acts as a temporary context\n    object to hold state associated with this particular (IndexSearcher, Query) tuple. The `weight` metric\n    shows how long this process takes\n\n`build_scorer`::\n\n    This parameter shows how long it takes to build a Scorer for the query. A Scorer is the mechanism that\n    iterates over matching documents and generates a score per-document (e.g. how well does \"foo\" match the document?).\n    Note, this records the time required to generate the Scorer object, not actually score the documents. Some\n    queries have faster or slower initialization of the Scorer, depending on optimizations, complexity, etc.\n    {empty} +\n    {empty} +\n    This may also show timing associated with caching, if enabled and/or applicable for the query\n\n`next_doc`::\n\n    The Lucene method `next_doc` returns Doc ID of the next document matching the query. This statistic shows\n    the time it takes to determine which document is the next match, a process that varies considerably depending\n    on the nature of the query. Next_doc is a specialized form of advance() which is more convenient for many\n    queries in Lucene. It is equivalent to advance(docId() + 1)\n\n`advance`::\n\n    `advance` is the \"lower level\" version of next_doc: it serves the same purpose of finding the next matching\n    doc, but requires the calling query to perform extra tasks such as identifying and moving past skips, etc.\n    However,  not all queries can use next_doc, so `advance` is also timed for those queries.\n    {empty} +\n    {empty} +\n    Conjunctions (e.g. `must` clauses in a Boolean) are typical consumers of `advance`\n\n`match`::\n\n    Some queries, such as phrase queries, match documents using a \"two-phase\" process. First, the document is\n    \"approximately\" matched, and if it matches approximately, it is checked a second time with a more rigorous\n    (and expensive) process. The second phase verification is what the `match` statistic measures.\n    {empty} +\n    {empty} +\n    For example, a phrase query first checks a document approximately by ensuring all terms in the phrase are\n    present in the doc. If all the terms are present, it then executes the second phase verification to ensure\n    the terms are in-order to form the phrase, which is relatively more expensive than just checking for presence\n    of the terms.\n    {empty} +\n    {empty} +\n    Because this two-phase process is only used by a handful of queries, the `match` statistic is often zero\n\n`score`::\n\n    This records the time taken to score a particular document via its Scorer\n\n`*_count`::\n    Records the number of invocations of the particular method. For example, `\"next_doc_count\": 2,`\n    means the `nextDoc()` method was called on two different documents. This can be used to help judge\n    how selective queries are, by comparing counts between different query components.\n\n\n[[collectors-section]]\n===== `collectors` Section\n\nThe Collectors portion of the response shows high-level execution details.\nLucene works by defining a \"Collector\" which is responsible for coordinating the\ntraversal, scoring, and collection of matching documents. Collectors are also\nhow a single query can record aggregation results, execute unscoped \"global\"\nqueries, execute post-query filters, etc.\n\nLooking at the previous example:\n\n[source,console-result]\n--------------------------------------------------\n\"collector\": [\n  {\n    \"name\": \"QueryPhaseCollector\",\n    \"reason\": \"search_query_phase\",\n    \"time_in_nanos\": 775274,\n    \"children\" : [\n      {\n        \"name\": \"SimpleTopScoreDocCollector\",\n        \"reason\": \"search_top_hits\",\n        \"time_in_nanos\": 775274\n      }\n    ]\n  }\n]\n--------------------------------------------------\n// TESTRESPONSE[s/^/{\\n\"took\": $body.took,\\n\"timed_out\": $body.timed_out,\\n\"_shards\": $body._shards,\\n\"hits\": $body.hits,\\n\"profile\": {\\n\"shards\": [ {\\n\"id\": \"$body.profile.shards.0.id\",\\n\"node_id\": \"$body.profile.shards.0.node_id\",\\n\"shard_id\": $body.profile.shards.0.shard_id,\\n\"index\": \"$body.profile.shards.0.index\",\\n\"cluster\": \"(local)\",\\n\"searches\": [{\\n\"query\": $body.$_path,\\n\"rewrite_time\": $body.$_path,/]\n// TESTRESPONSE[s/]$/]}], \"aggregations\": [], \"fetch\": $body.$_path}]}}/]\n// TESTRESPONSE[s/(?<=[\" ])\\d+(\\.\\d+)?/$body.$_path/]\n\n\nWe see a top-level collector named `QueryPhaseCollector` which holds a child\n`SimpleTopScoreDocCollector`. `SimpleTopScoreDocCollector` is the  default\n\"scoring and sorting\" `Collector` used by {es}. The `reason` field attempts\nto give a plain English description of the class name. The `time_in_nanos`\nis similar to the time in the Query tree: a wall-clock time inclusive of all\nchildren. Similarly, `children` lists all sub-collectors. When aggregations\nare requested, the `QueryPhaseCollector` will hold an additional child\ncollector with reason `aggregation` that is the one performing aggregations.\n\nIt should be noted that Collector times are **independent** from the Query\ntimes. They are calculated, combined, and normalized independently! Due to the\nnature of Lucene's execution, it is impossible to \"merge\" the times from the\nCollectors into the Query section, so they are displayed in separate portions.\n\nFor reference, the various collector reasons are:\n\n[horizontal]\n`search_top_hits`::\n\n    A collector that scores and sorts documents. This is the most common collector and will be seen in most\n    simple searches\n\n`search_count`::\n\n    A collector that only counts the number of documents that match the query, but does not fetch the source.\n    This is seen when `size: 0` is specified\n\n`search_query_phase`::\n\n    A collector that incorporates collecting top hits as well aggregations as part of the query phase.\n    It supports terminating the search execution after `n` matching documents have been found (when\n    `terminate_after` is specified), as well as only returning matching documents that have a score\n    greater than `n` (when `min_score` is provided). Additionally, it is able to filter matching top\n    hits based on the provided `post_filter`.\n\n`search_timeout`::\n\n    A collector that halts execution after a specified period of time. This is seen when a `timeout` top-level\n    parameter has been specified.\n\n`aggregation`::\n\n    A collector that Elasticsearch uses to run aggregations against the query scope. A single `aggregation`\n    collector is used to collect documents for *all* aggregations, so you will see a list of aggregations\n    in the name rather.\n\n`global_aggregation`::\n\n    A collector that executes an aggregation against the global query scope, rather than the specified query.\n    Because the global scope is necessarily different from the executed query, it must execute its own\n    match_all query (which you will see added to the Query section) to collect your entire dataset\n\n\n[[rewrite-section]]\n===== `rewrite` Section\n\nAll queries in Lucene undergo a \"rewriting\" process. A query (and its\nsub-queries) may be rewritten one or more times, and the process continues until\nthe query stops changing. This process allows Lucene to perform optimizations,\nsuch as removing redundant clauses, replacing one query for a more efficient\nexecution path, etc. For example a Boolean -> Boolean -> TermQuery can be\nrewritten to a TermQuery, because all the Booleans are unnecessary in this case.\n\nThe rewriting process is complex and difficult to display, since queries can\nchange drastically. Rather than showing the intermediate results, the total\nrewrite time is simply displayed as a value (in nanoseconds). This value is\ncumulative and contains the total time for all queries being rewritten.\n\n===== A more complex example\n\nTo demonstrate a slightly more complex query and the associated results, we can\nprofile the following query:\n\n[source,console]\n--------------------------------------------------\nGET /my-index-000001/_search\n{\n  \"profile\": true,\n  \"query\": {\n    \"term\": {\n      \"user.id\": {\n        \"value\": \"elkbee\"\n      }\n    }\n  },\n  \"aggs\": {\n    \"my_scoped_agg\": {\n      \"terms\": {\n        \"field\": \"http.response.status_code\"\n      }\n    },\n    \"my_global_agg\": {\n      \"global\": {},\n      \"aggs\": {\n        \"my_level_agg\": {\n          \"terms\": {\n            \"field\": \"http.response.status_code\"\n          }\n        }\n      }\n    }\n  },\n  \"post_filter\": {\n    \"match\": {\n      \"message\": \"search\"\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:my_index]\n// TEST[s/_search/_search\\?filter_path=profile.shards.id,profile.shards.node_id,profile.shards.shard_id,profile.shards.index,profile.shards.cluster,profile.shards.searches,profile.shards.aggregations,profile.shards.fetch/]\n\n\nThis example has:\n\n- A query\n- A scoped aggregation\n- A global aggregation\n- A post_filter\n\n\nThe API returns the following result:\n\n[source,console-result]\n--------------------------------------------------\n{\n  ...\n  \"profile\": {\n    \"shards\": [\n      {\n        \"id\": \"[P6xvulHtQRWuD4YnubWb7A][my-index-000001][0]\",\n        \"node_id\": \"P6xvulHtQRWuD4YnubWb7A\",\n        \"shard_id\": 0,\n        \"index\": \"my-index-000001\",\n        \"cluster\": \"(local)\",\n        \"searches\": [\n          {\n            \"query\": [\n              {\n                \"type\": \"TermQuery\",\n                \"description\": \"message:search\",\n                \"time_in_nanos\": 141618,\n                \"breakdown\": {\n                  \"set_min_competitive_score_count\": 0,\n                  \"match_count\": 0,\n                  \"shallow_advance_count\": 0,\n                  \"set_min_competitive_score\": 0,\n                  \"next_doc\": 0,\n                  \"match\": 0,\n                  \"next_doc_count\": 0,\n                  \"score_count\": 0,\n                  \"compute_max_score_count\": 0,\n                  \"compute_max_score\": 0,\n                  \"advance\": 3942,\n                  \"advance_count\": 4,\n                  \"count_weight_count\": 0,\n                  \"score\": 0,\n                  \"build_scorer_count\": 2,\n                  \"create_weight\": 38380,\n                  \"shallow_advance\": 0,\n                  \"count_weight\": 0,\n                  \"create_weight_count\": 1,\n                  \"build_scorer\": 99296\n                }\n              },\n              {\n                \"type\": \"TermQuery\",\n                \"description\": \"user.id:elkbee\",\n                \"time_in_nanos\": 163081,\n                \"breakdown\": {\n                  \"set_min_competitive_score_count\": 0,\n                  \"match_count\": 0,\n                  \"shallow_advance_count\": 0,\n                  \"set_min_competitive_score\": 0,\n                  \"next_doc\": 2447,\n                  \"match\": 0,\n                  \"next_doc_count\": 4,\n                  \"score_count\": 4,\n                  \"compute_max_score_count\": 0,\n                  \"compute_max_score\": 0,\n                  \"advance\": 3552,\n                  \"advance_count\": 1,\n                  \"score\": 5027,\n                  \"count_weight_count\": 0,\n                  \"build_scorer_count\": 2,\n                  \"create_weight\": 107840,\n                  \"shallow_advance\": 0,\n                  \"count_weight\": 0,\n                  \"create_weight_count\": 1,\n                  \"build_scorer\": 44215\n                }\n              }\n            ],\n            \"rewrite_time\": 4769,\n            \"collector\": [\n              {\n                \"name\": \"QueryPhaseCollector\",\n                \"reason\": \"search_query_phase\",\n                \"time_in_nanos\": 1945072,\n                \"children\": [\n                  {\n                    \"name\": \"SimpleTopScoreDocCollector\",\n                    \"reason\": \"search_top_hits\",\n                    \"time_in_nanos\": 22577\n                  },\n                  {\n                    \"name\": \"AggregatorCollector: [my_scoped_agg, my_global_agg]\",\n                    \"reason\": \"aggregation\",\n                    \"time_in_nanos\": 867617\n                  }\n                ]\n              }\n            ]\n          }\n        ],\n        \"aggregations\": [...], <1>\n        \"fetch\": {...}\n      }\n    ]\n  }\n}\n--------------------------------------------------\n// TESTRESPONSE[s/\"aggregations\": \\[\\.\\.\\.\\]/\"aggregations\": $body.$_path/]\n// TESTRESPONSE[s/\"fetch\": \\{\\.\\.\\.\\}/\"fetch\": $body.$_path/]\n// TESTRESPONSE[s/\\.\\.\\.//]\n// TESTRESPONSE[s/(?<=[\" ])\\d+(\\.\\d+)?/$body.$_path/]\n// TESTRESPONSE[s/\"id\": \"\\[P6xvulHtQRWuD4YnubWb7A\\]\\[my-index-000001\\]\\[0\\]\"/\"id\": $body.profile.shards.0.id/]\n// TESTRESPONSE[s/\"node_id\": \"P6xvulHtQRWuD4YnubWb7A\",/\"node_id\": \"$body.profile.shards.0.node_id\",/]\n<1> The `\"aggregations\"` portion has been omitted because it will be covered in\nthe next section.\n\nAs you can see, the output is significantly more verbose than before. All the\nmajor portions of the query are represented:\n\n1. The first `TermQuery` (user.id:elkbee) represents the main `term` query.\n2. The second `TermQuery` (message:search) represents the `post_filter` query.\n\nThe Collector tree is fairly straightforward, showing how a single\nQueryPhaseCollector that holds the normal scoring SimpleTopScoreDocCollector\nused to collect top hits, as well as BucketCollectorWrapper to run all scoped\naggregations.\n\n===== Understanding MultiTermQuery output\n\nA special note needs to be made about the `MultiTermQuery` class of queries.\nThis includes wildcards, regex, and fuzzy queries. These queries emit very\nverbose responses, and are not overly structured.\n\nEssentially, these queries rewrite themselves on a per-segment basis. If you\nimagine the wildcard query `b*`, it technically can match any token that begins\nwith the letter \"b\". It would be impossible to enumerate all possible\ncombinations, so Lucene rewrites the query in context of the segment being\nevaluated, e.g., one segment may contain the tokens `[bar, baz]`, so the query\nrewrites to a BooleanQuery combination of \"bar\" and \"baz\". Another segment may\nonly have the token `[bakery]`, so the query rewrites to a single TermQuery for\n\"bakery\".\n\nDue to this dynamic, per-segment rewriting, the clean tree structure becomes\ndistorted and no longer follows a clean \"lineage\" showing how one query rewrites\ninto the next. At present time, all we can do is apologize, and suggest you\ncollapse the details for that query's children if it is too confusing. Luckily,\nall the timing statistics are correct, just not the physical layout in the\nresponse, so it is sufficient to just analyze the top-level MultiTermQuery and\nignore its children if you find the details too tricky to interpret.\n\nHopefully this will be fixed in future iterations, but it is a tricky problem to\nsolve and still in-progress. :)\n\n[[profiling-aggregations]]\n===== Profiling Aggregations\n\n\n[[agg-section]]\n====== `aggregations` Section\n\n\nThe `aggregations` section contains detailed timing of the aggregation tree\nexecuted by a particular shard. The overall structure of this aggregation tree\nwill resemble your original {es} request. Let's execute the previous query again\nand look at the aggregation profile this time:\n\n[source,console]\n--------------------------------------------------\nGET /my-index-000001/_search\n{\n  \"profile\": true,\n  \"query\": {\n    \"term\": {\n      \"user.id\": {\n        \"value\": \"elkbee\"\n      }\n    }\n  },\n  \"aggs\": {\n    \"my_scoped_agg\": {\n      \"terms\": {\n        \"field\": \"http.response.status_code\"\n      }\n    },\n    \"my_global_agg\": {\n      \"global\": {},\n      \"aggs\": {\n        \"my_level_agg\": {\n          \"terms\": {\n            \"field\": \"http.response.status_code\"\n          }\n        }\n      }\n    }\n  },\n  \"post_filter\": {\n    \"match\": {\n      \"message\": \"search\"\n    }\n  }\n}\n--------------------------------------------------\n// TEST[s/_search/_search\\?filter_path=profile.shards.aggregations/]\n// TEST[continued]\n\n\nThis yields the following aggregation profile output:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"profile\": {\n    \"shards\": [\n      {\n        \"aggregations\": [\n          {\n            \"type\": \"NumericTermsAggregator\",\n            \"description\": \"my_scoped_agg\",\n            \"time_in_nanos\": 79294,\n            \"breakdown\": {\n              \"reduce\": 0,\n              \"build_aggregation\": 30885,\n              \"build_aggregation_count\": 1,\n              \"initialize\": 2623,\n              \"initialize_count\": 1,\n              \"reduce_count\": 0,\n              \"collect\": 45786,\n              \"collect_count\": 4,\n              \"build_leaf_collector\": 18211,\n              \"build_leaf_collector_count\": 1,\n              \"post_collection\": 929,\n              \"post_collection_count\": 1\n            },\n            \"debug\": {\n              \"total_buckets\": 1,\n              \"result_strategy\": \"long_terms\",\n              \"built_buckets\": 1\n            }\n          },\n          {\n            \"type\": \"GlobalAggregator\",\n            \"description\": \"my_global_agg\",\n            \"time_in_nanos\": 104325,\n            \"breakdown\": {\n              \"reduce\": 0,\n              \"build_aggregation\": 22470,\n              \"build_aggregation_count\": 1,\n              \"initialize\": 12454,\n              \"initialize_count\": 1,\n              \"reduce_count\": 0,\n              \"collect\": 69401,\n              \"collect_count\": 4,\n              \"build_leaf_collector\": 8150,\n              \"build_leaf_collector_count\": 1,\n              \"post_collection\": 1584,\n              \"post_collection_count\": 1\n            },\n            \"debug\": {\n              \"built_buckets\": 1\n            },\n            \"children\": [\n              {\n                \"type\": \"NumericTermsAggregator\",\n                \"description\": \"my_level_agg\",\n                \"time_in_nanos\": 76876,\n                \"breakdown\": {\n                  \"reduce\": 0,\n                  \"build_aggregation\": 13824,\n                  \"build_aggregation_count\": 1,\n                  \"initialize\": 1441,\n                  \"initialize_count\": 1,\n                  \"reduce_count\": 0,\n                  \"collect\": 61611,\n                  \"collect_count\": 4,\n                  \"build_leaf_collector\": 5564,\n                  \"build_leaf_collector_count\": 1,\n                  \"post_collection\": 471,\n                  \"post_collection_count\": 1\n                },\n                \"debug\": {\n                  \"total_buckets\": 1,\n                  \"result_strategy\": \"long_terms\",\n                  \"built_buckets\": 1\n                }\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  }\n}\n--------------------------------------------------\n// TESTRESPONSE[s/\\.\\.\\.//]\n// TESTRESPONSE[s/(?<=[\" ])\\d+(\\.\\d+)?/$body.$_path/]\n// TESTRESPONSE[s/\"id\": \"\\[P6-vulHtQRWuD4YnubWb7A\\]\\[my-index-000001\\]\\[0\\]\"/\"id\": $body.profile.shards.0.id/]\n\nFrom the profile structure we can see that the `my_scoped_agg` is internally\nbeing run as a `NumericTermsAggregator` (because the field it is aggregating,\n`http.response.status_code`, is a numeric field). At the same level, we see a `GlobalAggregator`\nwhich comes from `my_global_agg`. That aggregation then has a child\n`NumericTermsAggregator` which comes from the second term's aggregation on `http.response.status_code`.\n\nThe `time_in_nanos` field shows the time executed by each aggregation, and is\ninclusive of all children. While the overall time is useful, the `breakdown`\nfield will give detailed stats about how the time was spent.\n\nSome aggregations may return expert `debug` information that describe features\nof the underlying execution of the aggregation that are 'useful for folks that\nhack on aggregations but that we don't expect to be otherwise useful. They can\nvary wildly between versions, aggregations, and aggregation execution\nstrategies.\n\n===== Timing Breakdown\n\nThe `breakdown` component lists detailed statistics about low-level execution:\n\n[source,js]\n--------------------------------------------------\n\"breakdown\": {\n  \"reduce\": 0,\n  \"build_aggregation\": 30885,\n  \"build_aggregation_count\": 1,\n  \"initialize\": 2623,\n  \"initialize_count\": 1,\n  \"reduce_count\": 0,\n  \"collect\": 45786,\n  \"collect_count\": 4,\n  \"build_leaf_collector\": 18211,\n  \"build_leaf_collector_count\": 1\n}\n--------------------------------------------------\n// NOTCONSOLE\n\nEach property in the `breakdown` component corresponds to an internal method for\nthe aggregation. For example, the `build_leaf_collector` property measures\nnanoseconds spent running the aggregation's `getLeafCollector()` method.\nProperties ending in `_count` record the number of invocations of the particular\nmethod. For example, `\"collect_count\": 2` means the aggregation called the\n`collect()` on two different documents. The `reduce` property is reserved for\nfuture use and always returns `0`.\n\nTimings are listed in wall-clock nanoseconds and are not normalized at all. All\ncaveats about the overall `time` apply here. The intention of the breakdown is\nto give you a feel for A) what machinery in {es} is actually eating time, and B)\nthe magnitude of differences in times between the various components. Like the\noverall time, the breakdown is inclusive of all children times.\n\n[[profiling-fetch]]\n===== Profiling Fetch\n\nAll shards that fetched documents will have a `fetch` section in the profile.\nLet's execute a small search and have a look at the fetch profile:\n\n[source,console]\n----\nGET /my-index-000001/_search?filter_path=profile.shards.fetch\n{\n  \"profile\": true,\n  \"query\": {\n    \"term\": {\n      \"user.id\": {\n        \"value\": \"elkbee\"\n      }\n    }\n  }\n}\n----\n// TEST[continued]\n\nAnd here is the fetch profile:\n\n[source,console-result]\n----\n{\n  \"profile\": {\n    \"shards\": [\n      {\n        \"fetch\": {\n          \"type\": \"fetch\",\n          \"description\": \"\",\n          \"time_in_nanos\": 660555,\n          \"breakdown\": {\n            \"next_reader\": 7292,\n            \"next_reader_count\": 1,\n            \"load_stored_fields\": 299325,\n            \"load_stored_fields_count\": 5,\n            \"load_source\": 3863,\n            \"load_source_count\": 5\n          },\n          \"debug\": {\n            \"stored_fields\": [\"_id\", \"_routing\", \"_source\"]\n          },\n          \"children\": [\n            {\n              \"type\" : \"FetchFieldsPhase\",\n              \"description\" : \"\",\n              \"time_in_nanos\" : 238762,\n              \"breakdown\" : {\n                \"process_count\" : 5,\n                \"process\" : 227914,\n                \"next_reader\" : 10848,\n                \"next_reader_count\" : 1\n              }\n            },\n            {\n              \"type\": \"FetchSourcePhase\",\n              \"description\": \"\",\n              \"time_in_nanos\": 20443,\n              \"breakdown\": {\n                \"next_reader\": 745,\n                \"next_reader_count\": 1,\n                \"process\": 19698,\n                \"process_count\": 5\n              },\n              \"debug\": {\n                \"fast_path\": 4\n              }\n            },\n            {\n              \"type\": \"StoredFieldsPhase\",\n              \"description\": \"\",\n              \"time_in_nanos\": 5310,\n              \"breakdown\": {\n                \"next_reader\": 745,\n                \"next_reader_count\": 1,\n                \"process\": 4445,\n                \"process_count\": 5\n              }\n            }\n          ]\n        }\n      }\n    ]\n  }\n}\n----\n// TESTRESPONSE[s/(?<=[\" ])\\d+(\\.\\d+)?/$body.$_path/]\n\nSince this is debugging information about the way that Elasticsearch executes\nthe fetch it can change from request to request and version to version. Even\npatch versions may change the output here. That lack of consistency is what\nmakes it useful for debugging.\n\nAnyway! `time_in_nanos` measures the time total time of the fetch phase.\nThe `breakdown` counts and times the our\nper-link:{glossary}/terms.html#glossary-segment[segment] preparation in\n`next_reader` and the time taken loading stored fields in `load_stored_fields`.\nDebug contains miscellaneous non-timing information, specifically\n`stored_fields` lists the stored fields that fetch will have to load. If it is\nan empty list then fetch will entirely skip loading stored fields.\n\nThe `children` section lists the sub-phases that do the actual fetching work\nand the `breakdown` has counts and timings for the\nper-link:{glossary}/terms.html#glossary-segment[segment] preparation in\n`next_reader` and the per document fetching in `process`.\n\nNOTE: We try hard to load all of the stored fields that we will need for the\nfetch up front. This tends to make the `_source` phase a couple of microseconds\nper hit. In that case the true cost of `_source` phase is hidden in the\n`load_stored_fields` component of the breakdown. It's possible to entirely skip\nloading stored fields by setting\n`\"_source\": false, \"stored_fields\": [\"_none_\"]`.\n\n[[profiling-dfs]]\n===== Profiling DFS\n\nThe DFS phase runs before the query phase to collect global information\nrelevant to the query. It's currently used in two cases:\n\n. When the `search_type` is set to\n<<profiling-dfs-statistics, `dfs_query_then_fetch`>> and the index has\nmultiple shards.\n. When the search request contains a <<profiling-knn-search, knn section>>.\n\nBoth of these cases can be profiled by setting `profile` to `true` as\npart of the search request.\n\n[[profiling-dfs-statistics]]\n====== Profiling DFS Statistics\n\nWhen the `search_type` is set to `dfs_query_then_fetch` and the index\nhas multiple shards, the dfs phase collects term statistics to improve\nthe relevance of search results.\n\nThe following is an example of setting `profile` to `true` on a search\nthat uses `dfs_query_then_fetch`:\n\nLet's first setup an index with multiple shards and index\na pair of documents with different values on a `keyword` field.\n\n[source,console,id=profile_dfs]\n--------------------------------------------------\nPUT my-dfs-index\n{\n  \"settings\": {\n    \"number_of_shards\": 2, <1>\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n      \"properties\": {\n        \"my-keyword\": { \"type\": \"keyword\" }\n      }\n    }\n}\n\nPOST my-dfs-index/_bulk?refresh=true\n{ \"index\" : { \"_id\" : \"1\" } }\n{ \"my-keyword\" : \"a\" }\n{ \"index\" : { \"_id\" : \"2\" } }\n{ \"my-keyword\" : \"b\" }\n--------------------------------------------------\n<1> The `my-dfs-index` is created with multiple shards.\n\nWith an index setup, we can now profile the dfs phase of a\nsearch query. For this example we use a term query.\n\n[source,console]\n--------------------------------------------------\nGET /my-dfs-index/_search?search_type=dfs_query_then_fetch&pretty&size=0 <1>\n{\n  \"profile\": true, <2>\n  \"query\": {\n    \"term\": {\n      \"my-keyword\": {\n        \"value\": \"a\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[continued]\n<1> The `search_type` url parameter is set to `dfs_query_then_fetch` to\nensure the dfs phase is run.\n<2> The `profile` parameter is set to `true`.\n\nIn the response, we see a profile which includes a `dfs` section\nfor each shard along with profile output for the rest of the search phases.\nOne of the `dfs` sections for a shard looks like the following:\n\n[source,console-result]\n--------------------------------------------------\n\"dfs\" : {\n    \"statistics\" : {\n        \"type\" : \"statistics\",\n        \"description\" : \"collect term statistics\",\n        \"time_in_nanos\" : 236955,\n        \"breakdown\" : {\n            \"term_statistics\" : 4815,\n            \"collection_statistics\" : 27081,\n            \"collection_statistics_count\" : 1,\n            \"create_weight\" : 153278,\n            \"term_statistics_count\" : 1,\n            \"rewrite_count\" : 0,\n            \"create_weight_count\" : 1,\n            \"rewrite\" : 0\n        }\n    }\n}\n--------------------------------------------------\n// TESTRESPONSE[s/^/{\\n\"took\": $body.took,\\n\"timed_out\": $body.timed_out,\\n\"_shards\": $body._shards,\\n\"hits\": $body.hits,\\n\"profile\": {\\n\"shards\": [ \"$body.$_path\", {\\n\"id\": \"$body.$_path\",\\n\"node_id\": \"$body.$_path\",\\n\"shard_id\": \"$body.$_path\",\\n\"index\": \"$body.$_path\",\\n\"cluster\": \"$body.$_path\",\\n/]\n// TESTRESPONSE[s/}$/}, \"aggregations\": [], \"searches\": $body.$_path}]}}/]\n// TESTRESPONSE[s/(\\-)?[0-9]+/ $body.$_path/]\n\nIn the `dfs.statistics` portion of this response we see a `time_in_nanos`\nwhich is the total time it took to collect term statistics for this\nshard along with a further breakdown of the individual parts.\n\n[[profiling-knn-search]]\n====== Profiling kNN Search\n\nA <<approximate-knn, k-nearest neighbor (kNN)>> search runs during\nthe dfs phase.\n\nThe following is an example of setting `profile` to `true` on a search\nthat has a `knn` section:\n\nLet's first setup an index with several dense vectors.\n\n[source,console,id=profile_knn]\n--------------------------------------------------\nPUT my-knn-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my-vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 3,\n        \"index\": true,\n        \"similarity\": \"l2_norm\"\n      }\n    }\n  }\n}\n\nPOST my-knn-index/_bulk?refresh=true\n{ \"index\": { \"_id\": \"1\" } }\n{ \"my-vector\": [1, 5, -20] }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"my-vector\": [42, 8, -15] }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"my-vector\": [15, 11, 23] }\n--------------------------------------------------\n\nWith an index setup, we can now profile a kNN search query.\n\n[source,console]\n--------------------------------------------------\nPOST my-knn-index/_search\n{\n  \"profile\": true, <1>\n  \"knn\": {\n    \"field\": \"my-vector\",\n    \"query_vector\": [-5, 9, -12],\n    \"k\": 3,\n    \"num_candidates\": 100\n  }\n}\n--------------------------------------------------\n// TEST[continued]\n\n<1> The `profile` parameter is set to `true`.\n\nIn the response, we see a profile which includes a `knn` section\nas part of the `dfs` section for each shard along with profile output for the\nrest of the search phases.\n\nOne of the `dfs.knn` sections for a shard looks like the following:\n\n[source,js]\n--------------------------------------------------\n\"dfs\" : {\n    \"knn\" : [\n        {\n        \"vector_operations_count\" : 4,\n        \"query\" : [\n            {\n                \"type\" : \"DocAndScoreQuery\",\n                \"description\" : \"DocAndScoreQuery[0,...][0.008961825,...],0.008961825\",\n                \"time_in_nanos\" : 444414,\n                \"breakdown\" : {\n                  \"set_min_competitive_score_count\" : 0,\n                  \"match_count\" : 0,\n                  \"shallow_advance_count\" : 0,\n                  \"set_min_competitive_score\" : 0,\n                  \"next_doc\" : 1688,\n                  \"match\" : 0,\n                  \"next_doc_count\" : 3,\n                  \"score_count\" : 3,\n                  \"compute_max_score_count\" : 0,\n                  \"compute_max_score\" : 0,\n                  \"advance\" : 4153,\n                  \"advance_count\" : 1,\n                  \"score\" : 2099,\n                  \"build_scorer_count\" : 2,\n                  \"create_weight\" : 128879,\n                  \"shallow_advance\" : 0,\n                  \"create_weight_count\" : 1,\n                  \"build_scorer\" : 307595,\n                  \"count_weight\": 0,\n                  \"count_weight_count\": 0\n                }\n            }\n        ],\n        \"rewrite_time\" : 1275732,\n        \"collector\" : [\n            {\n                \"name\" : \"SimpleTopScoreDocCollector\",\n                \"reason\" : \"search_top_hits\",\n                \"time_in_nanos\" : 17163\n            }\n        ]\n    }   ]\n}\n--------------------------------------------------\n// TESTRESPONSE[s/^/{\\n\"took\": $body.took,\\n\"timed_out\": $body.timed_out,\\n\"_shards\": $body._shards,\\n\"hits\": $body.hits,\\n\"profile\": {\\n\"shards\": [ {\\n\"id\": \"$body.$_path\",\\n\"node_id\": \"$body.$_path\",\\n\"shard_id\": \"$body.$_path\",\\n\"index\": \"$body.$_path\",\\n\"cluster\": \"$body.$_path\",\\n/]\n// TESTRESPONSE[s/}$/}, \"aggregations\": [], \"searches\": $body.$_path, \"fetch\": $body.$_path}]}}/]\n// TESTRESPONSE[s/ (\\-)?[0-9]+/ $body.$_path/]\n// TESTRESPONSE[s/\"dfs\" : \\{/\"dfs\" : {\"statistics\": $body.$_path,/]\n\nIn the `dfs.knn` portion of the response we can see the output\nthe of timings for <<query-section, query>>, <<rewrite-section, rewrite>>,\nand <<collectors-section, collector>>. Unlike many other queries, kNN\nsearch does the bulk of the work during the query rewrite. This means\n`rewrite_time` represents the time spent on kNN search. The attribute `vector_operations_count` represents the overall count of vector operations performed during the kNN search.\n\n[[profiling-considerations]]\n===== Profiling Considerations\n\nLike any profiler, the Profile API introduces a non-negligible overhead to\nsearch execution. The act of instrumenting low-level method calls such as\n`collect`, `advance`, and `next_doc` can be fairly expensive, since these\nmethods are called in tight loops. Therefore, profiling should not be enabled\nin production settings by default, and should not be compared against\nnon-profiled query times. Profiling is just a diagnostic tool.\n\nThere are also cases where special Lucene optimizations are disabled, since they\nare not amenable to profiling. This could cause some queries to report larger\nrelative times than their non-profiled counterparts, but in general should not\nhave a drastic effect compared to other components in the profiled query.\n\n[[profile-limitations]]\n===== Limitations\n\n- Profiling currently does not measure the network overhead.\n- Profiling also does not account for time spent in the queue, merging shard\nresponses on the coordinating node, or additional work such as building global\nordinals (an internal data structure used to speed up search).\n- Profiling statistics are currently not available for suggestions.\n- Profiling of the reduce phase of aggregation is currently not available.\n- The Profiler is instrumenting internals that can change from version to\nversion. The resulting json should be considered mostly unstable, especially\nthings in the `debug` section.\n"
}