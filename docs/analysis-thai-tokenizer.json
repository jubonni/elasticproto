{
    "meta": {
        "timestamp": "2024-11-01T02:49:26.661068",
        "size": 2146,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-thai-tokenizer.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "analysis-thai-tokenizer",
        "version": "8.15"
    },
    "doc": "[[analysis-thai-tokenizer]]\n=== Thai tokenizer\n++++\n<titleabbrev>Thai</titleabbrev>\n++++\n\nThe `thai` tokenizer segments Thai text into words, using the Thai\nsegmentation algorithm included with Java. Text in other languages in general\nwill be treated the same as the\n<<analysis-standard-tokenizer,`standard` tokenizer>>.\n\nWARNING: This tokenizer may not be supported by all JREs. It is known to work\nwith Sun/Oracle and OpenJDK. If your application needs to be fully portable,\nconsider using the {plugins}/analysis-icu-tokenizer.html[ICU Tokenizer] instead.\n\n[discrete]\n=== Example output\n\n[source,console]\n---------------------------\nPOST _analyze\n{\n  \"tokenizer\": \"thai\",\n  \"text\": \"\u0e01\u0e32\u0e23\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e15\u0e49\u0e2d\u0e07\u0e41\u0e2a\u0e14\u0e07\u0e27\u0e48\u0e32\u0e07\u0e32\u0e19\u0e14\u0e35\"\n}\n---------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"\u0e01\u0e32\u0e23\",\n      \"start_offset\": 0,\n      \"end_offset\": 3,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"\u0e17\u0e35\u0e48\",\n      \"start_offset\": 3,\n      \"end_offset\": 6,\n      \"type\": \"word\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"\u0e44\u0e14\u0e49\",\n      \"start_offset\": 6,\n      \"end_offset\": 9,\n      \"type\": \"word\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"\u0e15\u0e49\u0e2d\u0e07\",\n      \"start_offset\": 9,\n      \"end_offset\": 13,\n      \"type\": \"word\",\n      \"position\": 3\n    },\n    {\n      \"token\": \"\u0e41\u0e2a\u0e14\u0e07\",\n      \"start_offset\": 13,\n      \"end_offset\": 17,\n      \"type\": \"word\",\n      \"position\": 4\n    },\n    {\n      \"token\": \"\u0e27\u0e48\u0e32\",\n      \"start_offset\": 17,\n      \"end_offset\": 20,\n      \"type\": \"word\",\n      \"position\": 5\n    },\n    {\n      \"token\": \"\u0e07\u0e32\u0e19\",\n      \"start_offset\": 20,\n      \"end_offset\": 23,\n      \"type\": \"word\",\n      \"position\": 6\n    },\n    {\n      \"token\": \"\u0e14\u0e35\",\n      \"start_offset\": 23,\n      \"end_offset\": 25,\n      \"type\": \"word\",\n      \"position\": 7\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe above sentence would produce the following terms:\n\n[source,text]\n---------------------------\n[ \u0e01\u0e32\u0e23, \u0e17\u0e35\u0e48, \u0e44\u0e14\u0e49, \u0e15\u0e49\u0e2d\u0e07, \u0e41\u0e2a\u0e14\u0e07, \u0e27\u0e48\u0e32, \u0e07\u0e32\u0e19, \u0e14\u0e35 ]\n---------------------------\n\n[discrete]\n=== Configuration\n\nThe `thai` tokenizer is not configurable.\n"
}