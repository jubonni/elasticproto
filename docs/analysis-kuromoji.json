{
    "meta": {
        "timestamp": "2024-11-01T03:07:08.745294",
        "size": 17837,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-kuromoji.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "analysis-kuromoji",
        "version": "8.15"
    },
    "doc": "[[analysis-kuromoji]]\n=== Japanese (kuromoji) analysis plugin\n\nThe Japanese (kuromoji) analysis plugin integrates Lucene kuromoji analysis\nmodule into {es}.\n\n:plugin_name: analysis-kuromoji\ninclude::install_remove.asciidoc[]\n\n[[analysis-kuromoji-analyzer]]\n==== `kuromoji` analyzer\n\nThe `kuromoji` analyzer uses the following analysis chain:\n\n* `CJKWidthCharFilter` from Lucene\n* <<analysis-kuromoji-tokenizer,`kuromoji_tokenizer`>>\n* <<analysis-kuromoji-baseform,`kuromoji_baseform`>> token filter\n* <<analysis-kuromoji-speech,`kuromoji_part_of_speech`>> token filter\n* <<analysis-kuromoji-stop,`ja_stop`>> token filter\n* <<analysis-kuromoji-stemmer,`kuromoji_stemmer`>> token filter\n* {ref}/analysis-lowercase-tokenfilter.html[`lowercase`] token filter\n\nIt supports the `mode` and `user_dictionary` settings from\n<<analysis-kuromoji-tokenizer,`kuromoji_tokenizer`>>.\n\n[discrete]\n[[kuromoji-analyzer-normalize-full-width-characters]]\n==== Normalize full-width characters\n\nThe `kuromoji_tokenizer` tokenizer uses characters from the MeCab-IPADIC\ndictionary to split text into tokens. The dictionary includes some full-width\ncharacters, such as `\uff4f` and `\uff46`. If a text contains full-width characters,\nthe tokenizer can produce unexpected tokens.\n\nFor example, the `kuromoji_tokenizer` tokenizer converts the text\n`\uff23\uff55\uff4c\uff54\uff55\uff52\uff45\u3000\uff4f\uff46\u3000\uff2a\uff41\uff50\uff41\uff4e` to the tokens `[ culture, o, f, japan ]`\ninstead of `[ culture, of, japan ]`.\n\nTo avoid this, add the <<analysis-icu-normalization-charfilter,`icu_normalizer`\ncharacter filter>> to a custom analyzer based on the `kuromoji` analyzer. The\n`icu_normalizer` character filter converts full-width characters to their normal\nequivalents.\n\nFirst, duplicate the `kuromoji` analyzer to create the basis for a custom\nanalyzer. Then add the `icu_normalizer` character filter to the custom analyzer.\nFor example:\n\n[source,console]\n----\nPUT index-00001\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"kuromoji_normalize\": {                 <1>\n            \"char_filter\": [\n              \"icu_normalizer\"                    <2>\n            ],\n            \"tokenizer\": \"kuromoji_tokenizer\",\n            \"filter\": [\n              \"kuromoji_baseform\",\n              \"kuromoji_part_of_speech\",\n              \"cjk_width\",\n              \"ja_stop\",\n              \"kuromoji_stemmer\",\n              \"lowercase\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n----\n<1> Creates a new custom analyzer, `kuromoji_normalize`, based on the `kuromoji`\nanalyzer.\n<2> Adds the `icu_normalizer` character filter to the analyzer.\n\n\n[[analysis-kuromoji-charfilter]]\n==== `kuromoji_iteration_mark` character filter\n\nThe `kuromoji_iteration_mark` normalizes Japanese horizontal iteration marks\n(_odoriji_) to their expanded form. It accepts the following settings:\n\n`normalize_kanji`::\n\n    Indicates whether kanji iteration marks should be normalized. Defaults to `true`.\n\n`normalize_kana`::\n\n    Indicates whether kana iteration marks should be normalized. Defaults to `true`\n\n\n[[analysis-kuromoji-tokenizer]]\n==== `kuromoji_tokenizer`\n\nThe `kuromoji_tokenizer` accepts the following settings:\n\n`mode`::\n+\n--\n\nThe tokenization mode determines how the tokenizer handles compound and\nunknown words. It can be set to:\n\n`normal`::\n\n    Normal segmentation, no decomposition for compounds. Example output:\n\n    \u95a2\u897f\u56fd\u969b\u7a7a\u6e2f\n    \u30a2\u30d6\u30e9\u30ab\u30c0\u30d6\u30e9\n\n`search`::\n\n    Segmentation geared towards search. This includes a decompounding process\n    for long nouns, also including the full compound token as a synonym.\n    Example output:\n\n    \u95a2\u897f, \u95a2\u897f\u56fd\u969b\u7a7a\u6e2f, \u56fd\u969b, \u7a7a\u6e2f\n    \u30a2\u30d6\u30e9\u30ab\u30c0\u30d6\u30e9\n\n`extended`::\n\n    Extended mode outputs unigrams for unknown words. Example output:\n\n    \u95a2\u897f, \u95a2\u897f\u56fd\u969b\u7a7a\u6e2f, \u56fd\u969b, \u7a7a\u6e2f\n    \u30a2, \u30d6, \u30e9, \u30ab, \u30c0, \u30d6, \u30e9\n--\n\n`discard_punctuation`::\n\n    Whether punctuation should be discarded from the output. Defaults to `true`.\n\n`lenient`::\n\n    Whether the `user_dictionary` should be deduplicated on the provided `text`.\n    False by default causing duplicates to generate an error.\n\n`user_dictionary`::\n+\n--\nThe Kuromoji tokenizer uses the MeCab-IPADIC dictionary by default. A `user_dictionary`\nmay be appended to the default dictionary. The dictionary should have the following CSV format:\n\n[source,csv]\n-----------------------\n<text>,<token 1> ... <token n>,<reading 1> ... <reading n>,<part-of-speech tag>\n-----------------------\n--\n\nAs a demonstration of how the user dictionary can be used, save the following\ndictionary to `$ES_HOME/config/userdict_ja.txt`:\n\n[source,csv]\n-----------------------\n\u6771\u4eac\u30b9\u30ab\u30a4\u30c4\u30ea\u30fc,\u6771\u4eac \u30b9\u30ab\u30a4\u30c4\u30ea\u30fc,\u30c8\u30a6\u30ad\u30e7\u30a6 \u30b9\u30ab\u30a4\u30c4\u30ea\u30fc,\u30ab\u30b9\u30bf\u30e0\u540d\u8a5e\n-----------------------\n\n--\n\nYou can also inline the rules directly in the tokenizer definition using\nthe `user_dictionary_rules` option:\n\n[source,console]\n--------------------------------------------------\nPUT kuromoji_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"tokenizer\": {\n          \"kuromoji_user_dict\": {\n            \"type\": \"kuromoji_tokenizer\",\n            \"mode\": \"extended\",\n            \"user_dictionary_rules\": [\"\u6771\u4eac\u30b9\u30ab\u30a4\u30c4\u30ea\u30fc,\u6771\u4eac \u30b9\u30ab\u30a4\u30c4\u30ea\u30fc,\u30c8\u30a6\u30ad\u30e7\u30a6 \u30b9\u30ab\u30a4\u30c4\u30ea\u30fc,\u30ab\u30b9\u30bf\u30e0\u540d\u8a5e\"]\n          }\n        },\n        \"analyzer\": {\n          \"my_analyzer\": {\n            \"type\": \"custom\",\n            \"tokenizer\": \"kuromoji_user_dict\"\n          }\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n--\n\n`nbest_cost`/`nbest_examples`::\n+\n--\nAdditional expert user parameters `nbest_cost` and `nbest_examples` can be used\nto include additional tokens that are most likely according to the statistical model.\nIf both parameters are used, the largest number of both is applied.\n\n`nbest_cost`::\n\n    The `nbest_cost` parameter specifies an additional Viterbi cost.\n    The KuromojiTokenizer will include all tokens in Viterbi paths that are\n    within the nbest_cost value of the best path.\n\n`nbest_examples`::\n\n    The `nbest_examples` can be used to find a `nbest_cost` value based on examples.\n    For example, a value of /\u7bb1\u6839\u5c71-\u7bb1\u6839/\u6210\u7530\u7a7a\u6e2f-\u6210\u7530/ indicates that in the texts,\n    \u7bb1\u6839\u5c71 (Mt. Hakone) and \u6210\u7530\u7a7a\u6e2f (Narita Airport) we'd like a cost that gives is us\n    \u7bb1\u6839 (Hakone) and \u6210\u7530 (Narita).\n--\n\n\nThen create an analyzer as follows:\n\n[source,console]\n--------------------------------------------------\nPUT kuromoji_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"tokenizer\": {\n          \"kuromoji_user_dict\": {\n            \"type\": \"kuromoji_tokenizer\",\n            \"mode\": \"extended\",\n            \"discard_punctuation\": \"false\",\n            \"user_dictionary\": \"userdict_ja.txt\",\n            \"lenient\": \"true\"\n          }\n        },\n        \"analyzer\": {\n          \"my_analyzer\": {\n            \"type\": \"custom\",\n            \"tokenizer\": \"kuromoji_user_dict\"\n          }\n        }\n      }\n    }\n  }\n}\n\nGET kuromoji_sample/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"\u6771\u4eac\u30b9\u30ab\u30a4\u30c4\u30ea\u30fc\"\n}\n--------------------------------------------------\n\nThe above `analyze` request returns the following:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"tokens\" : [ {\n    \"token\" : \"\u6771\u4eac\",\n    \"start_offset\" : 0,\n    \"end_offset\" : 2,\n    \"type\" : \"word\",\n    \"position\" : 0\n  }, {\n    \"token\" : \"\u30b9\u30ab\u30a4\u30c4\u30ea\u30fc\",\n    \"start_offset\" : 2,\n    \"end_offset\" : 8,\n    \"type\" : \"word\",\n    \"position\" : 1\n  } ]\n}\n--------------------------------------------------\n\n`discard_compound_token`::\n    Whether original compound tokens should be discarded from the output with `search` mode. Defaults to `false`.\n    Example output with `search` or `extended` mode and this option `true`:\n\n    \u95a2\u897f, \u56fd\u969b, \u7a7a\u6e2f\n\nNOTE: If a text contains full-width characters, the `kuromoji_tokenizer`\ntokenizer can produce unexpected tokens. To avoid this, add the\n<<analysis-icu-normalization-charfilter,`icu_normalizer` character filter>> to\nyour analyzer. See <<kuromoji-analyzer-normalize-full-width-characters>>.\n\n\n[[analysis-kuromoji-baseform]]\n==== `kuromoji_baseform` token filter\n\nThe `kuromoji_baseform` token filter replaces terms with their\nBaseFormAttribute. This acts as a lemmatizer for verbs and adjectives. Example:\n\n[source,console]\n--------------------------------------------------\nPUT kuromoji_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"my_analyzer\": {\n            \"tokenizer\": \"kuromoji_tokenizer\",\n            \"filter\": [\n              \"kuromoji_baseform\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n\nGET kuromoji_sample/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"\u98f2\u307f\"\n}\n--------------------------------------------------\n\nwhich responds with:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"tokens\" : [ {\n    \"token\" : \"\u98f2\u3080\",\n    \"start_offset\" : 0,\n    \"end_offset\" : 2,\n    \"type\" : \"word\",\n    \"position\" : 0\n  } ]\n}\n--------------------------------------------------\n\n\n[[analysis-kuromoji-speech]]\n==== `kuromoji_part_of_speech` token filter\n\nThe `kuromoji_part_of_speech` token filter removes tokens that match a set of\npart-of-speech tags. It accepts the following setting:\n\n`stoptags`::\n\n    An array of part-of-speech tags that should be removed. It defaults to the\n    `stoptags.txt` file embedded in the `lucene-analyzer-kuromoji.jar`.\n\nFor example:\n\n[source,console]\n--------------------------------------------------\nPUT kuromoji_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"my_analyzer\": {\n            \"tokenizer\": \"kuromoji_tokenizer\",\n            \"filter\": [\n              \"my_posfilter\"\n            ]\n          }\n        },\n        \"filter\": {\n          \"my_posfilter\": {\n            \"type\": \"kuromoji_part_of_speech\",\n            \"stoptags\": [\n              \"\u52a9\u8a5e-\u683c\u52a9\u8a5e-\u4e00\u822c\",\n              \"\u52a9\u8a5e-\u7d42\u52a9\u8a5e\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n\nGET kuromoji_sample/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"\u5bff\u53f8\u304c\u304a\u3044\u3057\u3044\u306d\"\n}\n--------------------------------------------------\n\nWhich responds with:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"tokens\" : [ {\n    \"token\" : \"\u5bff\u53f8\",\n    \"start_offset\" : 0,\n    \"end_offset\" : 2,\n    \"type\" : \"word\",\n    \"position\" : 0\n  }, {\n    \"token\" : \"\u304a\u3044\u3057\u3044\",\n    \"start_offset\" : 3,\n    \"end_offset\" : 7,\n    \"type\" : \"word\",\n    \"position\" : 2\n  } ]\n}\n--------------------------------------------------\n\n\n[[analysis-kuromoji-readingform]]\n==== `kuromoji_readingform` token filter\n\nThe `kuromoji_readingform` token filter replaces the token with its reading\nform in either katakana or romaji. It accepts the following setting:\n\n`use_romaji`::\n\n    Whether romaji reading form should be output instead of katakana. Defaults to `false`.\n\nWhen using the pre-defined `kuromoji_readingform` filter, `use_romaji` is set\nto `true`. The default when defining a custom `kuromoji_readingform`, however,\nis `false`. The only reason to use the custom form is if you need the\nkatakana reading form:\n\n[source,console]\n--------------------------------------------------\nPUT kuromoji_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"romaji_analyzer\": {\n            \"tokenizer\": \"kuromoji_tokenizer\",\n            \"filter\": [ \"romaji_readingform\" ]\n          },\n          \"katakana_analyzer\": {\n            \"tokenizer\": \"kuromoji_tokenizer\",\n            \"filter\": [ \"katakana_readingform\" ]\n          }\n        },\n        \"filter\": {\n          \"romaji_readingform\": {\n            \"type\": \"kuromoji_readingform\",\n            \"use_romaji\": true\n          },\n          \"katakana_readingform\": {\n            \"type\": \"kuromoji_readingform\",\n            \"use_romaji\": false\n          }\n        }\n      }\n    }\n  }\n}\n\nGET kuromoji_sample/_analyze\n{\n  \"analyzer\": \"katakana_analyzer\",\n  \"text\": \"\u5bff\u53f8\" <1>\n}\n\nGET kuromoji_sample/_analyze\n{\n  \"analyzer\": \"romaji_analyzer\",\n  \"text\": \"\u5bff\u53f8\" <2>\n}\n--------------------------------------------------\n\n<1> Returns `\u30b9\u30b7`.\n<2> Returns `sushi`.\n\n[[analysis-kuromoji-stemmer]]\n==== `kuromoji_stemmer` token filter\n\nThe `kuromoji_stemmer` token filter normalizes common katakana spelling\nvariations ending in a long sound character by removing this character\n(U+30FC). Only full-width katakana characters are supported.\n\nThis token filter accepts the following setting:\n\n`minimum_length`::\n\n    Katakana words shorter than the `minimum length` are not stemmed (default\n    is `4`).\n\n\n[source,console]\n--------------------------------------------------\nPUT kuromoji_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"my_analyzer\": {\n            \"tokenizer\": \"kuromoji_tokenizer\",\n            \"filter\": [\n              \"my_katakana_stemmer\"\n            ]\n          }\n        },\n        \"filter\": {\n          \"my_katakana_stemmer\": {\n            \"type\": \"kuromoji_stemmer\",\n            \"minimum_length\": 4\n          }\n        }\n      }\n    }\n  }\n}\n\nGET kuromoji_sample/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"\u30b3\u30d4\u30fc\" <1>\n}\n\nGET kuromoji_sample/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"\u30b5\u30fc\u30d0\u30fc\" <2>\n}\n--------------------------------------------------\n\n<1> Returns `\u30b3\u30d4\u30fc`.\n<2> Return `\u30b5\u30fc\u30d0`.\n\n\n[[analysis-kuromoji-stop]]\n==== `ja_stop` token filter\n\nThe `ja_stop` token filter filters out Japanese stopwords (`_japanese_`), and\nany other custom stopwords specified by the user. This filter only supports\nthe predefined `_japanese_` stopwords list. If you want to use a different\npredefined list, then use the\n{ref}/analysis-stop-tokenfilter.html[`stop` token filter] instead.\n\n[source,console]\n--------------------------------------------------\nPUT kuromoji_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"analyzer_with_ja_stop\": {\n            \"tokenizer\": \"kuromoji_tokenizer\",\n            \"filter\": [\n              \"ja_stop\"\n            ]\n          }\n        },\n        \"filter\": {\n          \"ja_stop\": {\n            \"type\": \"ja_stop\",\n            \"stopwords\": [\n              \"_japanese_\",\n              \"\u30b9\u30c8\u30c3\u30d7\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n\nGET kuromoji_sample/_analyze\n{\n  \"analyzer\": \"analyzer_with_ja_stop\",\n  \"text\": \"\u30b9\u30c8\u30c3\u30d7\u306f\u6d88\u3048\u308b\"\n}\n--------------------------------------------------\n\nThe above request returns:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"tokens\" : [ {\n    \"token\" : \"\u6d88\u3048\u308b\",\n    \"start_offset\" : 5,\n    \"end_offset\" : 8,\n    \"type\" : \"word\",\n    \"position\" : 2\n  } ]\n}\n--------------------------------------------------\n\n\n[[analysis-kuromoji-number]]\n==== `kuromoji_number` token filter\n\nThe `kuromoji_number` token filter normalizes Japanese numbers (kans\u016bji)\nto regular Arabic decimal numbers in half-width characters. For example:\n\n[source,console]\n--------------------------------------------------\nPUT kuromoji_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"my_analyzer\": {\n            \"tokenizer\": \"kuromoji_tokenizer\",\n            \"filter\": [\n              \"kuromoji_number\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n\nGET kuromoji_sample/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"\u4e00\u3007\u3007\u3007\"\n}\n--------------------------------------------------\n\nWhich results in:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"tokens\" : [ {\n    \"token\" : \"1000\",\n    \"start_offset\" : 0,\n    \"end_offset\" : 4,\n    \"type\" : \"word\",\n    \"position\" : 0\n  } ]\n}\n--------------------------------------------------\n\n[[analysis-kuromoji-hiragana-uppercase]]\n==== `hiragana_uppercase` token filter\n\nThe `hiragana_uppercase` token filter normalizes small letters (\u6368\u3066\u4eee\u540d) in hiragana into standard letters.\nThis filter is useful if you want to search against old style Japanese text such as\npatents, legal documents, contract policies, etc.\n\nFor example:\n\n[source,console]\n--------------------------------------------------\nPUT kuromoji_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"my_analyzer\": {\n            \"tokenizer\": \"kuromoji_tokenizer\",\n            \"filter\": [\n              \"hiragana_uppercase\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n\nGET kuromoji_sample/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"\u3061\u3087\u3063\u3068\u307e\u3063\u3066\"\n}\n--------------------------------------------------\n\nWhich results in:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"\u3061\u3088\u3064\u3068\",\n      \"start_offset\": 0,\n      \"end_offset\": 4,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"\u307e\u3064\",\n      \"start_offset\": 4,\n      \"end_offset\": 6,\n      \"type\": \"word\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"\u3066\",\n      \"start_offset\": 6,\n      \"end_offset\": 7,\n      \"type\": \"word\",\n      \"position\": 2\n    }\n  ]\n}\n--------------------------------------------------\n\n[[analysis-kuromoji-katakana-uppercase]]\n==== `katakana_uppercase` token filter\n\nThe `katakana_uppercase` token filter normalizes small letters (\u6368\u3066\u4eee\u540d) in katakana into standard letters.\nThis filter is useful if you want to search against old style Japanese text such as\npatents, legal documents, contract policies, etc.\n\nFor example:\n\n[source,console]\n--------------------------------------------------\nPUT kuromoji_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"my_analyzer\": {\n            \"tokenizer\": \"kuromoji_tokenizer\",\n            \"filter\": [\n              \"katakana_uppercase\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n\nGET kuromoji_sample/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"\u30b9\u30c8\u30c3\u30d7\u30a6\u30a9\u30c3\u30c1\"\n}\n--------------------------------------------------\n\nWhich results in:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"\u30b9\u30c8\u30c4\u30d7\u30a6\u30aa\u30c4\u30c1\",\n      \"start_offset\": 0,\n      \"end_offset\": 8,\n      \"type\": \"word\",\n      \"position\": 0\n    }\n  ]\n}\n--------------------------------------------------\n"
}