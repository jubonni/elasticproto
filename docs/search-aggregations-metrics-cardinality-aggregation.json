{
    "meta": {
        "timestamp": "2024-11-01T02:49:24.592066",
        "size": 8183,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "search-aggregations-metrics-cardinality-aggregation",
        "version": "8.15"
    },
    "doc": "[[search-aggregations-metrics-cardinality-aggregation]]\n=== Cardinality aggregation\n++++\n<titleabbrev>Cardinality</titleabbrev>\n++++\n\nA `single-value` metrics aggregation that calculates an approximate count of\ndistinct values.\n\nAssume you are indexing store sales and would like to count the unique number of sold products that match a query:\n\n[source,console]\n--------------------------------------------------\nPOST /sales/_search?size=0\n{\n  \"aggs\": {\n    \"type_count\": {\n      \"cardinality\": {\n        \"field\": \"type\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:sales]\n\nResponse:\n\n[source,console-result]\n--------------------------------------------------\n{\n  ...\n  \"aggregations\": {\n    \"type_count\": {\n      \"value\": 3\n    }\n  }\n}\n--------------------------------------------------\n// TESTRESPONSE[s/\\.\\.\\./\"took\": $body.took,\"timed_out\": false,\"_shards\": $body._shards,\"hits\": $body.hits,/]\n\n==== Precision control\n\nThis aggregation also supports the `precision_threshold` option:\n\n[source,console]\n--------------------------------------------------\nPOST /sales/_search?size=0\n{\n  \"aggs\": {\n    \"type_count\": {\n      \"cardinality\": {\n        \"field\": \"type\",\n        \"precision_threshold\": 100 <1>\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:sales]\n\n<1> The `precision_threshold` options allows to trade memory for accuracy, and\ndefines a unique count below which counts are expected to be close to\naccurate. Above this value, counts might become a bit more fuzzy. The maximum\nsupported value is 40000, thresholds above this number will have the same\neffect as a threshold of 40000. The default value is +3000+.\n\n==== Counts are approximate\n\nComputing exact counts requires loading values into a hash set and returning its\nsize. This doesn't scale when working on high-cardinality sets and/or large\nvalues as the required memory usage and the need to communicate those\nper-shard sets between nodes would utilize too many resources of the cluster.\n\nThis `cardinality` aggregation is based on the\nhttps://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/40671.pdf[HyperLogLog++]\nalgorithm, which counts based on the hashes of the values with some interesting\nproperties:\n\n// tag::explanation[]\n\n * configurable precision, which decides on how to trade memory for accuracy,\n * excellent accuracy on low-cardinality sets,\n * fixed memory usage: no matter if there are tens or billions of unique values,\n   memory usage only depends on the configured precision.\n\nFor a precision threshold of `c`, the implementation that we are using requires\nabout `c * 8` bytes.\n\nThe following chart shows how the error varies before and after the threshold:\n\n////\nTo generate this chart use this gnuplot script:\n[source,gnuplot]\n-------\n#!/usr/bin/gnuplot\nreset\nset terminal png size 1000,400\n\nset xlabel \"Actual cardinality\"\nset logscale x\n\nset ylabel \"Relative error (%)\"\nset yrange [0:8]\n\nset title \"Cardinality error\"\nset grid\n\nset style data lines\n\nplot \"test.dat\" using 1:2 title \"threshold=100\", \\\n\"\" using 1:3 title \"threshold=1000\", \\\n\"\" using 1:4 title \"threshold=10000\"\n#\n-------\n\nand generate data in a 'test.dat' file using the below Java code:\n\n[source,java]\n-------\nprivate static double error(HyperLogLogPlusPlus h, long expected) {\n    double actual = h.cardinality(0);\n    return Math.abs(expected - actual) / expected;\n}\n\npublic static void main(String[] args) {\n    HyperLogLogPlusPlus h100 = new HyperLogLogPlusPlus(precisionFromThreshold(100), BigArrays.NON_RECYCLING_INSTANCE, 1);\n    HyperLogLogPlusPlus h1000 = new HyperLogLogPlusPlus(precisionFromThreshold(1000), BigArrays.NON_RECYCLING_INSTANCE, 1);\n    HyperLogLogPlusPlus h10000 = new HyperLogLogPlusPlus(precisionFromThreshold(10000), BigArrays.NON_RECYCLING_INSTANCE, 1);\n\n    int next = 100;\n    int step = 10;\n\n    for (int i = 1; i <= 10000000; ++i) {\n        long h = BitMixer.mix64(i);\n        h100.collect(0, h);\n        h1000.collect(0, h);\n        h10000.collect(0, h);\n\n        if (i == next) {\n            System.out.println(i + \" \" + error(h100, i)*100 + \" \" + error(h1000, i)*100 + \" \" + error(h10000, i)*100);\n            next += step;\n            if (next >= 100 * step) {\n                step *= 10;\n            }\n        }\n    }\n}\n-------\n\n////\n\nimage:images/cardinality_error.png[]\n\nFor all 3 thresholds, counts have been accurate up to the configured threshold.\nAlthough not guaranteed, this is likely to be the case. Accuracy in practice depends\non the dataset in question. In general, most datasets show consistently good\naccuracy. Also note that even with a threshold as low as 100, the error\nremains very low (1-6% as seen in the above graph) even when counting millions of items.\n\nThe HyperLogLog++ algorithm depends on the leading zeros of hashed\nvalues, the exact distributions of hashes in a dataset can affect the\naccuracy of the cardinality.\n\n// end::explanation[]\n\n==== Pre-computed hashes\n\nOn string fields that have a high cardinality, it might be faster to store the\nhash of your field values in your index and then run the cardinality aggregation\non this field. This can either be done by providing hash values from client-side\nor by letting Elasticsearch compute hash values for you by using the\n{plugins}/mapper-murmur3.html[`mapper-murmur3`] plugin.\n\nNOTE: Pre-computing hashes is usually only useful on very large and/or\nhigh-cardinality fields as it saves CPU and memory. However, on numeric\nfields, hashing is very fast and storing the original values requires as much\nor less memory than storing the hashes. This is also true on low-cardinality\nstring fields, especially given that those have an optimization in order to\nmake sure that hashes are computed at most once per unique value per segment.\n\n==== Script\n\nIf you need the cardinality of the combination of two fields,\ncreate a <<runtime,runtime field>> combining them and aggregate it.\n\n[source,console]\n----\nPOST /sales/_search?size=0\n{\n  \"runtime_mappings\": {\n    \"type_and_promoted\": {\n      \"type\": \"keyword\",\n      \"script\": \"emit(doc['type'].value + ' ' + doc['promoted'].value)\"\n    }\n  },\n  \"aggs\": {\n    \"type_promoted_count\": {\n      \"cardinality\": {\n        \"field\": \"type_and_promoted\"\n      }\n    }\n  }\n}\n----\n// TEST[setup:sales]\n// TEST[s/size=0/size=0&filter_path=aggregations/]\n\n////\n[source,console-result]\n--------------------------------------------------\n{\n  \"aggregations\": {\n    \"type_promoted_count\": {\n      \"value\": 5\n    }\n  }\n}\n--------------------------------------------------\n////\n\n==== Missing value\n\nThe `missing` parameter defines how documents that are missing a value should be treated.\nBy default they will be ignored but it is also possible to treat them as if they\nhad a value.\n\n[source,console]\n--------------------------------------------------\nPOST /sales/_search?size=0\n{\n  \"aggs\": {\n    \"tag_cardinality\": {\n      \"cardinality\": {\n        \"field\": \"tag\",\n        \"missing\": \"N/A\" <1>\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:sales]\n<1> Documents without a value in the `tag` field will fall into the same bucket as documents that have the value `N/A`.\n\n==== Execution hint\n\nYou can run cardinality aggregations using different mechanisms:\n\n - by using field values directly (`direct`)\n - by using global ordinals of the field and resolving those values after\n   finishing a shard (`global_ordinals`)\n - by using segment ordinal values and resolving those values after each\n   segment (`segment_ordinals`)\n\nAdditionally, there are two \"heuristic based\" modes.  These modes will cause\n{es} to use some data about the state of the index to choose an\nappropriate execution method.  The two heuristics are:\n\n - `save_time_heuristic` - this is the default in {es} 8.4 and later.\n - `save_memory_heuristic` - this was the default in {es} 8.3 and\n   earlier\n\nWhen not specified, {es} will apply a heuristic to choose the\nappropriate mode.  Also note that for some data (non-ordinal fields), `direct`\nis the only option, and the hint will be ignored in these cases.  Generally\nspeaking, it should not be necessary to set this value.\n"
}