{
    "meta": {
        "timestamp": "2024-11-01T03:02:52.771586",
        "size": 3738,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/index-rollover.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "index-rollover",
        "version": "8.15"
    },
    "doc": "[[index-rollover]]\n=== Rollover\n\nWhen indexing time series data like logs or metrics, you can't write to a single index indefinitely. \nTo meet your indexing and search performance requirements and manage resource usage, \nyou write to an index until some threshold is met and then create a new index and start writing to it instead. \nUsing rolling indices enables you to:\n\n* Optimize the active index for high ingest rates on high-performance _hot_ nodes.\n* Optimize for search performance on _warm_ nodes.\n* Shift older, less frequently accessed data to less expensive _cold_ nodes,\n* Delete data according to your retention policies by removing entire indices.\n\nWe recommend using <<indices-create-data-stream, data streams>> to manage time series\ndata. Data streams automatically track the write index while keeping configuration to a minimum.\n\nEach data stream requires an <<index-templates,index template>> that contains:\n\n* A name or wildcard (`*`) pattern for the data stream.\n\n* The data stream's timestamp field. This field must be mapped as a\n  <<date,`date`>> or <<date_nanos,`date_nanos`>> field data type and must be\n  included in every document indexed to the data stream.\n  \n  * The mappings and settings applied to each backing index when it's created.\n\nData streams are designed for append-only data, where the data stream name\ncan be used as the operations (read, write, rollover, shrink etc.) target.\nIf your use case requires data to be updated in place, you can instead manage\nyour time series data using <<aliases,index aliases>>. However, there are a few\nmore configuration steps and concepts:\n\n* An _index template_ that specifies the settings for each new index in the series.\nYou optimize this configuration for ingestion, typically using as many shards as you have hot nodes.\n* An _index alias_ that references the entire set of indices. \n* A single index designated as the _write index_.\nThis is the active index that handles all write requests. \nOn each rollover, the new index becomes the write index. \n\n[NOTE]\n====\nWhen an index is rolled over, the previous index's age is updated to reflect the rollover time. \nThis date, rather than the index's `creation_date`, is used in {ilm} \n`min_age` phase calculations. <<min-age-calculation,Learn more>>.\n====\n\n[discrete]\n[[ilm-automatic-rollover]]\n=== Automatic rollover\n\n{ilm-init} and the data stream lifecycle (in preview:[]]) enable you to automatically roll over to a new index based\non conditions like the index size, document count, or age. When a rollover is triggered, a new\nindex is created, the write alias is updated to point to the new index, and all\nsubsequent updates are written to the new index.\n\nTIP: Rolling over to a new index based on size, document count, or age is preferable\nto time-based rollovers. Rolling over at an arbitrary time often results in\nmany small indices, which can have a negative impact on performance and\nresource usage.\n\nIMPORTANT: Empty indices will not be rolled over, even if they have an associated `max_age` that\nwould otherwise result in a roll over occurring. A policy can override this behavior, and explicitly\nopt in to rolling over empty indices, by adding a `\"min_docs\": 0` condition. This can also be\ndisabled on a cluster-wide basis by setting `indices.lifecycle.rollover.only_if_has_documents` to\n`false`.\n\nIMPORTANT: The rollover action implicitly always rolls over a data stream or alias if one or more shards contain\n200000000 or more documents. Normally a shard will reach 50GB long before it reaches 200M documents,\nbut this isn't the case for space efficient data sets. Search performance will very likely suffer\nif a shard contains more than 200M documents. This is the reason of the builtin limit.\n\n"
}