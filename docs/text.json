{
    "meta": {
        "timestamp": "2024-11-01T02:49:26.660068",
        "size": 11998,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/text.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "text",
        "version": "8.15"
    },
    "doc": "[[text]]\n=== Text type family\n++++\n<titleabbrev>Text</titleabbrev>\n++++\n\nThe text family includes the following field types:\n\n* <<text-field-type,`text`>>, the traditional field type for full-text content\nsuch as the body of an email or the description of a product.\n* <<match-only-text-field-type,`match_only_text`>>, a space-optimized variant\nof `text` that disables scoring and performs slower on queries that need\npositions. It is best suited for indexing log messages.\n\n\n[discrete]\n[[text-field-type]]\n=== Text field type\n\nA field to index full-text values, such as the body of an email or the\ndescription of a product. These fields are `analyzed`, that is they are passed through an\n<<analysis,analyzer>> to convert the string into a list of individual terms\nbefore being indexed. The analysis process allows Elasticsearch to search for\nindividual words _within_ each full text field. Text fields are not\nused for sorting and seldom used for aggregations (although the\n<<search-aggregations-bucket-significanttext-aggregation,significant text aggregation>>\nis a notable exception).\n\n`text` fields are best suited for unstructured but human-readable content. If\nyou need to index unstructured machine-generated content, see\n<<mapping-unstructured-content>>.\n\nIf you need to index structured content such as email addresses, hostnames, status\ncodes, or tags, it is likely that you should rather use a <<keyword,`keyword`>> field.\n\nBelow is an example of a mapping for a text field:\n\n[source,console]\n--------------------------------\nPUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"full_name\": {\n        \"type\":  \"text\"\n      }\n    }\n  }\n}\n--------------------------------\n\n[[text-multi-fields]]\n==== Use a field as both text and keyword\nSometimes it is useful to have both a full text (`text`) and a keyword\n(`keyword`) version of the same field: one for full text search and the\nother for aggregations and sorting. This can be achieved with\n<<multi-fields,multi-fields>>.\n\n[[text-params]]\n==== Parameters for text fields\n\nThe following parameters are accepted by `text` fields:\n\n[horizontal]\n\n<<analyzer,`analyzer`>>::\n\n    The <<analysis,analyzer>> which should be used for\n    the `text` field, both at index-time and at\n    search-time (unless overridden by the  <<search-analyzer,`search_analyzer`>>).\n    Defaults to the default index analyzer, or the\n    <<analysis-standard-analyzer,`standard` analyzer>>.\n\n<<eager-global-ordinals,`eager_global_ordinals`>>::\n\n    Should global ordinals be loaded eagerly on refresh? Accepts `true` or `false`\n    (default). Enabling this is a good idea on fields that are frequently used for\n    (significant) terms aggregations.\n\n<<fielddata-mapping-param,`fielddata`>>::\n\n    Can the field use in-memory fielddata for sorting, aggregations,\n    or scripting? Accepts `true` or `false` (default).\n\n<<field-data-filtering,`fielddata_frequency_filter`>>::\n\n    Expert settings which allow to decide which values to load in memory when `fielddata`\n    is enabled. By default all values are loaded.\n\n<<multi-fields,`fields`>>::\n\n    Multi-fields allow the same string value to be indexed in multiple ways for\n    different purposes, such as one field for search and a multi-field for\n    sorting and aggregations, or the same string value analyzed by different\n    analyzers.\n\n<<mapping-index,`index`>>::\n\n    Should the field be searchable? Accepts `true` (default) or `false`.\n\n<<index-options,`index_options`>>::\n\n    What information should be stored in the index, for search and highlighting purposes.\n    Defaults to `positions`.\n\n<<index-prefixes,`index_prefixes`>>::\n\n    If enabled, term prefixes of between 2 and 5 characters are indexed into a\n    separate field. This allows prefix searches to run more efficiently, at\n    the expense of a larger index.\n\n<<index-phrases,`index_phrases`>>::\n\n    If enabled, two-term word combinations ('shingles') are indexed into a separate\n    field. This allows exact phrase queries (no slop) to run more efficiently, at the expense\n    of a larger index. Note that this works best when stopwords are not removed,\n    as phrases containing stopwords will not use the subsidiary field and will fall\n    back to a standard phrase query. Accepts `true` or `false` (default).\n\n<<norms,`norms`>>::\n\n    Whether field-length should be taken into account when scoring queries.\n    Accepts `true` (default) or `false`.\n\n<<position-increment-gap,`position_increment_gap`>>::\n\n    The number of fake term position which should be inserted between each\n    element of an array of strings. Defaults to the `position_increment_gap`\n    configured on the analyzer which defaults to `100`. `100` was chosen because it\n    prevents phrase queries with reasonably large slops (less than 100) from\n    matching terms across field values.\n\n<<mapping-store,`store`>>::\n\n    Whether the field value should be stored and retrievable separately from\n    the <<mapping-source-field,`_source`>> field. Accepts `true` or `false` (default).\n\n<<search-analyzer,`search_analyzer`>>::\n\n    The <<analyzer,`analyzer`>> that should be used at search time on\n    the `text` field. Defaults to the `analyzer` setting.\n\n<<search-quote-analyzer,`search_quote_analyzer`>>::\n\n    The <<analyzer,`analyzer`>> that should be used at search time when a\n    phrase is encountered. Defaults to the `search_analyzer` setting.\n\n<<similarity,`similarity`>>::\n\n    Which scoring algorithm or _similarity_ should be used. Defaults\n    to `BM25`.\n\n<<term-vector,`term_vector`>>::\n\n    Whether term vectors should be stored for the field. Defaults to `no`.\n\n<<mapping-field-meta,`meta`>>::\n\n    Metadata about the field.\n\n[[text-synthetic-source]]\n==== Synthetic `_source`\n\nIMPORTANT: Synthetic `_source` is Generally Available only for TSDB indices\n(indices that have `index.mode` set to `time_series`). For other indices\nsynthetic `_source` is in technical preview. Features in technical preview may\nbe changed or removed in a future release. Elastic will work to fix\nany issues, but features in technical preview are not subject to the support SLA\nof official GA features.\n\n`text` fields support <<synthetic-source,synthetic `_source`>> if they have\na <<keyword-synthetic-source, `keyword`>> sub-field that supports synthetic\n`_source` or if the `text` field sets `store` to `true`. Either way, it may\nnot have <<copy-to,`copy_to`>>.\n\nIf using a sub-`keyword` field, then the values are sorted in the same way as\na `keyword` field's values are sorted. By default, that means sorted with\nduplicates removed. So:\n[source,console,id=synthetic-source-text-example-default]\n----\nPUT idx\n{\n  \"settings\": {\n    \"index\": {\n      \"mapping\": {\n        \"source\": {\n          \"mode\": \"synthetic\"\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"text\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"raw\": {\n            \"type\": \"keyword\"\n          }\n        }\n      }\n    }\n  }\n}\nPUT idx/_doc/1\n{\n  \"text\": [\n    \"the quick brown fox\",\n    \"the quick brown fox\",\n    \"jumped over the lazy dog\"\n  ]\n}\n----\n// TEST[s/$/\\nGET idx\\/_doc\\/1?filter_path=_source\\n/]\n\nWill become:\n[source,console-result]\n----\n{\n  \"text\": [\n    \"jumped over the lazy dog\",\n    \"the quick brown fox\"\n  ]\n}\n----\n// TEST[s/^/{\"_source\":/ s/\\n$/}/]\n\nNOTE: Reordering text fields can have an effect on <<query-dsl-match-query-phrase,phrase>>\n      and <<span-queries,span>> queries. See the discussion about\n      <<position-increment-gap,`position_increment_gap`>> for more detail. You\n      can avoid this by making sure the `slop` parameter on the phrase queries\n      is lower than the `position_increment_gap`. This is the default.\n\nIf the `text` field sets `store` to true then order and duplicates\nare preserved.\n[source,console,id=synthetic-source-text-example-stored]\n----\nPUT idx\n{\n  \"settings\": {\n    \"index\": {\n      \"mapping\": {\n        \"source\": {\n          \"mode\": \"synthetic\"\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"text\": { \"type\": \"text\", \"store\": true }\n    }\n  }\n}\nPUT idx/_doc/1\n{\n  \"text\": [\n    \"the quick brown fox\",\n    \"the quick brown fox\",\n    \"jumped over the lazy dog\"\n  ]\n}\n----\n// TEST[s/$/\\nGET idx\\/_doc\\/1?filter_path=_source\\n/]\n\nWill become:\n[source,console-result]\n----\n{\n  \"text\": [\n    \"the quick brown fox\",\n    \"the quick brown fox\",\n    \"jumped over the lazy dog\"\n  ]\n}\n----\n// TEST[s/^/{\"_source\":/ s/\\n$/}/]\n\n[[fielddata-mapping-param]]\n==== `fielddata` mapping parameter\n\n`text` fields are searchable by default, but by default are not available for\naggregations, sorting, or scripting. If you try to sort, aggregate, or access\nvalues from a `text` field using a script, you'll see an exception indicating\nthat field data is disabled by default on text fields. To load field data in\nmemory, set `fielddata=true` on your field.\n\nNOTE: Loading field data in memory can consume significant memory.\n\nField data is the only way to access the analyzed tokens from a full text field\nin aggregations, sorting, or scripting. For example, a full text field like `New York`\nwould get analyzed as `new` and `york`. To aggregate on these tokens requires field data.\n\n[[before-enabling-fielddata]]\n==== Before enabling fielddata\n\nIt usually doesn't make sense to enable fielddata on text fields. Field data\nis stored in the heap with the <<modules-fielddata, field data cache>> because it\nis expensive to calculate. Calculating the field data can cause latency spikes, and\nincreasing heap usage is a cause of cluster performance issues.\n\nMost users who want to do more with text fields use <<multi-fields, multi-field mappings>>\nby having both a `text` field for full text searches, and an\nunanalyzed <<keyword,`keyword`>> field for aggregations, as follows:\n\n[source,console]\n---------------------------------\nPUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_field\": { <1>\n        \"type\": \"text\",\n        \"fields\": {\n          \"keyword\": { <2>\n            \"type\": \"keyword\"\n          }\n        }\n      }\n    }\n  }\n}\n---------------------------------\n\n<1> Use the `my_field` field for searches.\n<2> Use the `my_field.keyword` field for aggregations, sorting, or in scripts.\n\n[[enable-fielddata-text-fields]]\n==== Enabling fielddata on `text` fields\n\nYou can enable fielddata on an existing `text` field using the\n<<indices-put-mapping,update mapping API>> as follows:\n\n[source,console]\n-----------------------------------\nPUT my-index-000001/_mapping\n{\n  \"properties\": {\n    \"my_field\": { <1>\n      \"type\":     \"text\",\n      \"fielddata\": true\n    }\n  }\n}\n-----------------------------------\n// TEST[continued]\n\n<1> The mapping that you specify for `my_field` should consist of the existing\n    mapping for that field, plus the `fielddata` parameter.\n\n[[field-data-filtering]]\n==== `fielddata_frequency_filter` mapping parameter\n\nFielddata filtering can be used to reduce the number of terms loaded into\nmemory, and thus reduce memory usage. Terms can be filtered by _frequency_:\n\nThe frequency filter allows you to only load terms whose document frequency falls\nbetween a `min` and `max` value, which can be expressed an absolute\nnumber (when the number is bigger than 1.0) or as a percentage\n(eg `0.01` is `1%` and `1.0` is `100%`). Frequency is calculated\n*per segment*. Percentages are based on the number of docs which have a\nvalue for the field, as opposed to all docs in the segment.\n\nSmall segments can be excluded completely by specifying the minimum\nnumber of docs that the segment should contain with `min_segment_size`:\n\n[source,console]\n--------------------------------------------------\nPUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"tag\": {\n        \"type\": \"text\",\n        \"fielddata\": true,\n        \"fielddata_frequency_filter\": {\n          \"min\": 0.001,\n          \"max\": 0.1,\n          \"min_segment_size\": 500\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\ninclude::match-only-text.asciidoc[]\n"
}