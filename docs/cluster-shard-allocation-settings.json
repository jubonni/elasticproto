{
    "meta": {
        "size": 12094,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-shard-allocation-settings.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "cluster-shard-allocation-settings",
        "version": "8.15"
    },
    "doc": "[[cluster-shard-allocation-settings]]\n==== Cluster-level shard allocation settings\n\nYou can use the following settings to control shard allocation and recovery:\n\n[[cluster-routing-allocation-enable]]\n`cluster.routing.allocation.enable`::\n+\n--\n(<<dynamic-cluster-setting,Dynamic>>)\nEnable or disable allocation for specific kinds of shards:\n\n* `all` -             (default) Allows shard allocation for all kinds of shards.\n* `primaries` -       Allows shard allocation only for primary shards.\n* `new_primaries` -   Allows shard allocation only for primary shards for new indices.\n* `none` -            No shard allocations of any kind are allowed for any indices.\n\nThis setting only affects future allocations, and does not re-allocate or un-allocate currently allocated shards.\nIt also does not affect the recovery of local primary shards when\nrestarting a node. A restarted node that has a copy of an unassigned primary\nshard will recover that primary immediately, assuming that its allocation id matches\none of the active allocation ids in the cluster state.\n\n--\n\n[[cluster-routing-allocation-same-shard-host]]\n`cluster.routing.allocation.same_shard.host`::\n      (<<dynamic-cluster-setting,Dynamic>>)\n      If `true`, forbids multiple copies of a shard from being allocated to\n      distinct nodes on the same host, i.e. which have the same network\n      address. Defaults to `false`, meaning that copies of a shard may\n      sometimes be allocated to nodes on the same host. This setting is only\n      relevant if you run multiple nodes on each host.\n\n`cluster.routing.allocation.node_concurrent_incoming_recoveries`::\n     (<<dynamic-cluster-setting,Dynamic>>)\n     How many concurrent incoming shard recoveries are allowed to happen on a\n     node. Incoming recoveries are the recoveries where the target shard (most\n     likely the replica unless a shard is relocating) is allocated on the node.\n     Defaults to `2`. Increasing this setting may cause shard movements to have\n     a performance impact on other activity in your cluster, but may not make\n     shard movements complete noticeably sooner. We do not recommend adjusting\n     this setting from its default of `2`.\n\n`cluster.routing.allocation.node_concurrent_outgoing_recoveries`::\n     (<<dynamic-cluster-setting,Dynamic>>)\n     How many concurrent outgoing shard recoveries are allowed to happen on a\n     node. Outgoing recoveries are the recoveries where the source shard (most\n     likely the primary unless a shard is relocating) is allocated on the node.\n     Defaults to `2`. Increasing this setting may cause shard movements to have\n     a performance impact on other activity in your cluster, but may not make\n     shard movements complete noticeably sooner. We do not recommend adjusting\n     this setting from its default of `2`.\n\n`cluster.routing.allocation.node_concurrent_recoveries`::\n     (<<dynamic-cluster-setting,Dynamic>>)\n     A shortcut to set both\n     `cluster.routing.allocation.node_concurrent_incoming_recoveries` and\n     `cluster.routing.allocation.node_concurrent_outgoing_recoveries`. The\n     value of this setting takes effect only when the more specific setting is\n     not configured.  Defaults to `2`. Increasing this setting may cause shard\n     movements to have a performance impact on other activity in your cluster,\n     but may not make shard movements complete noticeably sooner. We do not\n     recommend adjusting this setting from its default of `2`.\n\n`cluster.routing.allocation.node_initial_primaries_recoveries`::\n     (<<dynamic-cluster-setting,Dynamic>>)\n     While the recovery of replicas happens over the network, the recovery of\n     an unassigned primary after node restart uses data from the local disk.\n     These should be fast so more initial primary recoveries can happen in\n     parallel on each node. Defaults to `4`. Increasing this setting may cause\n     shard recoveries to have a performance impact on other activity in your\n     cluster, but may not make shard recoveries complete noticeably sooner. We\n     do not recommend adjusting this setting from its default of `4`.\n\n[[shards-rebalancing-settings]]\n==== Shard rebalancing settings\n\nA cluster is _balanced_ when it has an equal number of shards on each node, with\nall nodes needing equal resources, without having a concentration of shards from\nany index on any node. {es} runs an automatic process called _rebalancing_ which\nmoves shards between the nodes in your cluster to improve its balance.\nRebalancing obeys all other shard allocation rules such as\n<<cluster-shard-allocation-filtering,allocation filtering>> and\n<<forced-awareness,forced awareness>> which may prevent it from completely\nbalancing the cluster. In that case, rebalancing strives to achieve the most\nbalanced cluster possible within the rules you have configured. If you are using\n<<data-tiers,data tiers>> then {es} automatically applies allocation filtering\nrules to place each shard within the appropriate tier. These rules mean that the\nbalancer works independently within each tier.\n\nYou can use the following settings to control the rebalancing of shards across\nthe cluster:\n\n`cluster.routing.allocation.allow_rebalance`::\n+\n--\n(<<dynamic-cluster-setting,Dynamic>>)\nSpecify when shard rebalancing is allowed:\n\n\n* `always` -                    (default) Always allow rebalancing.\n* `indices_primaries_active` -  Only when all primaries in the cluster are allocated.\n* `indices_all_active` -        Only when all shards (primaries and replicas) in the cluster are allocated.\n--\n\n`cluster.routing.rebalance.enable`::\n+\n--\n(<<dynamic-cluster-setting,Dynamic>>)\nEnable or disable rebalancing for specific kinds of shards:\n\n* `all` -         (default) Allows shard balancing for all kinds of shards.\n* `primaries` -   Allows shard balancing only for primary shards.\n* `replicas` -    Allows shard balancing only for replica shards.\n* `none` -        No shard balancing of any kind are allowed for any indices.\n\nRebalancing is important to ensure the cluster returns to a healthy and fully\nresilient state after a disruption. If you adjust this setting, remember to set\nit back to `all` as soon as possible.\n--\n\n`cluster.routing.allocation.cluster_concurrent_rebalance`::\n(<<dynamic-cluster-setting,Dynamic>>)\nDefines the number of concurrent shard rebalances are allowed across the whole\ncluster. Defaults to `2`. Note that this setting only controls the number of\nconcurrent shard relocations due to imbalances in the cluster. This setting\ndoes not limit shard relocations due to\n<<cluster-shard-allocation-filtering,allocation filtering>> or\n<<forced-awareness,forced awareness>>. Increasing this setting may cause the\ncluster to use additional resources moving shards between nodes, so we\ngenerally do not recommend adjusting this setting from its default of `2`.\n\n`cluster.routing.allocation.type`::\n+\n--\nSelects the algorithm used for computing the cluster balance. Defaults to\n`desired_balance` which selects the _desired balance allocator_. This allocator\nruns a background task which computes the desired balance of shards in the\ncluster. Once this background task completes, {es} moves shards to their\ndesired locations.\n\ndeprecated:[8.8,The `balanced` allocator type is deprecated and no longer recommended]\nMay also be set to `balanced` to select the legacy _balanced allocator_. This\nallocator was the default allocator in versions of {es} before 8.6.0. It runs\nin the foreground, preventing the master from doing other work in parallel. It\nworks by selecting a small number of shard movements which immediately improve\nthe balance of the cluster, and when those shard movements complete it runs\nagain and selects another few shards to move. Since this allocator makes its\ndecisions based only on the current state of the cluster, it will sometimes\nmove a shard several times while balancing the cluster.\n--\n\n[[shards-rebalancing-heuristics]]\n==== Shard balancing heuristics settings\n\nRebalancing works by computing a _weight_ for each node based on its allocation\nof shards, and then moving shards between nodes to reduce the weight of the\nheavier nodes and increase the weight of the lighter ones. The cluster is\nbalanced when there is no possible shard movement that can bring the weight of\nany node closer to the weight of any other node by more than a configurable\nthreshold.\n\nThe weight of a node depends on the number of shards it holds and on the total\nestimated resource usage of those shards expressed in terms of the size of the\nshard on disk and the number of threads needed to support write traffic to the\nshard. {es} estimates the resource usage of shards belonging to data streams\nwhen they are created by a rollover. The estimated disk size of the new shard\nis the mean size of the other shards in the data stream. The estimated write\nload of the new shard is a weighted average of the actual write loads of recent\nshards in the data stream. Shards that do not belong to the write index of a\ndata stream have an estimated write load of zero.\n\nThe following settings control how {es} combines these values into an overall\nmeasure of each node's weight.\n\n`cluster.routing.allocation.balance.threshold`::\n(float, <<dynamic-cluster-setting,Dynamic>>)\nThe minimum improvement in weight which triggers a rebalancing shard movement.\nDefaults to `1.0f`. Raising this value will cause {es} to stop rebalancing\nshards sooner, leaving the cluster in a more unbalanced state.\n\n`cluster.routing.allocation.balance.shard`::\n(float, <<dynamic-cluster-setting,Dynamic>>)\nDefines the weight factor for the total number of shards allocated to each node.\nDefaults to `0.45f`. Raising this value increases the tendency of {es} to\nequalize the total number of shards across nodes ahead of the other balancing\nvariables.\n\n`cluster.routing.allocation.balance.index`::\n(float, <<dynamic-cluster-setting,Dynamic>>)\nDefines the weight factor for the number of shards per index allocated to each\nnode. Defaults to `0.55f`. Raising this value increases the tendency of {es} to\nequalize the number of shards of each index across nodes ahead of the other\nbalancing variables.\n\n`cluster.routing.allocation.balance.disk_usage`::\n(float, <<dynamic-cluster-setting,Dynamic>>)\nDefines the weight factor for balancing shards according to their predicted disk\nsize in bytes. Defaults to `2e-11f`. Raising this value increases the tendency\nof {es} to equalize the total disk usage across nodes ahead of the other\nbalancing variables.\n\n`cluster.routing.allocation.balance.write_load`::\n(float, <<dynamic-cluster-setting,Dynamic>>)\nDefines the weight factor for the write load of each shard, in terms of the\nestimated number of indexing threads needed by the shard. Defaults to `10.0f`.\nRaising this value increases the tendency of {es} to equalize the total write\nload across nodes ahead of the other balancing variables.\n\n[NOTE]\n====\n* If you have a large cluster, it may be unnecessary to keep it in\na perfectly balanced state at all times. It is less resource-intensive for the\ncluster to operate in a somewhat unbalanced state rather than to perform all\nthe shard movements needed to achieve the perfect balance. If so, increase the\nvalue of `cluster.routing.allocation.balance.threshold` to define the\nacceptable imbalance between nodes. For instance, if you have an average of 500\nshards per node and can accept a difference of 5% (25 typical shards) between\nnodes, set `cluster.routing.allocation.balance.threshold` to `25`.\n\n* We do not recommend adjusting the values of the heuristic weight factor\nsettings. The default values work well in all reasonable clusters. Although\ndifferent values may improve the current balance in some ways, it is possible\nthat they will create unexpected problems in the future or prevent it from\ngracefully handling an unexpected disruption.\n\n* Regardless of the result of the balancing algorithm, rebalancing might\nnot be allowed due to allocation rules such as forced awareness and allocation\nfiltering. Use the <<cluster-allocation-explain>> API to explain the current\nallocation of shards.\n====\n"
}