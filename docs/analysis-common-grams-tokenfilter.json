{
    "meta": {
        "timestamp": "2024-11-01T03:07:08.905276",
        "size": 5524,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-common-grams-tokenfilter.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "analysis-common-grams-tokenfilter",
        "version": "8.15"
    },
    "doc": "[[analysis-common-grams-tokenfilter]]\n=== Common grams token filter\n++++\n<titleabbrev>Common grams</titleabbrev>\n++++\n\nGenerates {wikipedia}/Bigram[bigrams] for a specified set of\ncommon words.\n\nFor example, you can specify `is` and `the` as common words. This filter then\nconverts the tokens `[the, quick, fox, is, brown]` to `[the, the_quick, quick,\nfox, fox_is, is, is_brown, brown]`.\n\nYou can use the `common_grams` filter in place of the\n<<analysis-stop-tokenfilter,stop token filter>> when you don't want to\ncompletely ignore common words.\n\nThis filter uses Lucene's\n{lucene-analysis-docs}/commongrams/CommonGramsFilter.html[CommonGramsFilter].\n\n[[analysis-common-grams-analyze-ex]]\n==== Example\n\nThe following <<indices-analyze,analyze API>> request creates bigrams for `is`\nand `the`:\n\n[source,console]\n--------------------------------------------------\nGET /_analyze\n{\n  \"tokenizer\" : \"whitespace\",\n  \"filter\" : [\n    {\n      \"type\": \"common_grams\",\n      \"common_words\": [\"is\", \"the\"]\n    }\n  ],\n  \"text\" : \"the quick fox is brown\"\n}\n--------------------------------------------------\n\nThe filter produces the following tokens:\n\n[source,text]\n--------------------------------------------------\n[ the, the_quick, quick, fox, fox_is, is, is_brown, brown ]\n--------------------------------------------------\n\n/////////////////////\n[source,console-result]\n--------------------------------------------------\n{\n  \"tokens\" : [\n    {\n      \"token\" : \"the\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 3,\n      \"type\" : \"word\",\n      \"position\" : 0\n    },\n    {\n      \"token\" : \"the_quick\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 9,\n      \"type\" : \"gram\",\n      \"position\" : 0,\n      \"positionLength\" : 2\n    },\n    {\n      \"token\" : \"quick\",\n      \"start_offset\" : 4,\n      \"end_offset\" : 9,\n      \"type\" : \"word\",\n      \"position\" : 1\n    },\n    {\n      \"token\" : \"fox\",\n      \"start_offset\" : 10,\n      \"end_offset\" : 13,\n      \"type\" : \"word\",\n      \"position\" : 2\n    },\n    {\n      \"token\" : \"fox_is\",\n      \"start_offset\" : 10,\n      \"end_offset\" : 16,\n      \"type\" : \"gram\",\n      \"position\" : 2,\n      \"positionLength\" : 2\n    },\n    {\n      \"token\" : \"is\",\n      \"start_offset\" : 14,\n      \"end_offset\" : 16,\n      \"type\" : \"word\",\n      \"position\" : 3\n    },\n    {\n      \"token\" : \"is_brown\",\n      \"start_offset\" : 14,\n      \"end_offset\" : 22,\n      \"type\" : \"gram\",\n      \"position\" : 3,\n      \"positionLength\" : 2\n    },\n    {\n      \"token\" : \"brown\",\n      \"start_offset\" : 17,\n      \"end_offset\" : 22,\n      \"type\" : \"word\",\n      \"position\" : 4\n    }\n  ]\n}\n--------------------------------------------------\n/////////////////////\n\n[[analysis-common-grams-tokenfilter-analyzer-ex]]\n==== Add to an analyzer\n\nThe following <<indices-create-index,create index API>> request uses the\n`common_grams` filter to configure a new \n<<analysis-custom-analyzer,custom analyzer>>:\n\n[source,console]\n--------------------------------------------------\nPUT /common_grams_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"index_grams\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"common_grams\" ]\n        }\n      },\n      \"filter\": {\n        \"common_grams\": {\n          \"type\": \"common_grams\",\n          \"common_words\": [ \"a\", \"is\", \"the\" ]\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n[[analysis-common-grams-tokenfilter-configure-parms]]\n==== Configurable parameters\n\n`common_words`::\n+\n--\n(Required+++*+++, array of strings)\nA list of tokens. The filter generates bigrams for these tokens.\n\nEither this or the `common_words_path` parameter is required.\n--\n\n`common_words_path`::\n+\n--\n(Required+++*+++, string)\nPath to a file containing a list of tokens. The filter generates bigrams for\nthese tokens.\n\nThis path must be absolute or relative to the `config` location. The file must\nbe UTF-8 encoded. Each token in the file must be separated by a line break.\n\nEither this or the `common_words` parameter is required.\n--\n\n`ignore_case`::\n(Optional, Boolean)\nIf `true`, matches for common words matching are case-insensitive.\nDefaults to `false`.\n\n`query_mode`::\n+\n--\n(Optional, Boolean)\nIf `true`, the filter excludes the following tokens from the output:\n\n* Unigrams for common words\n* Unigrams for terms followed by common words\n\nDefaults to `false`. We recommend enabling this parameter for\n<<search-analyzer,search analyzers>>.\n\nFor example, you can enable this parameter and specify `is` and `the` as\ncommon words. This filter converts the tokens `[the, quick, fox, is, brown]` to\n`[the_quick, quick, fox_is, is_brown,]`.\n--\n\n[[analysis-common-grams-tokenfilter-customize]]\n==== Customize\n\nTo customize the `common_grams` filter, duplicate it to create the basis\nfor a new custom token filter. You can modify the filter using its configurable\nparameters.\n\nFor example, the following request creates a custom `common_grams` filter with\n`ignore_case` and `query_mode` set to `true`:\n\n[source,console]\n--------------------------------------------------\nPUT /common_grams_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"index_grams\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"common_grams_query\" ]\n        }\n      },\n      \"filter\": {\n        \"common_grams_query\": {\n          \"type\": \"common_grams\",\n          \"common_words\": [ \"a\", \"is\", \"the\" ],\n          \"ignore_case\": true,\n          \"query_mode\": true\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n"
}