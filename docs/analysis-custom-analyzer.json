{
    "meta": {
        "timestamp": "2024-11-01T03:07:09.018271",
        "size": 6020,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-custom-analyzer.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "analysis-custom-analyzer",
        "version": "8.15"
    },
    "doc": "[[analysis-custom-analyzer]]\n=== Create a custom analyzer\n\nWhen the built-in analyzers do not fulfill your needs, you can create a\n`custom` analyzer which uses the appropriate combination of:\n\n* zero or more <<analysis-charfilters, character filters>>\n* a <<analysis-tokenizers,tokenizer>>\n* zero or more <<analysis-tokenfilters,token filters>>.\n\n[discrete]\n=== Configuration\n\nThe `custom` analyzer accepts the following parameters:\n\n[horizontal]\n`type`::\n    Analyzer type. Accepts <<analysis-analyzers, built-in analyzer types>>. For\n    custom analyzers, use `custom` or omit this parameter.\n\n`tokenizer`::\n\n    A built-in or customised <<analysis-tokenizers,tokenizer>>.\n    (Required)\n\n`char_filter`::\n\n    An optional array of built-in or customised\n    <<analysis-charfilters, character filters>>.\n\n`filter`::\n\n    An optional array of built-in or customised\n    <<analysis-tokenfilters, token filters>>.\n\n`position_increment_gap`::\n\n    When indexing an array of text values, Elasticsearch inserts a fake \"gap\"\n    between the last term of one value and the first term of the next value to\n    ensure that a phrase query doesn't match two terms from different array\n    elements. Defaults to `100`. See <<position-increment-gap>> for more.\n\n[discrete]\n=== Example configuration\n\nHere is an example that combines the following:\n\nCharacter Filter::\n* <<analysis-htmlstrip-charfilter,HTML Strip Character Filter>>\n\nTokenizer::\n* <<analysis-standard-tokenizer,Standard Tokenizer>>\n\nToken Filters::\n* <<analysis-lowercase-tokenfilter,Lowercase Token Filter>>\n* <<analysis-asciifolding-tokenfilter,ASCII-Folding Token Filter>>\n\n[source,console]\n--------------------------------\nPUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_custom_analyzer\": {\n          \"type\": \"custom\", <1>\n          \"tokenizer\": \"standard\",\n          \"char_filter\": [\n            \"html_strip\"\n          ],\n          \"filter\": [\n            \"lowercase\",\n            \"asciifolding\"\n          ]\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_custom_analyzer\",\n  \"text\": \"Is this <b>d\u00e9j\u00e0 vu</b>?\"\n}\n--------------------------------\n\n<1> For `custom` analyzers, use a `type` of `custom` or omit the `type`\nparameter.\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"is\",\n      \"start_offset\": 0,\n      \"end_offset\": 2,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"this\",\n      \"start_offset\": 3,\n      \"end_offset\": 7,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"deja\",\n      \"start_offset\": 11,\n      \"end_offset\": 15,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"vu\",\n      \"start_offset\": 16,\n      \"end_offset\": 22,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 3\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe above example produces the following terms:\n\n[source,text]\n---------------------------\n[ is, this, deja, vu ]\n---------------------------\n\nThe previous example used tokenizer, token filters, and character filters with\ntheir default configurations, but it is possible to create configured versions\nof each and to use them in a custom analyzer.\n\nHere is a more complicated example that combines the following:\n\nCharacter Filter::\n* <<analysis-mapping-charfilter,Mapping Character Filter>>, configured to replace `:)` with `_happy_` and `:(` with `_sad_`\n\nTokenizer::\n*  <<analysis-pattern-tokenizer,Pattern Tokenizer>>, configured to split on punctuation characters\n\nToken Filters::\n* <<analysis-lowercase-tokenfilter,Lowercase Token Filter>>\n* <<analysis-stop-tokenfilter,Stop Token Filter>>, configured to use the pre-defined list of English stop words\n\n\nHere is an example:\n\n[source,console]\n--------------------------------------------------\nPUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_custom_analyzer\": { <1>\n          \"char_filter\": [\n            \"emoticons\"\n          ],\n          \"tokenizer\": \"punctuation\",\n          \"filter\": [\n            \"lowercase\",\n            \"english_stop\"\n          ]\n        }\n      },\n      \"tokenizer\": {\n        \"punctuation\": { <2>\n          \"type\": \"pattern\",\n          \"pattern\": \"[ .,!?]\"\n        }\n      },\n      \"char_filter\": {\n        \"emoticons\": { <3>\n          \"type\": \"mapping\",\n          \"mappings\": [\n            \":) => _happy_\",\n            \":( => _sad_\"\n          ]\n        }\n      },\n      \"filter\": {\n        \"english_stop\": { <4>\n          \"type\": \"stop\",\n          \"stopwords\": \"_english_\"\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_custom_analyzer\",\n  \"text\": \"I'm a :) person, and you?\"\n}\n--------------------------------------------------\n\n<1> Assigns the index a default custom analyzer, `my_custom_analyzer`. This\nanalyzer uses a custom tokenizer, character filter, and token filter that\nare defined later in the request. This analyzer also omits the `type` parameter.\n<2> Defines the custom `punctuation` tokenizer.\n<3> Defines the custom `emoticons` character filter.\n<4> Defines the custom `english_stop` token filter.\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"i'm\",\n      \"start_offset\": 0,\n      \"end_offset\": 3,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"_happy_\",\n      \"start_offset\": 6,\n      \"end_offset\": 8,\n      \"type\": \"word\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"person\",\n      \"start_offset\": 9,\n      \"end_offset\": 15,\n      \"type\": \"word\",\n      \"position\": 3\n    },\n    {\n      \"token\": \"you\",\n      \"start_offset\": 21,\n      \"end_offset\": 24,\n      \"type\": \"word\",\n      \"position\": 5\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe above example produces the following terms:\n\n[source,text]\n---------------------------\n[ i'm, _happy_, person, you ]\n---------------------------\n"
}