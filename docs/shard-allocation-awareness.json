{
    "meta": {
        "size": 5669,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/shard-allocation-awareness.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "shard-allocation-awareness",
        "version": "8.15"
    },
    "doc": "[[shard-allocation-awareness]]\n==== Shard allocation awareness\n\nYou can use custom node attributes as _awareness attributes_ to enable {es}\nto take your physical hardware configuration into account when allocating shards.\nIf {es} knows which nodes are on the same physical server, in the same rack, or\nin the same zone, it can distribute the primary shard and its replica shards to\nminimize the risk of losing all shard copies in the event of a failure.\n\nWhen shard allocation awareness is enabled with the\n<<dynamic-cluster-setting,dynamic>>\n`cluster.routing.allocation.awareness.attributes` setting, shards are only\nallocated to nodes that have values set for the specified awareness attributes.\nIf you use multiple awareness attributes, {es} considers each attribute\nseparately when allocating shards.\n\nNOTE: The number of attribute values determines how many shard copies are\nallocated in each location. If the number of nodes in each location is\nunbalanced and there are a lot of replicas, replica shards might be left\nunassigned.\n\nTIP: Learn more about <<high-availability-cluster-design-large-clusters,designing resilient clusters>>.\n\n[[enabling-awareness]]\n===== Enabling shard allocation awareness\n\nTo enable shard allocation awareness:\n\n. Specify the location of each node with a custom node attribute. For example, \nif you want Elasticsearch to distribute shards across different racks, you might \nuse an awareness attribute called `rack_id`. \n+\nYou can set custom attributes in two ways:\n\n- By editing the `elasticsearch.yml` config file:\n+\n[source,yaml]\n--------------------------------------------------------\nnode.attr.rack_id: rack_one\n--------------------------------------------------------\n+\n- Using the `-E` command line argument when you start a node:\n+\n[source,sh]\n--------------------------------------------------------\n./bin/elasticsearch -Enode.attr.rack_id=rack_one\n--------------------------------------------------------\n\n. Tell {es} to take one or more awareness attributes into account when\nallocating shards by setting\n`cluster.routing.allocation.awareness.attributes` in *every* master-eligible\nnode's `elasticsearch.yml` config file.\n+\n--\n[source,yaml]\n--------------------------------------------------------\ncluster.routing.allocation.awareness.attributes: rack_id <1>\n--------------------------------------------------------\n<1> Specify multiple attributes as a comma-separated list.\n--\n+\nYou can also use the\n<<cluster-update-settings,cluster-update-settings>> API to set or update\na cluster's awareness attributes:\n+\n[source,console]\n--------------------------------------------------\nPUT /_cluster/settings\n{\n  \"persistent\" : {\n    \"cluster.routing.allocation.awareness.attributes\" : \"rack_id\"\n  }\n}\n--------------------------------------------------\n\nWith this example configuration, if you start two nodes with\n`node.attr.rack_id` set to `rack_one` and create an index with 5 primary\nshards and 1 replica of each primary, all primaries and replicas are\nallocated across the two node.\n\n.All primaries and replicas allocated across two nodes in the same rack\nimage::images/shard-allocation/shard-allocation-awareness-one-rack.png[All primaries and replicas are allocated across two nodes in the same rack]\n\nIf you add two nodes with `node.attr.rack_id` set to `rack_two`,\n{es} moves shards to the new nodes, ensuring (if possible)\nthat no two copies of the same shard are in the same rack.\n\n.Primaries and replicas allocated across four nodes in two racks, with no two copies of the same shard in the same rack\nimage::images/shard-allocation/shard-allocation-awareness-two-racks.png[Primaries and replicas are allocated across four nodes in two racks with no two copies of the same shard in the same rack]\n\nIf `rack_two` fails and takes down both its nodes, by default {es}\nallocates the lost shard copies to nodes in `rack_one`. To prevent multiple\ncopies of a particular shard from being allocated in the same location, you can\nenable forced awareness.\n\n[[forced-awareness]]\n===== Forced awareness\n\nBy default, if one location fails, {es} spreads its shards across the remaining\nlocations. This might be undesirable if the cluster does not have sufficient\nresources to host all its shards when one location is missing.\n\nTo prevent the remaining locations from being overloaded in the event of a\nwhole-location failure, specify the attribute values that should exist with the\n`cluster.routing.allocation.awareness.force.*` settings. This will mean that\n{es} will prefer to leave some replicas unassigned in the event of a\nwhole-location failure instead of overloading the nodes in the remaining\nlocations.\n\nFor example, if you have an awareness attribute called `zone` and configure\nnodes in `zone1` and `zone2`, you can use forced awareness to make {es} leave\nhalf of your shard copies unassigned if only one zone is available:\n\n[source,yaml]\n-------------------------------------------------------------------\ncluster.routing.allocation.awareness.attributes: zone\ncluster.routing.allocation.awareness.force.zone.values: zone1,zone2 <1>\n-------------------------------------------------------------------\n<1> Specify all possible `zone` attribute values.\n\nWith this example configuration, if you have two nodes with `node.attr.zone`\nset to `zone1` and an index with `number_of_replicas` set to `1`, {es}\nallocates all the primary shards but none of the replicas. It will assign the\nreplica shards once nodes with a different value for `node.attr.zone` join the\ncluster. In contrast, if you do not configure forced awareness, {es} will\nallocate all primaries and replicas to the two nodes even though they are in\nthe same zone.\n"
}