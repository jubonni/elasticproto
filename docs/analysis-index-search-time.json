{
    "meta": {
        "timestamp": "2024-11-01T03:07:09.472271",
        "size": 4745,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-index-search-time.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "analysis-index-search-time",
        "version": "8.15"
    },
    "doc": "[[analysis-index-search-time]]\n=== Index and search analysis\n\nText analysis occurs at two times:\n\nIndex time::\nWhen a document is indexed, any <<text,`text`>> field values are analyzed.\n\nSearch time::\nWhen running a <<full-text-queries,full-text search>> on a `text` field,\nthe query string (the text the user is searching for) is analyzed.\n+\nSearch time is also called _query time_.\n\nThe analyzer, or set of analysis rules, used at each time is called the _index\nanalyzer_ or _search analyzer_ respectively.\n\n[[analysis-same-index-search-analyzer]]\n==== How the index and search analyzer work together\n\nIn most cases, the same analyzer should be used at index and search time. This\nensures the values and query strings for a field are changed into the same form\nof tokens. In turn, this ensures the tokens match as expected during a search.\n\n.**Example**\n[%collapsible]\n====\n\nA document is indexed with the following value in a `text` field:\n\n[source,text]\n------\nThe QUICK brown foxes jumped over the dog!\n------\n\nThe index analyzer for the field converts the value into tokens and normalizes\nthem. In this case, each of the tokens represents a word:\n\n[source,text]\n------\n[ quick, brown, fox, jump, over, dog ]\n------\n\nThese tokens are then indexed.\n\nLater, a user searches the same `text` field for:\n\n[source,text]\n------\n\"Quick fox\"\n------\n\nThe user expects this search to match the sentence indexed earlier,\n`The QUICK brown foxes jumped over the dog!`.\n\nHowever, the query string does not contain the exact words used in the\ndocument's original text:\n\n* `Quick` vs `QUICK`\n* `fox` vs `foxes`\n\nTo account for this, the query string is analyzed using the same analyzer. This\nanalyzer produces the following tokens:\n\n[source,text]\n------\n[ quick, fox ]\n------\n\nTo execute the search, {es} compares these query string tokens to the tokens\nindexed in the `text` field.\n\n[options=\"header\"]\n|===\n|Token     | Query string | `text` field\n|`quick`   | X            | X\n|`brown`   |              | X\n|`fox`     | X            | X\n|`jump`    |              | X\n|`over`    |              | X\n|`dog`     |              | X\n|===\n\nBecause the field value and query string were analyzed in the same way, they\ncreated similar tokens. The tokens `quick` and `fox` are exact matches. This\nmeans the search matches the document containing\n`\"The QUICK brown foxes jumped over the dog!\"`, just as the user expects.\n====\n\n[[different-analyzers]]\n==== When to use a different search analyzer\n\nWhile less common, it sometimes makes sense to use different analyzers at index\nand search time. To enable this, {es} allows you to\n<<specify-search-analyzer,specify a separate search analyzer>>.\n\nGenerally, a separate search analyzer should only be specified when using the\nsame form of tokens for field values and query strings would create unexpected\nor irrelevant search matches.\n\n[[different-analyzer-ex]]\n.*Example*\n[%collapsible]\n====\n{es} is used to create a search engine that matches only words that start with\na provided prefix. For instance, a search for `tr` should return `tram` or\n`trope`\u2014but never `taxi` or `bat`.\n\nA document is added to the search engine's index; this document contains one\nsuch word in a `text` field:\n\n[source,text]\n------\n\"Apple\"\n------\n\nThe index analyzer for the field converts the value into tokens and normalizes\nthem. In this case, each of the tokens represents a potential prefix for\nthe word:\n\n[source,text]\n------\n[ a, ap, app, appl, apple]\n------\n\nThese tokens are then indexed.\n\nLater, a user searches the same `text` field for:\n\n[source,text]\n------\n\"appli\"\n------\n\nThe user expects this search to match only words that start with `appli`,\nsuch as `appliance` or `application`. The search should not match `apple`.\n\nHowever, if the index analyzer is used to analyze this query string, it would\nproduce the following tokens:\n\n[source,text]\n------\n[ a, ap, app, appl, appli ]\n------\n\nWhen {es} compares these query string tokens to the ones indexed for `apple`,\nit finds several matches.\n\n[options=\"header\"]\n|===\n|Token      | `appli`      | `apple`\n|`a`        | X            | X\n|`ap`       | X            | X\n|`app`      | X            | X\n|`appl`     | X            | X\n|`appli`    |              | X\n|===\n\nThis means the search would erroneously match `apple`. Not only that, it would\nmatch any word starting with `a`.\n\nTo fix this, you can specify a different search analyzer for query strings used\non the `text` field.\n\nIn this case, you could specify a search analyzer that produces a single token\nrather than a set of prefixes:\n\n[source,text]\n------\n[ appli ]\n------\n\nThis query string token would only match tokens for words that start with\n`appli`, which better aligns with the user's search expectations.\n====\n"
}