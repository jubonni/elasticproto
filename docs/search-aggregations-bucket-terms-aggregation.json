{
    "meta": {
        "timestamp": "2024-11-01T03:02:53.712580",
        "size": 32953,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "search-aggregations-bucket-terms-aggregation",
        "version": "8.15"
    },
    "doc": "[[search-aggregations-bucket-terms-aggregation]]\n=== Terms aggregation\n++++\n<titleabbrev>Terms</titleabbrev>\n++++\n\nA multi-bucket value source based aggregation where buckets are dynamically built - one per unique value.\n\n//////////////////////////\n\n[source,console]\n--------------------------------------------------\nPUT /products\n{\n  \"mappings\": {\n    \"properties\": {\n      \"genre\": {\n        \"type\": \"keyword\"\n      },\n      \"product\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\n\nPOST /products/_bulk?refresh\n{\"index\":{\"_id\":0}}\n{\"genre\": \"rock\", \"product\": \"Product A\"}\n{\"index\":{\"_id\":1}}\n{\"genre\": \"rock\", \"product\": \"Product B\"}\n{\"index\":{\"_id\":2}}\n{\"genre\": \"rock\", \"product\": \"Product C\"}\n{\"index\":{\"_id\":3}}\n{\"genre\": \"jazz\", \"product\": \"Product D\"}\n{\"index\":{\"_id\":4}}\n{\"genre\": \"jazz\", \"product\": \"Product E\"}\n{\"index\":{\"_id\":5}}\n{\"genre\": \"electronic\", \"product\": \"Anthology A\"}\n{\"index\":{\"_id\":6}}\n{\"genre\": \"electronic\", \"product\": \"Anthology A\"}\n{\"index\":{\"_id\":7}}\n{\"genre\": \"electronic\", \"product\": \"Product F\"}\n{\"index\":{\"_id\":8}}\n{\"genre\": \"electronic\", \"product\": \"Product G\"}\n{\"index\":{\"_id\":9}}\n{\"genre\": \"electronic\", \"product\": \"Product H\"}\n{\"index\":{\"_id\":10}}\n{\"genre\": \"electronic\", \"product\": \"Product I\"}\n-------------------------------------------------\n// TESTSETUP\n\n//////////////////////////\n\nExample:\n\n[source,console,id=terms-aggregation-example]\n--------------------------------------------------\nGET /_search\n{\n  \"aggs\": {\n    \"genres\": {\n      \"terms\": { \"field\": \"genre\" }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[s/_search/_search\\?filter_path=aggregations/]\n\nResponse:\n\n[source,console-result]\n--------------------------------------------------\n{\n  ...\n  \"aggregations\": {\n    \"genres\": {\n      \"doc_count_error_upper_bound\": 0,   <1>\n      \"sum_other_doc_count\": 0,           <2>\n      \"buckets\": [                        <3>\n        {\n          \"key\": \"electronic\",\n          \"doc_count\": 6\n        },\n        {\n          \"key\": \"rock\",\n          \"doc_count\": 3\n        },\n        {\n          \"key\": \"jazz\",\n          \"doc_count\": 2\n        }\n      ]\n    }\n  }\n}\n--------------------------------------------------\n// TESTRESPONSE[s/\\.\\.\\.//]\n\n<1> an upper bound of the error on the document counts for each term, see <<terms-agg-doc-count-error,below>>\n<2> when there are lots of unique terms, Elasticsearch only returns the top terms; this number is the sum of the document counts for all buckets that are not part of the response\n<3> the list of the top buckets, the meaning of `top` being defined by the <<search-aggregations-bucket-terms-aggregation-order,order>>\n\n[[search-aggregations-bucket-terms-aggregation-types]]\nThe `field` can be <<keyword>>, <<number>>, <<ip, `ip`>>, <<boolean, `boolean`>>,\nor <<binary, `binary`>>.\n\nNOTE: By default, you cannot run a `terms` aggregation on a `text` field. Use a\n`keyword` <<multi-fields,sub-field>> instead. Alternatively, you can enable\n<<fielddata-mapping-param,`fielddata`>> on the `text` field to create buckets for the field's\n<<analysis,analyzed>> terms. Enabling `fielddata` can significantly increase\nmemory usage.\n\n[[search-aggregations-bucket-terms-aggregation-size]]\n==== Size\n\nBy default, the `terms` aggregation returns the top ten terms with the most\ndocuments. Use the `size` parameter to return more terms, up to the\n<<search-settings-max-buckets,search.max_buckets>> limit.\n\nIf your data contains 100 or 1000 unique terms, you can increase the `size` of\nthe `terms` aggregation to return them all. If you have more unique terms and\nyou need them all, use the\n<<search-aggregations-bucket-composite-aggregation,composite aggregation>>\ninstead.\n\nLarger values of `size` use more memory to compute and, push the whole\naggregation close to the `max_buckets` limit. You'll know you've gone too large\nif the request fails with a message about `max_buckets`.\n\n[[search-aggregations-bucket-terms-aggregation-shard-size]]\n==== Shard size\n\nTo get more accurate results, the `terms` agg fetches more than\nthe top `size` terms from each shard. It fetches the top `shard_size` terms,\nwhich defaults to `size * 1.5 + 10`.\n\nThis is to handle the case when one term has many documents on one shard but is\njust below the `size` threshold on all other shards. If each shard only\nreturned `size` terms, the aggregation would return an partial doc count for\nthe term. So `terms` returns more terms in an attempt to catch the missing\nterms. This helps, but it's still quite possible to return a partial doc\ncount for a term. It just takes a term with more disparate per-shard doc counts.\n\nYou can increase `shard_size` to better account for these disparate doc counts\nand improve the accuracy of the selection of top terms. It is much cheaper to increase\nthe `shard_size` than to increase the `size`. However, it still takes more\nbytes over the wire and waiting in memory on the coordinating node.\n\nIMPORTANT: This guidance only applies if you're using the `terms` aggregation's\ndefault sort `order`. If you're sorting by anything other than document count in\ndescending order, see <<search-aggregations-bucket-terms-aggregation-order>>.\n\nNOTE:   `shard_size` cannot be smaller than `size` (as it doesn't make much sense). When it is, Elasticsearch will\n        override it and reset it to be equal to `size`.\n\n[[terms-agg-doc-count-error]]\n==== Document count error\n\nEven with a larger `shard_size` value, `doc_count` values for a `terms`\naggregation may be approximate. As a result, any sub-aggregations on the `terms`\naggregation may also be approximate.\n\n`sum_other_doc_count` is the number of documents that didn't make it into the\nthe top `size` terms. If this is greater than `0`, you can be sure that the\n`terms` agg had to throw away some buckets, either because they didn't fit into\n`size` on the coordinating node or they didn't fit into `shard_size` on the\ndata node.\n\n==== Per bucket document count error\n\nIf you set the `show_term_doc_count_error` parameter to `true`, the `terms`\naggregation will include `doc_count_error_upper_bound`, which is an upper bound\nto the error on the `doc_count` returned by each shard. It's the\nsum of the size of the largest bucket on each shard that didn't fit into\n`shard_size`.\n\nIn more concrete terms, imagine there is one bucket that is very large on one\nshard and just outside the `shard_size` on all the other shards. In that case,\nthe `terms` agg will return the bucket because it is large, but it'll be missing\ndata from many documents on the shards where the term fell below the `shard_size` threshold.\n`doc_count_error_upper_bound` is the maximum number of those missing documents.\n\n[source,console,id=terms-aggregation-doc-count-error-example]\n--------------------------------------------------\nGET /_search\n{\n  \"aggs\": {\n    \"products\": {\n      \"terms\": {\n        \"field\": \"product\",\n        \"size\": 5,\n        \"show_term_doc_count_error\": true\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[s/_search/_search\\?filter_path=aggregations/]\n\n\nThese errors can only be calculated in this way when the terms are ordered by descending document count. When the aggregation is\nordered by the terms values themselves (either ascending or descending) there is no error in the document count since if a shard\ndoes not return a particular term which appears in the results from another shard, it must not have that term in its index. When the\naggregation is either sorted by a sub aggregation or in order of ascending document count, the error in the document counts cannot be\ndetermined and is given a value of -1 to indicate this.\n\n[[search-aggregations-bucket-terms-aggregation-order]]\n==== Order\n\nBy default, the `terms` aggregation orders terms by descending document\n`_count`.  This produces a bounded <<terms-agg-doc-count-error,document count>>\nerror that {es} can report.\n\nYou can use the `order` parameter to specify a different sort order, but we\ndon't recommend it.  It is extremely easy to create a terms ordering that will\njust return wrong results, and not obvious to see when you have done so.\nChange this only with caution.\n\nWARNING: Especially avoid using `\"order\": { \"_count\": \"asc\" }`. If you need to find rare\nterms, use the\n<<search-aggregations-bucket-rare-terms-aggregation,`rare_terms`>> aggregation\ninstead. Due to the way the `terms` aggregation\n<<search-aggregations-bucket-terms-aggregation-shard-size,gets terms from\nshards>>, sorting by ascending doc count often produces inaccurate results.\n\n\n===== Ordering by the term value\nIn this case, the buckets are ordered by the actual term values, such as\nlexicographic order for keywords or numerically for numbers. This sorting is\nsafe in both ascending and descending directions, and produces accurate\nresults.\n\nExample of ordering the buckets alphabetically by their terms in an ascending manner:\n\n[source,console,id=terms-aggregation-asc-example]\n--------------------------------------------------\nGET /_search\n{\n  \"aggs\": {\n    \"genres\": {\n      \"terms\": {\n        \"field\": \"genre\",\n        \"order\": { \"_key\": \"asc\" }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n===== Ordering by a sub aggregation\n\nWARNING: Sorting by a sub aggregation generally produces incorrect ordering, due to the way the `terms` aggregation\n<<search-aggregations-bucket-terms-aggregation-shard-size,gets results from\nshards>>.\n\nThere are two cases when sub-aggregation ordering is safe and returns correct\nresults: sorting by a maximum in descending order, or sorting by a minimum in\nascending order. These approaches work because they align with the behavior of\nsub aggregations. That is, if you're looking for the largest maximum or the\nsmallest minimum, the global answer (from combined shards) must be included in\none of the local shard answers. Conversely, the smallest maximum and largest\nminimum wouldn't be accurately computed.\n\nNote also that in these cases, the ordering is correct but the doc counts and\nnon-ordering sub aggregations may still have errors (and {es} does not calculate a\nbound for those errors).\n\nOrdering the buckets by single value metrics sub-aggregation (identified by the aggregation name):\n\n[source,console,id=terms-aggregation-subaggregation-example]\n--------------------------------------------------\nGET /_search\n{\n  \"aggs\": {\n    \"genres\": {\n      \"terms\": {\n        \"field\": \"genre\",\n        \"order\": { \"max_play_count\": \"desc\" }\n      },\n      \"aggs\": {\n        \"max_play_count\": { \"max\": { \"field\": \"play_count\" } }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\nOrdering the buckets by multi value metrics sub-aggregation (identified by the aggregation name):\n\n[source,console,id=terms-aggregation-multivalue-subaggregation-example]\n--------------------------------------------------\nGET /_search\n{\n  \"aggs\": {\n    \"genres\": {\n      \"terms\": {\n        \"field\": \"genre\",\n        \"order\": { \"playback_stats.max\": \"desc\" }\n      },\n      \"aggs\": {\n        \"playback_stats\": { \"stats\": { \"field\": \"play_count\" } }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n[NOTE]\n.Pipeline aggs cannot be used for sorting\n=======================================\n\n<<search-aggregations-pipeline,Pipeline aggregations>> are run during the\nreduce phase after all other aggregations have already completed. For this\nreason, they cannot be used for ordering.\n\n=======================================\n\nIt is also possible to order the buckets based on a \"deeper\" aggregation in the hierarchy. This is supported as long\nas the aggregations path are of a single-bucket type, where the last aggregation in the path may either be a single-bucket\none or a metrics one. If it's a single-bucket type, the order will be defined by the number of docs in the bucket (i.e. `doc_count`),\nin case it's a metrics one, the same rules as above apply (where the path must indicate the metric name to sort by in case of\na multi-value metrics aggregation, and in case of a single-value metrics aggregation the sort will be applied on that value).\n\nThe path must be defined in the following form:\n\n// {wikipedia}/Extended_Backus%E2%80%93Naur_Form\n[source,ebnf]\n--------------------------------------------------\nAGG_SEPARATOR       =  '>' ;\nMETRIC_SEPARATOR    =  '.' ;\nAGG_NAME            =  <the name of the aggregation> ;\nMETRIC              =  <the name of the metric (in case of multi-value metrics aggregation)> ;\nPATH                =  <AGG_NAME> [ <AGG_SEPARATOR>, <AGG_NAME> ]* [ <METRIC_SEPARATOR>, <METRIC> ] ;\n--------------------------------------------------\n\n[source,console,id=terms-aggregation-hierarchy-example]\n--------------------------------------------------\nGET /_search\n{\n  \"aggs\": {\n    \"countries\": {\n      \"terms\": {\n        \"field\": \"artist.country\",\n        \"order\": { \"rock>playback_stats.avg\": \"desc\" }\n      },\n      \"aggs\": {\n        \"rock\": {\n          \"filter\": { \"term\": { \"genre\": \"rock\" } },\n          \"aggs\": {\n            \"playback_stats\": { \"stats\": { \"field\": \"play_count\" } }\n          }\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\nThe above will sort the artist's countries buckets based on the average play count among the rock songs.\n\nMultiple criteria can be used to order the buckets by providing an array of order criteria such as the following:\n\n[source,console,id=terms-aggregation-multicriteria-example]\n--------------------------------------------------\nGET /_search\n{\n  \"aggs\": {\n    \"countries\": {\n      \"terms\": {\n        \"field\": \"artist.country\",\n        \"order\": [ { \"rock>playback_stats.avg\": \"desc\" }, { \"_count\": \"desc\" } ]\n      },\n      \"aggs\": {\n        \"rock\": {\n          \"filter\": { \"term\": { \"genre\": \"rock\" } },\n          \"aggs\": {\n            \"playback_stats\": { \"stats\": { \"field\": \"play_count\" } }\n          }\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\nThe above will sort the artist's countries buckets based on the average play count among the rock songs and then by\ntheir `doc_count` in descending order.\n\nNOTE: In the event that two buckets share the same values for all order criteria the bucket's term value is used as a\ntie-breaker in ascending alphabetical order to prevent non-deterministic ordering of buckets.\n\n===== Ordering by count ascending\n\nOrdering terms by ascending document `_count` produces an unbounded error that\n{es} can't accurately report. We therefore strongly recommend against using\n`\"order\": { \"_count\": \"asc\" }` as shown in the following example:\n\n[source,console,id=terms-aggregation-count-example]\n--------------------------------------------------\nGET /_search\n{\n  \"aggs\": {\n    \"genres\": {\n      \"terms\": {\n        \"field\": \"genre\",\n        \"order\": { \"_count\": \"asc\" }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n==== Minimum document count\n\nIt is possible to only return terms that match more than a configured number of hits using the `min_doc_count` option:\n\n[source,console,id=terms-aggregation-min-doc-count-example]\n--------------------------------------------------\nGET /_search\n{\n  \"aggs\": {\n    \"tags\": {\n      \"terms\": {\n        \"field\": \"tags\",\n        \"min_doc_count\": 10\n      }\n    }\n  }\n}\n--------------------------------------------------\n\nThe above aggregation would only return tags which have been found in 10 hits or more. Default value is `1`.\n\n\nTerms are collected and ordered on a shard level and merged with the terms collected from other shards in a second step. However, the shard does not have the information about the global document count available. The decision if a term is added to a candidate list depends only on the order computed on the shard using local shard frequencies. The `min_doc_count` criterion is only applied after merging local terms statistics of all shards. In a way the decision to add the term as a candidate is made without being very _certain_ about if the term will actually reach the required `min_doc_count`. This might cause many (globally) high frequent terms to be missing in the final result if low frequent terms populated the candidate lists. To avoid this, the `shard_size` parameter can be increased to allow more candidate terms on the shards. However, this increases memory consumption and network traffic.\n\n[[search-aggregations-bucket-terms-shard-min-doc-count]]\n===== `shard_min_doc_count`\n\n// tag::min-doc-count[]\nThe parameter `shard_min_doc_count` regulates the _certainty_ a shard has if the term should actually be added to the candidate list or not with respect to the `min_doc_count`. Terms will only be considered if their local shard frequency within the set is higher than the `shard_min_doc_count`. If your dictionary contains many low frequent terms and you are not interested in those (for example misspellings), then you can set the `shard_min_doc_count` parameter to filter out candidate terms on a shard level that will with a reasonable certainty not reach the required `min_doc_count` even after merging the local counts. `shard_min_doc_count` is set to `0` per default and has no effect unless you explicitly set it.\n// end::min-doc-count[]\n\n\nNOTE:    Setting `min_doc_count`=`0` will also return buckets for terms that didn't match any hit. However, some of\n         the returned terms which have a document count of zero might only belong to deleted documents or documents\n         from other types, so there is no warranty that a `match_all` query would find a positive document count for\n         those terms.\n\nWARNING: When NOT sorting on `doc_count` descending, high values of `min_doc_count` may return a number of buckets\n         which is less than `size` because not enough data was gathered from the shards. Missing buckets can be\n         back by increasing `shard_size`.\n         Setting `shard_min_doc_count` too high will cause terms to be filtered out on a shard level. This value should be set much lower than `min_doc_count/#shards`.\n\n[[search-aggregations-bucket-terms-aggregation-script]]\n==== Script\n\nUse a <<runtime,runtime field>> if the data in your documents doesn't\nexactly match what you'd like to aggregate. If, for example, \"anthologies\"\nneed to be in a special category then you could run this:\n\n[source,console,id=terms-aggregation-script-example]\n--------------------------------------------------\nGET /_search\n{\n  \"size\": 0,\n  \"runtime_mappings\": {\n    \"normalized_genre\": {\n      \"type\": \"keyword\",\n      \"script\": \"\"\"\n        String genre = doc['genre'].value;\n        if (doc['product'].value.startsWith('Anthology')) {\n          emit(genre + ' anthology');\n        } else {\n          emit(genre);\n        }\n      \"\"\"\n    }\n  },\n  \"aggs\": {\n    \"genres\": {\n      \"terms\": {\n        \"field\": \"normalized_genre\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n\nWhich will look like:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"aggregations\": {\n    \"genres\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"electronic\",\n          \"doc_count\": 4\n        },\n        {\n          \"key\": \"rock\",\n          \"doc_count\": 3\n        },\n        {\n          \"key\": \"electronic anthology\",\n          \"doc_count\": 2\n        },\n        {\n          \"key\": \"jazz\",\n          \"doc_count\": 2\n        }\n      ]\n    }\n  },\n  ...\n}\n--------------------------------------------------\n// TESTRESPONSE[s/\\.\\.\\./\"took\": \"$body.took\", \"timed_out\": false, \"_shards\": \"$body._shards\", \"hits\": \"$body.hits\"/]\n\nThis is a little slower because the runtime field has to access two fields\ninstead of one and because there are some optimizations that work on\nnon-runtime `keyword` fields that we have to give up for for runtime\n`keyword` fields. If you need the speed, you can index the\n`normalized_genre` field.\n\n// TODO when we have calculated fields we can link to them here.\n\n\n==== Filtering Values\n\nIt is possible to filter the values for which buckets will be created. This can be done using the `include` and\n`exclude` parameters which are based on regular expression strings or arrays of exact values. Additionally,\n`include` clauses can filter using `partition` expressions.\n\n===== Filtering Values with regular expressions\n\n[source,console,id=terms-aggregation-regex-example]\n--------------------------------------------------\nGET /_search\n{\n  \"aggs\": {\n    \"tags\": {\n      \"terms\": {\n        \"field\": \"tags\",\n        \"include\": \".*sport.*\",\n        \"exclude\": \"water_.*\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n\nIn the above example, buckets will be created for all the tags that has the word `sport` in them, except those starting\nwith `water_` (so the tag `water_sports` will not be aggregated). The `include` regular expression will determine what\nvalues are \"allowed\" to be aggregated, while the `exclude` determines the values that should not be aggregated. When\nboth are defined, the `exclude` has precedence, meaning, the `include` is evaluated first and only then the `exclude`.\n\nThe syntax is the same as <<regexp-syntax,regexp queries>>.\n\n===== Filtering Values with exact values\n\nFor matching based on exact values the `include` and `exclude` parameters can simply take an array of\nstrings that represent the terms as they are found in the index:\n\n[source,console,id=terms-aggregation-exact-example]\n--------------------------------------------------\nGET /_search\n{\n  \"aggs\": {\n    \"JapaneseCars\": {\n      \"terms\": {\n        \"field\": \"make\",\n        \"include\": [ \"mazda\", \"honda\" ]\n      }\n    },\n    \"ActiveCarManufacturers\": {\n      \"terms\": {\n        \"field\": \"make\",\n        \"exclude\": [ \"rover\", \"jensen\" ]\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n===== Filtering Values with partitions\n\nSometimes there are too many unique terms to process in a single request/response pair so\nit can be useful to break the analysis up into multiple requests.\nThis can be achieved by grouping the field's values into a number of partitions at query-time and processing\nonly one partition in each request.\nConsider this request which is looking for accounts that have not logged any access recently:\n\n[source,console,id=terms-aggregation-partitions-example]\n--------------------------------------------------\nGET /_search\n{\n   \"size\": 0,\n   \"aggs\": {\n      \"expired_sessions\": {\n         \"terms\": {\n            \"field\": \"account_id\",\n            \"include\": {\n               \"partition\": 0,\n               \"num_partitions\": 20\n            },\n            \"size\": 10000,\n            \"order\": {\n               \"last_access\": \"asc\"\n            }\n         },\n         \"aggs\": {\n            \"last_access\": {\n               \"max\": {\n                  \"field\": \"access_date\"\n               }\n            }\n         }\n      }\n   }\n}\n--------------------------------------------------\n\nThis request is finding the last logged access date for a subset of customer accounts because we\nmight want to expire some customer accounts who haven't been seen for a long while.\nThe `num_partitions` setting has requested that the unique account_ids are organized evenly into twenty\npartitions (0 to 19). and the `partition` setting in this request filters to only consider account_ids falling\ninto partition 0. Subsequent requests should ask for partitions 1 then 2 etc to complete the expired-account analysis.\n\nNote that the `size` setting for the number of results returned needs to be tuned with the `num_partitions`.\nFor this particular account-expiration example the process for balancing values for `size` and `num_partitions` would be as follows:\n\n1. Use the `cardinality` aggregation to estimate the total number of unique account_id values\n2. Pick a value for `num_partitions` to break the number from 1) up into more manageable chunks\n3. Pick a `size` value for the number of responses we want from each partition\n4. Run a test request\n\nIf we have a circuit-breaker error we are trying to do too much in one request and must increase `num_partitions`.\nIf the request was successful but the last account ID in the date-sorted test response was still an account we might want to\nexpire then we may be missing accounts of interest and have set our numbers too low. We must either\n\n* increase the `size` parameter to return more results per partition (could be heavy on memory) or\n* increase the `num_partitions` to consider less accounts per request (could increase overall processing time as we need to make more requests)\n\nUltimately this is a balancing act between managing the Elasticsearch resources required to process a single request and the volume\nof requests that the client application must issue to complete a task.\n\nWARNING: Partitions cannot be used together with an `exclude` parameter.\n\n==== Multi-field terms aggregation\n\nThe `terms` aggregation does not support collecting terms from multiple fields\nin the same document. The reason is that the `terms` agg doesn't collect the\nstring term values themselves, but rather uses\n<<search-aggregations-bucket-terms-aggregation-execution-hint,global ordinals>>\nto produce a list of all of the unique values in the field. Global ordinals\nresults in an important performance boost which would not be possible across\nmultiple fields.\n\nThere are three approaches that you can use to perform a `terms` agg across\nmultiple fields:\n\n<<search-aggregations-bucket-terms-aggregation-script,Script>>::\n\nUse a script to retrieve terms from multiple fields. This disables the global\nordinals optimization and will be slower than collecting terms from a single\nfield, but it gives you the flexibility to implement this option at search\ntime.\n\n<<copy-to,`copy_to` field>>::\n\nIf you know ahead of time that you want to collect the terms from two or more\nfields, then use `copy_to` in your mapping to create a new dedicated field at\nindex time which contains the values from both fields. You can aggregate on\nthis single field, which will benefit from the global ordinals optimization.\n\n<<search-aggregations-bucket-multi-terms-aggregation, `multi_terms` aggregation>>::\n\nUse multi_terms aggregation to combine terms from multiple fields into a compound key. This\nalso disables the global ordinals and will be slower than collecting terms from a single field.\nIt is faster but less flexible than using a script.\n\n[[search-aggregations-bucket-terms-aggregation-collect]]\n==== Collect mode\n\nDeferring calculation of child aggregations\n\nFor fields with many unique terms and a small number of required results it can be more efficient to delay the calculation\nof child aggregations until the top parent-level aggs have been pruned. Ordinarily, all branches of the aggregation tree\nare expanded in one depth-first pass and only then any pruning occurs.\nIn some scenarios this can be very wasteful and can hit memory constraints.\nAn example problem scenario is querying a movie database for the 10 most popular actors and their 5 most common co-stars:\n\n[source,console,id=terms-aggregation-collect-mode-example]\n--------------------------------------------------\nGET /_search\n{\n  \"aggs\": {\n    \"actors\": {\n      \"terms\": {\n        \"field\": \"actors\",\n        \"size\": 10\n      },\n      \"aggs\": {\n        \"costars\": {\n          \"terms\": {\n            \"field\": \"actors\",\n            \"size\": 5\n          }\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\nEven though the number of actors may be comparatively small and we want only 50 result buckets there is a combinatorial explosion of buckets\nduring calculation - a single actor can produce n\u00b2 buckets where n is the number of actors. The sane option would be to first determine\nthe 10 most popular actors and only then examine the top co-stars for these 10 actors. This alternative strategy is what we call the `breadth_first` collection\nmode as opposed to the `depth_first` mode.\n\nNOTE: The `breadth_first` is the default mode for fields with a cardinality bigger than the requested size or when the cardinality is unknown (numeric fields or scripts for instance).\nIt is possible to override the default heuristic and to provide a collect mode directly in the request:\n\n[source,console,id=terms-aggregation-breadth-first-example]\n--------------------------------------------------\nGET /_search\n{\n  \"aggs\": {\n    \"actors\": {\n      \"terms\": {\n        \"field\": \"actors\",\n        \"size\": 10,\n        \"collect_mode\": \"breadth_first\" <1>\n      },\n      \"aggs\": {\n        \"costars\": {\n          \"terms\": {\n            \"field\": \"actors\",\n            \"size\": 5\n          }\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n<1> the possible values are `breadth_first` and `depth_first`\n\nWhen using `breadth_first` mode the set of documents that fall into the uppermost buckets are\ncached for subsequent replay so there is a memory overhead in doing this which is linear with the number of matching documents.\nNote that the `order` parameter can still be used to refer to data from a child aggregation when using the `breadth_first` setting - the parent\naggregation understands that this child aggregation will need to be called first before any of the other child aggregations.\n\nWARNING: Nested aggregations such as `top_hits` which require access to score information under an aggregation that uses the `breadth_first`\ncollection mode need to replay the query on the second pass but only for the documents belonging to the top buckets.\n\n[[search-aggregations-bucket-terms-aggregation-execution-hint]]\n==== Execution hint\n\nThere are different mechanisms by which terms aggregations can be executed:\n\n - by using field values directly in order to aggregate data per-bucket (`map`)\n - by using global ordinals of the field and allocating one bucket per global ordinal (`global_ordinals`)\n\nElasticsearch tries to have sensible defaults so this is something that generally doesn't need to be configured.\n\n`global_ordinals` is the default option for `keyword` field, it uses global ordinals to allocates buckets dynamically\nso memory usage is linear to the number of values of the documents that are part of the aggregation scope.\n\n`map` should only be considered when very few documents match a query. Otherwise the ordinals-based execution mode\nis significantly faster. By default, `map` is only used when running an aggregation on scripts, since they don't have\nordinals.\n\n[source,console,id=terms-aggregation-execution-hint-example]\n--------------------------------------------------\nGET /_search\n{\n  \"aggs\": {\n    \"tags\": {\n      \"terms\": {\n        \"field\": \"tags\",\n        \"execution_hint\": \"map\" <1>\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n<1> The possible values are `map`, `global_ordinals`\n\nPlease note that Elasticsearch will ignore this execution hint if it is not applicable and that there is no backward compatibility guarantee on these hints.\n\n==== Missing value\n\nThe `missing` parameter defines how documents that are missing a value should be treated.\nBy default they will be ignored but it is also possible to treat them as if they\nhad a value.\n\n[source,console,id=terms-aggregation-missing-example]\n--------------------------------------------------\nGET /_search\n{\n  \"aggs\": {\n    \"tags\": {\n      \"terms\": {\n        \"field\": \"tags\",\n        \"missing\": \"N/A\" <1>\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n<1> Documents without a value in the `tags` field will fall into the same bucket as documents that have the value `N/A`.\n\n==== Mixing field types\n\nWARNING: When aggregating on multiple indices the type of the aggregated field may not be the same in all indices.\nSome types are compatible with each other (`integer` and `long` or `float` and `double`) but when the types are a mix\nof decimal and non-decimal number the terms aggregation will promote the non-decimal numbers to decimal numbers.\nThis can result in a loss of precision in the bucket values.\n\n[discrete]\n[[search-aggregations-bucket-terms-aggregation-troubleshooting]]\n==== Troubleshooting\n\n===== Failed Trying to Format Bytes\nWhen running a terms aggregation (or other aggregation, but in practice usually\nterms) over multiple indices, you may get an error that starts with \"Failed\ntrying to format bytes...\".  This is usually caused by two of the indices not\nhaving the same mapping type for the field being aggregated.\n\n**Use an explicit `value_type`**\nAlthough it's best to correct the mappings, you can work around this issue if\nthe field is unmapped in one of the indices.  Setting the `value_type` parameter\ncan resolve the issue by coercing the unmapped field into the correct type.\n\n[source,console,id=terms-aggregation-value_type-example]\n----\nGET /_search\n{\n  \"aggs\": {\n    \"ip_addresses\": {\n      \"terms\": {\n        \"field\": \"destination_ip\",\n        \"missing\": \"0.0.0.0\",\n        \"value_type\": \"ip\"\n      }\n    }\n  }\n}\n----\n"
}