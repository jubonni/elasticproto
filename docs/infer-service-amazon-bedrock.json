{
    "meta": {
        "timestamp": "2024-11-01T03:02:53.483580",
        "size": 6558,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/infer-service-amazon-bedrock.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "infer-service-amazon-bedrock",
        "version": "8.15"
    },
    "doc": "[[infer-service-amazon-bedrock]]\n=== Amazon Bedrock {infer} service\n\nCreates an {infer} endpoint to perform an {infer} task with the `amazonbedrock` service.\n\n[discrete]\n[[infer-service-amazon-bedrock-api-request]]\n==== {api-request-title}\n\n`PUT /_inference/<task_type>/<inference_id>`\n\n[discrete]\n[[infer-service-amazon-bedrock-api-path-params]]\n==== {api-path-parms-title}\n\n`<inference_id>`::\n(Required, string)\ninclude::inference-shared.asciidoc[tag=inference-id]\n\n`<task_type>`::\n(Required, string)\ninclude::inference-shared.asciidoc[tag=task-type]\n+\n--\nAvailable task types:\n\n* `completion`,\n* `text_embedding`.\n--\n\n[discrete]\n[[infer-service-amazon-bedrock-api-request-body]]\n==== {api-request-body-title}\n\n`chunking_settings`::\n(Optional, object)\ninclude::inference-shared.asciidoc[tag=chunking-settings]\n\n`max_chunking_size`:::\n(Optional, integer)\ninclude::inference-shared.asciidoc[tag=chunking-settings-max-chunking-size]\n\n`overlap`:::\n(Optional, integer)\ninclude::inference-shared.asciidoc[tag=chunking-settings-overlap]\n\n`sentence_overlap`:::\n(Optional, integer)\ninclude::inference-shared.asciidoc[tag=chunking-settings-sentence-overlap]\n\n`strategy`:::\n(Optional, string)\ninclude::inference-shared.asciidoc[tag=chunking-settings-strategy]\n\n`service`::\n(Required, string) The type of service supported for the specified task type.\nIn this case,\n`amazonbedrock`.\n\n`service_settings`::\n(Required, object)\ninclude::inference-shared.asciidoc[tag=service-settings]\n+\n--\nThese settings are specific to the `amazonbedrock` service.\n--\n\n`access_key`:::\n(Required, string)\nA valid AWS access key that has permissions to use Amazon Bedrock and access to models for inference requests.\n\n`secret_key`:::\n(Required, string)\nA valid AWS secret key that is paired with the `access_key`.\nTo create or manage access and secret keys, see https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html[Managing access keys for IAM users] in the AWS documentation.\n\nIMPORTANT: You need to provide the access and secret keys only once, during the {infer} model creation.\nThe <<get-inference-api>> does not retrieve your access or secret keys.\nAfter creating the {infer} model, you cannot change the associated key pairs.\nIf you want to use a different access and secret key pair, delete the {infer} model and recreate it with the same name and the updated keys.\n\n`provider`:::\n(Required, string)\nThe model provider for your deployment.\nNote that some providers may support only certain task types.\nSupported providers include:\n\n* `amazontitan` - available for `text_embedding` and `completion` task types\n* `anthropic` - available for `completion` task type only\n* `ai21labs` - available for `completion` task type only\n* `cohere` - available for `text_embedding` and `completion` task types\n* `meta` - available for `completion` task type only\n* `mistral` - available for `completion` task type only\n\n`model`:::\n(Required, string)\nThe base model ID or an ARN to a custom model based on a foundational model.\nThe base model IDs can be found in the https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html[Amazon Bedrock model IDs] documentation.\nNote that the model ID must be available for the provider chosen, and your IAM user must have access to the model.\n\n`region`:::\n(Required, string)\nThe region that your model or ARN is deployed in.\nThe list of available regions per model can be found in the https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html[Model support by AWS region] documentation.\n\n`rate_limit`:::\n(Optional, object)\nBy default, the `amazonbedrock` service sets the number of requests allowed per minute to `240`.\nThis helps to minimize the number of rate limit errors returned from Amazon Bedrock.\nTo modify this, set the `requests_per_minute` setting of this object in your service settings:\n+\n--\ninclude::inference-shared.asciidoc[tag=request-per-minute-example]\n--\n\n`task_settings`::\n(Optional, object)\ninclude::inference-shared.asciidoc[tag=task-settings]\n+\n.`task_settings` for the `completion` task type\n[%collapsible%closed]\n=====\n\n`max_new_tokens`:::\n(Optional, integer)\nSets the maximum number for the output tokens to be generated.\nDefaults to 64.\n\n`temperature`:::\n(Optional, float)\nA number between 0.0 and 1.0 that controls the apparent creativity of the results. At temperature 0.0 the model is most deterministic, at temperature 1.0 most random.\nShould not be used if `top_p` or `top_k` is specified.\n\n`top_p`:::\n(Optional, float)\nAlternative to `temperature`. A number in the range of 0.0 to 1.0, to eliminate low-probability tokens. Top-p uses nucleus sampling to select top tokens whose sum of likelihoods does not exceed a certain value, ensuring both variety and coherence.\nShould not be used if `temperature` is specified.\n\n`top_k`:::\n(Optional, float)\nOnly available for `anthropic`, `cohere`, and `mistral` providers.\nAlternative to `temperature`. Limits samples to the top-K most likely words, balancing coherence and variability.\nShould not be used if `temperature` is specified.\n\n=====\n\n[discrete]\n[[inference-example-amazonbedrock]]\n==== Amazon Bedrock service example\n\nThe following example shows how to create an {infer} endpoint called `amazon_bedrock_embeddings` to perform a `text_embedding` task type.\n\nChoose chat completion and embeddings models that you have access to from the https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html[Amazon Bedrock base models].\n\n[source,console]\n------------------------------------------------------------\nPUT _inference/text_embedding/amazon_bedrock_embeddings\n{\n    \"service\": \"amazonbedrock\",\n    \"service_settings\": {\n        \"access_key\": \"<aws_access_key>\",\n        \"secret_key\": \"<aws_secret_key>\",\n        \"region\": \"us-east-1\",\n        \"provider\": \"amazontitan\",\n        \"model\": \"amazon.titan-embed-text-v2:0\"\n    }\n}\n------------------------------------------------------------\n// TEST[skip:TBD]\n\nThe next example shows how to create an {infer} endpoint called `amazon_bedrock_completion` to perform a `completion` task type.\n\n[source,console]\n------------------------------------------------------------\nPUT _inference/completion/amazon_bedrock_completion\n{\n    \"service\": \"amazonbedrock\",\n    \"service_settings\": {\n        \"access_key\": \"<aws_access_key>\",\n        \"secret_key\": \"<aws_secret_key>\",\n        \"region\": \"us-east-1\",\n        \"provider\": \"amazontitan\",\n        \"model\": \"amazon.titan-text-premier-v1:0\"\n    }\n}\n------------------------------------------------------------\n// TEST[skip:TBD]\n"
}