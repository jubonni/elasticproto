{
    "meta": {
        "timestamp": "2024-11-01T03:02:52.840600",
        "size": 2295,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-keyword-tokenizer.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "analysis-keyword-tokenizer",
        "version": "8.15"
    },
    "doc": "[[analysis-keyword-tokenizer]]\n=== Keyword tokenizer\n++++\n<titleabbrev>Keyword</titleabbrev>\n++++\n\nThe `keyword` tokenizer is a ``noop'' tokenizer that accepts whatever text it\nis given and outputs the exact same text as a single term. It can be combined\nwith token filters to normalise output, e.g. lower-casing email addresses.\n\n[discrete]\n=== Example output\n\n[source,console]\n---------------------------\nPOST _analyze\n{\n  \"tokenizer\": \"keyword\",\n  \"text\": \"New York\"\n}\n---------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"New York\",\n      \"start_offset\": 0,\n      \"end_offset\": 8,\n      \"type\": \"word\",\n      \"position\": 0\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe above sentence would produce the following term:\n\n[source,text]\n---------------------------\n[ New York ]\n---------------------------\n\n[discrete]\n[[analysis-keyword-tokenizer-token-filters]]\n=== Combine with token filters\nYou can combine the `keyword` tokenizer with token filters to normalise\nstructured data, such as product IDs or email addresses.\n\nFor example, the following <<indices-analyze,analyze API>> request uses the\n`keyword` tokenizer and <<analysis-lowercase-tokenfilter,`lowercase`>> filter to\nconvert an email address to lowercase.\n\n[source,console]\n---------------------------\nPOST _analyze\n{\n  \"tokenizer\": \"keyword\",\n  \"filter\": [ \"lowercase\" ],\n  \"text\": \"john.SMITH@example.COM\"\n}\n---------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"john.smith@example.com\",\n      \"start_offset\": 0,\n      \"end_offset\": 22,\n      \"type\": \"word\",\n      \"position\": 0\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe request produces the following token:\n\n[source,text]\n---------------------------\n[ john.smith@example.com ]\n---------------------------\n\n\n[discrete]\n=== Configuration\n\nThe `keyword` tokenizer accepts the following parameters:\n\n[horizontal]\n`buffer_size`::\n\n    The number of characters read into the term buffer in a single pass.\n    Defaults to `256`. The term buffer will grow by this size until all the\n    text has been consumed. It is advisable not to change this setting.\n\n"
}