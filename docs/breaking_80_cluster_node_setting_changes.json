{
    "meta": {
        "timestamp": "2024-11-01T03:02:52.181582",
        "size": 35687,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_80_cluster_node_setting_changes.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "breaking_80_cluster_node_setting_changes",
        "version": "8.15"
    },
    "doc": "[discrete]\n[[breaking_80_cluster_node_setting_changes]]\n==== Cluster and node setting changes\n\nTIP: {ess-setting-change}\n\n.`action.destructive_requires_name` now defaults to `true`. {ess-icon}\n[%collapsible]\n====\n*Details* +\nThe default for the `action.destructive_requires_name` setting changes from `false`\nto `true` in {es} 8.0.0.\n\nPreviously, defaulting to `false` allowed users to use wildcard\npatterns to delete, close, or change index blocks on indices.\nTo prevent the accidental deletion of indices that happen to match a\nwildcard pattern, we now default to requiring that destructive\noperations explicitly name the indices to be modified.\n\n*Impact* +\nTo use wildcard patterns for destructive actions, set\n`action.destructive_requires_name` to `false` using the\n{ref}/cluster-update-settings.html[] cluster settings API].\n====\n\n.You can no longer set `xpack.searchable.snapshot.shared_cache.size` on non-frozen nodes.\n[%collapsible]\n====\n*Details* +\nYou can no longer set\n{ref}/searchable-snapshots.html#searchable-snapshots-shared-cache[`xpack.searchable.snapshot.shared_cache.size`]\non a node that doesn't have the `data_frozen` node role. This setting reserves\ndisk space for the shared cache of partially mounted indices. {es} only\nallocates partially mounted indices to nodes with the `data_frozen` role.\n\n*Impact* +\nRemove `xpack.searchable.snapshot.shared_cache.size` from `elasticsearch.yml`\nfor nodes that don't have the `data_frozen` role. Specifying the setting on a\nnon-frozen node will result in an error on startup.\n====\n\n[[max_clause_count_change]]\n.`indices.query.bool.max_clause_count` is deprecated and has no effect.\n[%collapsible]\n====\n*Details* +\nElasticsearch will now dynamically set the maximum number of allowed clauses\nin a query, using a heuristic based on the size of the search thread pool and\nthe size of the heap allocated to the JVM. This limit has a minimum value of\n1024 and will in most cases be larger (for example, a node with 30Gb RAM and\n48 CPUs will have a maximum clause count of around 27,000). Larger heaps lead\nto higher values, and larger thread pools result in lower values.\n\n*Impact* +\nQueries with many clauses should be avoided whenever possible.\nIf you previously bumped this setting to accommodate heavy queries,\nyou might need to increase the amount of memory available to Elasticsearch,\nor to reduce the size of your search thread pool so that more memory is\navailable to each concurrent search.\n\nIn previous versions of Lucene you could get around this limit by nesting\nboolean queries within each other, but the limit is now based on the total\nnumber of leaf queries within the query as a whole and this workaround will\nno longer help.\n\nSpecifying `indices.query.bool.max_clause_count` will have no effect\nbut will generate deprecation warnings. To avoid these warnings, remove the\nsetting from `elasticsearch.yml` during an upgrade or node restart.\n====\n\n[[ilm-poll-interval-limit]]\n.`indices.lifecycle.poll_interval` must be greater than `1s`.\n[%collapsible]\n====\n*Details* +\nSetting `indices.lifecycle.poll_interval` too low can cause\nexcessive load on a cluster. The poll interval must now be at least `1s` (one second).\n\n*Impact* +\nSet `indices.lifecycle.poll_interval` setting to `1s` or\ngreater in `elasticsearch.yml` or through the\n{ref}/cluster-update-settings.html[cluster update settings API].\n\nSetting `indices.lifecycle.poll_interval` to less than `1s` in\n`elasticsearch.yml` will result in an error on startup.\n{ref}/cluster-update-settings.html[Cluster update settings API] requests that\nset `indices.lifecycle.poll_interval` to less than `1s` will return an error.\n====\n\n.The file and native realms are now enabled unless explicitly disabled.\n[%collapsible]\n====\n*Details* +\nThe file and native realms are now enabled unless explicitly disabled. If\nexplicitly disabled, the file and native realms remain disabled at all times.\n\nPreviously, the file and native realms had the following implicit behaviors:\n\n* If the file and native realms were not configured, they were implicitly disabled\nif any other realm was configured.\n\n* If no other realm was available because realms were either not configured,\nnot permitted by license, or explicitly disabled, the file and native realms\nwere enabled, even if explicitly disabled.\n\n*Impact* +\nTo explicitly disable the file or native realm, set the respective\n`file.<realm-name>.enabled` or `native.<realm-name>.enabled` setting to `false`\nunder the `xpack.security.authc.realms` namespace in `elasticsearch.yml`.\n\nThe following configuration example disables the native realm and the file realm.\n\n[source,yaml]\n----\nxpack.security.authc.realms:\n\n  native.realm1.enabled: false\n  file.realm2.enabled: false\n\n  ...\n----\n====\n\n.The realm `order` setting is now required.\n[%collapsible]\n====\n*Details* +\nThe `xpack.security.authc.realms.{type}.{name}.order` setting is now required and must be\nspecified for each explicitly configured realm. Each value must be unique.\n\n*Impact* +\nThe cluster will fail to start if the requirements are not met.\n\nFor example, the following configuration is invalid:\n[source,yaml]\n--------------------------------------------------\nxpack.security.authc.realms.kerberos.kerb1:\n  keytab.path: es.keytab\n  remove_realm_name: false\n--------------------------------------------------\n\nAnd must be configured as:\n[source,yaml]\n--------------------------------------------------\nxpack.security.authc.realms.kerberos.kerb1:\n  order: 0\n  keytab.path: es.keytab\n  remove_realm_name: false\n--------------------------------------------------\n====\n\n[[breaking_80_allocation_change_include_relocations_removed]]\n.`cluster.routing.allocation.disk.include_relocations` has been removed.\n[%collapsible]\n====\n*Details* +\n{es} now always accounts for the sizes of relocating shards when making\nallocation decisions based on the disk usage of the nodes in the cluster. In\nearlier versions, you could disable this by setting `cluster.routing.allocation.disk.include_relocations` to `false`.\nThat could result in poor allocation decisions that could overshoot watermarks and require significant\nextra work to correct. The `cluster.routing.allocation.disk.include_relocations` setting has been removed.\n\n*Impact* +\nRemove the `cluster.routing.allocation.disk.include_relocations`\nsetting. Specifying this setting in `elasticsearch.yml` will result in an error\non startup.\n====\n\n.`cluster.join.timeout` has been removed.\n[%collapsible]\n====\n*Details* +\nThe `cluster.join.timeout` setting has been removed. Join attempts no longer\ntime out.\n\n*Impact* +\nRemove `cluster.join.timeout` from `elasticsearch.yml`.\n====\n\n.`discovery.zen` settings have been removed.\n[%collapsible]\n====\n*Details* +\nAll settings under the `discovery.zen` namespace are no longer supported. They existed only only for BWC reasons in 7.x. This includes:\n\n- `discovery.zen.minimum_master_nodes`\n- `discovery.zen.no_master_block`\n- `discovery.zen.hosts_provider`\n- `discovery.zen.publish_timeout`\n- `discovery.zen.commit_timeout`\n- `discovery.zen.publish_diff.enable`\n- `discovery.zen.ping.unicast.concurrent_connects`\n- `discovery.zen.ping.unicast.hosts.resolve_timeout`\n- `discovery.zen.ping.unicast.hosts`\n- `discovery.zen.ping_timeout`\n- `discovery.zen.unsafe_rolling_upgrades_enabled`\n- `discovery.zen.fd.connect_on_network_disconnect`\n- `discovery.zen.fd.ping_interval`\n- `discovery.zen.fd.ping_timeout`\n- `discovery.zen.fd.ping_retries`\n- `discovery.zen.fd.register_connection_listener`\n- `discovery.zen.join_retry_attempts`\n- `discovery.zen.join_retry_delay`\n- `discovery.zen.join_timeout`\n- `discovery.zen.max_pings_from_another_master`\n- `discovery.zen.send_leave_request`\n- `discovery.zen.master_election.wait_for_joins_timeout`\n- `discovery.zen.master_election.ignore_non_master_pings`\n- `discovery.zen.publish.max_pending_cluster_states`\n- `discovery.zen.bwc_ping_timeout`\n\n*Impact* +\nRemove the `discovery.zen` settings from `elasticsearch.yml`. Specifying these settings will result in an error on startup.\n====\n\n.`http.content_type.required` has been removed.\n[%collapsible]\n====\n*Details* +\nThe `http.content_type.required` setting was deprecated in Elasticsearch 6.0\nand has been removed in Elasticsearch 8.0. The setting was introduced in\nElasticsearch 5.3 to prepare users for Elasticsearch 6.0, where content type\nauto detection was removed for HTTP requests.\n\n*Impact* +\nRemove the `http.content_type.required` setting from `elasticsearch.yml`. Specifying this setting  will result in an error on startup.\n====\n\n.`http.tcp_no_delay` has been removed.\n[%collapsible]\n====\n*Details* +\nThe `http.tcp_no_delay` setting was deprecated in 7.x and has been removed in 8.0. Use `http.tcp.no_delay` instead.\n\n*Impact* +\nReplace the `http.tcp_no_delay` setting with `http.tcp.no_delay`.\nSpecifying  `http.tcp_no_delay` in `elasticsearch.yml` will\nresult in an error on startup.\n====\n\n.`network.tcp.connect_timeout` has been removed.\n[%collapsible]\n====\n*Details* +\nThe `network.tcp.connect_timeout` setting was deprecated in 7.x and has been removed in 8.0. This setting\nwas a fallback setting for `transport.connect_timeout`.\n\n*Impact* +\nRemove the `network.tcp.connect_timeout` setting.\nUse the `transport.connect_timeout` setting to change the default connection\ntimeout for client connections. Specifying\n`network.tcp.connect_timeout` in `elasticsearch.yml` will result in an\nerror on startup.\n====\n\n.`node.max_local_storage_nodes` has been removed.\n[%collapsible]\n====\n*Details* +\nThe `node.max_local_storage_nodes` setting was deprecated in 7.x and\nhas been removed in 8.0. Nodes should be run on separate data paths\nto ensure that each node is consistently assigned to the same data path.\n\n*Impact* +\nRemove the `node.max_local_storage_nodes` setting. Specifying this\nsetting in `elasticsearch.yml` will result in an error on startup.\n====\n\n[[accept-default-password-removed]]\n.The `accept_default_password` setting has been removed.\n[%collapsible]\n====\n*Details* +\nThe `xpack.security.authc.accept_default_password` setting has not had any affect\nsince the 6.0 release of {es} and is no longer allowed.\n\n*Impact* +\nRemove  the `xpack.security.authc.accept_default_password` setting from `elasticsearch.yml`.\nSpecifying this setting will result in an error on startup.\n====\n\n[[roles-index-cache-removed]]\n.The `roles.index.cache.*` settings have been removed.\n[%collapsible]\n====\n*Details* +\nThe `xpack.security.authz.store.roles.index.cache.max_size` and\n`xpack.security.authz.store.roles.index.cache.ttl` settings have\nbeen removed. These settings have been redundant and deprecated\nsince the 5.2 release of {es}.\n\n*Impact* +\nRemove the `xpack.security.authz.store.roles.index.cache.max_size`\nand `xpack.security.authz.store.roles.index.cache.ttl` settings from `elasticsearch.yml` .\nSpecifying these settings will result in an error on startup.\n====\n\n[[separating-node-and-client-traffic]]\n.The `transport.profiles.*.xpack.security.type` setting has been removed.\n[%collapsible]\n====\n*Details* +\nThe `transport.profiles.*.xpack.security.type` setting is no longer supported.\nThe Transport Client has been removed and all client traffic now uses\nthe HTTP transport. Transport profiles using this setting should be removed.\n\n*Impact* +\nRemove the `transport.profiles.*.xpack.security.type` setting from `elasticsearch.yml`.\nSpecifying this setting in a transport profile will result in an error on startup.\n====\n\n[discrete]\n[[saml-realm-nameid-changes]]\n.The `nameid_format` SAML realm setting no longer has a default value.\n[%collapsible]\n====\n*Details* +\nIn SAML, Identity Providers (IdPs) can either be explicitly configured to\nrelease a `NameID` with a specific format, or configured to attempt to conform\nwith the requirements of a Service Provider (SP). The SP declares its\nrequirements in the `NameIDPolicy` element of a SAML Authentication Request.\nIn {es}, the `nameid_format` SAML realm setting controls the `NameIDPolicy`\nvalue.\n\nPreviously, the default value for `nameid_format` was\n`urn:oasis:names:tc:SAML:2.0:nameid-format:transient`. This setting created\nauthentication requests that required the IdP to release `NameID` with a\n`transient` format.\n\nThe default value has been removed, which means that {es} will create SAML Authentication Requests by default that don't put this requirement on the\nIdP. If you want to retain the previous behavior, set `nameid_format` to\n`urn:oasis:names:tc:SAML:2.0:nameid-format:transient`.\n\n*Impact* +\nIf you currently don't configure `nameid_format` explicitly, it's possible\nthat your IdP will reject authentication requests from {es} because the requests\ndo not specify a `NameID` format (and your IdP is configured to expect one).\nThis mismatch can result in a broken SAML configuration. If you're unsure whether\nyour IdP is explicitly configured to use a certain `NameID` format and you want to retain current behavior\n, try setting `nameid_format` to `urn:oasis:names:tc:SAML:2.0:nameid-format:transient` explicitly.\n====\n\n.The `xpack.security.transport.ssl.enabled` setting is now required to configure `xpack.security.transport.ssl` settings.\n[%collapsible]\n====\n*Details* +\nIt is now an error to configure any SSL settings for\n`xpack.security.transport.ssl` without also configuring\n`xpack.security.transport.ssl.enabled`.\n\n*Impact* +\nIf using other `xpack.security.transport.ssl` settings, you must explicitly\nspecify the `xpack.security.transport.ssl.enabled` setting.\n\nIf you do not want to enable SSL and are currently using other\n`xpack.security.transport.ssl` settings, do one of the following:\n\n* Explicitly specify `xpack.security.transport.ssl.enabled` as `false`\n* Discontinue use of other `xpack.security.transport.ssl` settings\n\nIf you want to enable SSL, follow the instructions in\n{ref}/configuring-tls.html#tls-transport[Encrypting communications between nodes\nin a cluster]. As part of this configuration, explicitly specify\n`xpack.security.transport.ssl.enabled` as `true`.\n\nFor example, the following configuration is invalid:\n[source,yaml]\n--------------------------------------------------\nxpack.security.transport.ssl.keystore.path: elastic-certificates.p12\nxpack.security.transport.ssl.truststore.path: elastic-certificates.p12\n--------------------------------------------------\n\nAnd must be configured as:\n[source,yaml]\n--------------------------------------------------\nxpack.security.transport.ssl.enabled: true <1>\nxpack.security.transport.ssl.keystore.path: elastic-certificates.p12\nxpack.security.transport.ssl.truststore.path: elastic-certificates.p12\n--------------------------------------------------\n<1> or `false`.\n====\n\n.The `xpack.security.http.ssl.enabled` setting is now required to configure `xpack.security.http.ssl` settings.\n[%collapsible]\n====\n*Details* +\nIt is now an error to configure any SSL settings for\n`xpack.security.http.ssl` without also configuring\n`xpack.security.http.ssl.enabled`.\n\n*Impact* +\nIf using other `xpack.security.http.ssl` settings, you must explicitly\nspecify the `xpack.security.http.ssl.enabled` setting.\n\nIf you do not want to enable SSL and are currently using other\n`xpack.security.http.ssl` settings, do one of the following:\n\n* Explicitly specify `xpack.security.http.ssl.enabled` as `false`\n* Discontinue use of other `xpack.security.http.ssl` settings\n\nIf you want to enable SSL, follow the instructions in\n{ref}/security-basic-setup-https.html#encrypt-http-communication[Encrypting HTTP client communications]. As part\nof this configuration, explicitly specify `xpack.security.http.ssl.enabled`\nas `true`.\n\nFor example, the following configuration is invalid:\n[source,yaml]\n--------------------------------------------------\nxpack.security.http.ssl.certificate: elasticsearch.crt\nxpack.security.http.ssl.key: elasticsearch.key\nxpack.security.http.ssl.certificate_authorities: [ \"corporate-ca.crt\" ]\n--------------------------------------------------\n\nAnd must be configured as either:\n[source,yaml]\n--------------------------------------------------\nxpack.security.http.ssl.enabled: true <1>\nxpack.security.http.ssl.certificate: elasticsearch.crt\nxpack.security.http.ssl.key: elasticsearch.key\nxpack.security.http.ssl.certificate_authorities: [ \"corporate-ca.crt\" ]\n--------------------------------------------------\n<1> or `false`.\n====\n\n.A `xpack.security.transport.ssl` certificate and key are now required to enable SSL for the transport interface.\n[%collapsible]\n====\n*Details* +\nIt is now an error to enable SSL for the transport interface without also configuring\na certificate and key through use of the `xpack.security.transport.ssl.keystore.path`\nsetting or the `xpack.security.transport.ssl.certificate` and\n`xpack.security.transport.ssl.key` settings.\n\n*Impact* +\nIf `xpack.security.transport.ssl.enabled` is set to `true`, provide a\ncertificate and key using the `xpack.security.transport.ssl.keystore.path`\nsetting or the `xpack.security.transport.ssl.certificate` and\n`xpack.security.transport.ssl.key` settings. If a certificate and key is not\nprovided, {es} will return in an error on startup.\n====\n\n.A `xpack.security.http.ssl` certificate and key are now required to enable SSL for the HTTP server.\n[%collapsible]\n====\n*Details* +\nIt is now an error to enable SSL for the HTTP (Rest) server without also configuring\na certificate and key through use of the `xpack.security.http.ssl.keystore.path`\nsetting or the `xpack.security.http.ssl.certificate` and\n`xpack.security.http.ssl.key` settings.\n\n*Impact* +\nIf `xpack.security.http.ssl.enabled` is set to `true`, provide a certificate and\nkey using the `xpack.security.http.ssl.keystore.path` setting or the\n`xpack.security.http.ssl.certificate` and `xpack.security.http.ssl.key`\nsettings. If certificate and key is not provided, {es} will return in an error\non startup.\n====\n\n.PKCS#11 keystores and trustores cannot be configured in `elasticsearch.yml`\n[%collapsible]\n====\n*Details* +\nThe settings `*.ssl.keystore.type` and `*.ssl.truststore.type` no longer accept \"PKCS11\" as a valid type.\nThis applies to all SSL settings in Elasticsearch, including\n\n- `xpack.security.http.keystore.type`\n- `xpack.security.transport.keystore.type`\n- `xpack.security.http.truststore.type`\n- `xpack.security.transport.truststore.type`\n\nAs well as SSL settings for security realms, watcher and monitoring.\n\nUse of a PKCS#11 keystore or truststore as the JRE's default store is not affected.\n\n*Impact* +\nIf you have a PKCS#11 keystore configured within your `elasticsearch.yml` file, you must remove that\nconfiguration and switch to a supported keystore type, or configure your PKCS#11 keystore as the\nJRE default store.\n====\n\n.The `kibana` user has been replaced by `kibana_system`.\n[%collapsible]\n====\n*Details* +\nThe `kibana` user was historically used to authenticate {kib} to {es}.\nThe name of this user was confusing, and was often mistakenly used to login to {kib}.\nThis has been renamed to `kibana_system` in order to reduce confusion, and to better\nalign with other built-in system accounts.\n\n*Impact* +\nReplace any use of the `kibana` user with the `kibana_system` user. Specifying\nthe `kibana` user in `kibana.yml` will result in an error on startup.\n\nIf your `kibana.yml` used to contain:\n[source,yaml]\n--------------------------------------------------\nelasticsearch.username: kibana\n--------------------------------------------------\n\nthen you should update to use the new `kibana_system` user instead:\n[source,yaml]\n--------------------------------------------------\nelasticsearch.username: kibana_system\n--------------------------------------------------\n\nIMPORTANT: The new `kibana_system` user does not preserve the previous `kibana`\nuser password. You must explicitly set a password for the `kibana_system` user.\n====\n\n[[search-remote-settings-removed]]\n.The `search.remote.*` settings have been removed.\n[%collapsible]\n====\n*Details* +\nIn 6.5 these settings were deprecated in favor of `cluster.remote`. In 7.x we\nprovided automatic upgrading of these settings to their `cluster.remote`\ncounterparts. In 8.0.0, these settings have been removed. Elasticsearch will\nrefuse to start if you have these settings in your configuration or cluster\nstate.\n\n*Impact* +\nUse the replacement `cluster.remote` settings. Discontinue use of the\n`search.remote.*` settings. Specifying these settings in `elasticsearch.yml`\nwill result in an error on startup.\n====\n\n[[remove-pidfile]]\n.The `pidfile` setting has been replaced by `node.pidfile`.\n[%collapsible]\n====\n*Details* +\nTo ensure that all settings are in a proper namespace, the `pidfile` setting was\npreviously deprecated in version 7.4.0 of Elasticsearch, and is removed in\nversion 8.0.0. Instead, use `node.pidfile`.\n\n*Impact* +\nUse the `node.pidfile` setting. Discontinue use of the `pidfile` setting.\nSpecifying the `pidfile` setting in `elasticsearch.yml` will result in an error\non startup.\n====\n\n[[remove-processors]]\n.The `processors` setting has been replaced by `node.processors`.\n[%collapsible]\n====\n*Details* +\nTo ensure that all settings are in a proper namespace, the `processors` setting\nwas previously deprecated in version 7.4.0 of Elasticsearch, and is removed in\nversion 8.0.0. Instead, use `node.processors`.\n\n*Impact* +\nUse the `node.processors` setting. Discontinue use of the `processors` setting.\nSpecifying the `processors` setting in `elasticsearch.yml` will result in an\nerror on startup.\n====\n\n.The `node.processors` setting can no longer exceed the available number of processors.\n[%collapsible]\n====\n*Details* +\nPreviously it was possible to set the number of processors used to set the\ndefault sizes for the thread pools to be more than the number of available\nprocessors. As this leads to more context switches and more threads but without\nan increase in the number of physical CPUs on which to schedule these additional\nthreads, the `node.processors` setting is now bounded by the number of available\nprocessors.\n\n*Impact* +\nIf specified, ensure the value of `node.processors` setting does not exceed the\nnumber of available processors. Setting the `node.processors` value greater than\nthe number of available processors in `elasticsearch.yml` will result in an\nerror on startup.\n====\n\n.The `cluster.remote.connect` setting has been removed.\n[%collapsible]\n====\n*Details* +\nIn Elasticsearch 7.7.0, the setting `cluster.remote.connect` was deprecated in\nfavor of setting `node.remote_cluster_client`. In Elasticsearch 8.0.0, the\nsetting `cluster.remote.connect` is removed.\n\n*Impact* +\nUse the `node.remote_cluster_client` setting. Discontinue use of the\n`cluster.remote.connect` setting. Specifying the `cluster.remote.connect`\nsetting in `elasticsearch.yml` will result in an error on startup.\n====\n\n.The `node.local_storage` setting has been removed.\n[%collapsible]\n====\n*Details* +\nIn Elasticsearch 7.8.0, the setting `node.local_storage` was deprecated and\nbeginning in Elasticsearch 8.0.0 all nodes will require local storage. Therefore,\nthe `node.local_storage` setting has been removed.\n\n*Impact* +\nDiscontinue use of the `node.local_storage` setting. Specifying this setting in\n`elasticsearch.yml` will result in an error on startup.\n====\n\n.The `auth.password` setting for HTTP monitoring has been removed.\n[%collapsible]\n====\n*Details* +\nIn Elasticsearch 7.7.0, the setting `xpack.monitoring.exporters.<exporterName>.auth.password`\nwas deprecated in favor of setting `xpack.monitoring.exporters.<exporterName>.auth.secure_password`.\nIn Elasticsearch 8.0.0, the setting `xpack.monitoring.exporters.<exporterName>.auth.password` is\nremoved.\n\n*Impact* +\nUse the `xpack.monitoring.exporters.<exporterName>.auth.secure_password`\nsetting. Discontinue use of the\n`xpack.monitoring.exporters.<exporterName>.auth.password` setting. Specifying\nthe `xpack.monitoring.exporters.<exporterName>.auth.password` setting in\n`elasticsearch.yml` will result in an error on startup.\n====\n\n.Settings used to disable basic license features have been removed.\n[%collapsible]\n====\n*Details* +\nThe following settings were deprecated in {es} 7.8.0 and have been removed\nin {es} 8.0.0:\n\n* `xpack.enrich.enabled`\n* `xpack.flattened.enabled`\n* `xpack.ilm.enabled`\n* `xpack.monitoring.enabled`\n* `xpack.rollup.enabled`\n* `xpack.slm.enabled`\n* `xpack.sql.enabled`\n* `xpack.transform.enabled`\n* `xpack.vectors.enabled`\n\nThese basic license features are now always enabled.\n\nIf you have disabled ILM so that you can use another tool to manage Watcher\nindices, the newly introduced `xpack.watcher.use_ilm_index_management` setting\nmay be set to false.\n\n*Impact* +\nDiscontinue use of the removed settings. Specifying these settings in\n`elasticsearch.yml` will result in an error on startup.\n====\n\n.Settings used to defer cluster recovery pending a certain number of master nodes have been removed.\n[%collapsible]\n====\n*Details* +\nThe following cluster settings have been removed:\n\n* `gateway.expected_nodes`\n* `gateway.expected_master_nodes`\n* `gateway.recover_after_nodes`\n* `gateway.recover_after_master_nodes`\n\nIt is safe to recover the cluster as soon as a majority of master-eligible\nnodes have joined so there is no benefit in waiting for any additional\nmaster-eligible nodes to start.\n\n*Impact* +\nDiscontinue use of the removed settings. If needed, use\n`gateway.expected_data_nodes` or `gateway.recover_after_data_nodes` to defer\ncluster recovery pending a certain number of data nodes.\n====\n\n.Legacy role settings have been removed.\n[%collapsible]\n====\n*Details* +\nThe legacy role settings:\n\n* `node.data`\n* `node.ingest`\n* `node.master`\n* `node.ml`\n* `node.remote_cluster_client`\n* `node.transform`\n* `node.voting_only`\n\nhave been removed. Instead, use the `node.roles` setting. If you were previously\nusing the legacy role settings on a 7.13 or later cluster, you will have a\ndeprecation log message on each of your nodes indicating the exact replacement\nvalue for `node.roles`.\n\n*Impact* +\nDiscontinue use of the removed settings. Specifying these settings in\n`elasticsearch.yml` will result in an error on startup.\n====\n\n[[system-call-filter-setting]]\n.The system call filter setting has been removed.\n[%collapsible]\n====\n*Details* +\nElasticsearch uses system call filters to remove its ability to fork another\nprocess. This is useful to mitigate remote code exploits. These system call\nfilters are enabled by default, and were previously controlled via the setting\n`bootstrap.system_call_filter`. Starting in Elasticsearch 8.0, system call\nfilters will be required. As such, the setting `bootstrap.system_call_filter`\nwas deprecated in Elasticsearch 7.13.0, and is removed as of Elasticsearch\n8.0.0.\n\n*Impact* +\nDiscontinue use of the removed setting. Specifying this setting in Elasticsearch\nconfiguration will result in an error on startup.\n====\n\n[[tier-filter-setting]]\n.Tier filtering settings have been removed.\n[%collapsible]\n====\n*Details* +\nThe cluster and index level settings ending in `._tier` used for filtering the allocation of a shard\nto a particular set of nodes have been removed. Instead, the\n{ref}/data-tier-shard-filtering.html#tier-preference-allocation-filter[tier\npreference setting], `index.routing.allocation.include._tier_preference` should\nbe used. The removed settings are:\n\nCluster level settings:\n\n- `cluster.routing.allocation.include._tier`\n- `cluster.routing.allocation.exclude._tier`\n- `cluster.routing.allocation.require._tier`\n\nIndex settings:\n\n- `index.routing.allocation.include._tier`\n- `index.routing.allocation.exclude._tier`\n- `index.routing.allocation.require._tier`\n\n*Impact* +\nDiscontinue use of the removed settings. Specifying any of these cluster settings in Elasticsearch\nconfiguration will result in an error on startup. Any indices using these settings will have the\nsettings archived (and they will have no effect) when the index metadata is loaded.\n====\n\n[[shared-data-path-setting]]\n.Shared data path and per index data path settings are deprecated.\n[%collapsible]\n====\n*Details* +\nElasticsearch uses the shared data path as the base path of per index data\npaths. This feature was previously used with shared replicas. Starting in\n7.13.0, these settings are deprecated. Starting in 8.0 only existing\nindices created in 7.x will be capable of using the shared data path and\nper index data path settings.\n\n*Impact* +\nDiscontinue use of the deprecated settings.\n====\n\n[[single-data-node-watermark-setting]]\n.The single data node watermark setting is deprecated and now only accepts `true`.\n[%collapsible]\n====\n*Details* +\nIn 7.14, setting `cluster.routing.allocation.disk.watermark.enable_for_single_data_node`\nto false was deprecated. Starting in 8.0, the only legal value will be\ntrue. In a future release, the setting will be removed completely, with same\nbehavior as if the setting was `true`.\n\nIf the old behavior is desired for a single data node cluster, disk based\nallocation can be disabled by setting\n`cluster.routing.allocation.disk.threshold_enabled: false`\n\n*Impact* +\nDiscontinue use of the deprecated setting.\n====\n\n[[auto-import-dangling-indices-removed]]\n.The `gateway.auto_import_dangling_indices` setting has been removed.\n[%collapsible]\n====\n*Details* +\nThe `gateway.auto_import_dangling_indices` cluster setting has been removed.\nPreviously, you could use this setting to automatically import\n{ref}/modules-gateway.html#dangling-indices[dangling indices]. However,\nautomatically importing dangling indices is unsafe. Use the\n{ref}/indices.html#dangling-indices-api[dangling indices APIs] to manage and\nimport dangling indices instead.\n\n*Impact* +\nDiscontinue use of the removed setting. Specifying the setting in\n`elasticsearch.yml` will result in an error on startup.\n====\n\n.The `listener` thread pool has been removed.\n[%collapsible]\n====\n*Details* +\nPreviously, the transport client used the thread pool to ensure listeners aren't\ncalled back on network threads. The transport client has been removed\nin 8.0, and the thread pool is no longer needed.\n\n*Impact* +\nRemove `listener` thread pool settings from `elasticsearch.yml` for any nodes.\nSpecifying `listener` thread pool settings in `elasticsearch.yml` will result in\nan error on startup.\n====\n\n.The `fixed_auto_queue_size` thread pool type has been removed.\n[%collapsible]\n====\n*Details* +\nThe `fixed_auto_queue_size` thread pool type, previously marked as an\nexperimental feature, was deprecated in 7.x and has been removed in 8.0.\nThe `search` and `search_throttled` thread pools have the `fixed` type now.\n\n*Impact* +\nNo action needed.\n====\n\n.Several `transport` settings have been replaced.\n[%collapsible]\n====\n*Details* +\nThe following settings have been deprecated in 7.x and removed in 8.0. Each setting has a replacement\nsetting that was introduced in 6.7.\n\n- `transport.tcp.port` replaced by `transport.port`\n- `transport.tcp.compress` replaced by `transport.compress`\n- `transport.tcp.connect_timeout` replaced by `transport.connect_timeout`\n- `transport.tcp_no_delay` replaced by `transport.tcp.no_delay`\n- `transport.profiles.profile_name.tcp_no_delay` replaced by `transport.profiles.profile_name.tcp.no_delay`\n- `transport.profiles.profile_name.tcp_keep_alive` replaced by `transport.profiles.profile_name.tcp.keep_alive`\n- `transport.profiles.profile_name.reuse_address` replaced by `transport.profiles.profile_name.tcp.reuse_address`\n- `transport.profiles.profile_name.send_buffer_size` replaced by `transport.profiles.profile_name.tcp.send_buffer_size`\n- `transport.profiles.profile_name.receive_buffer_size` replaced by `transport.profiles.profile_name.tcp.receive_buffer_size`\n\n*Impact* +\nUse the replacement settings. Discontinue use of the removed settings.\nSpecifying the removed settings in `elasticsearch.yml` will result in an error\non startup.\n====\n\n.Selective transport compression has been enabled by default.\n[%collapsible]\n====\n*Details* +\nPrior to 8.0, transport compression was disabled by default. Starting in 8.0,\n`transport.compress` defaults to `indexing_data`. This configuration means that\nthe propagation of raw indexing data will be compressed between nodes.\n\n*Impact* +\nInter-node transit will get reduced along the indexing path. In some scenarios,\nCPU usage could increase.\n====\n\n.Transport compression defaults to lz4.\n[%collapsible]\n====\n*Details* +\nPrior to 8.0, the `transport.compression_scheme` setting defaulted to `deflate`. Starting in\n8.0,  `transport.compress_scheme` defaults to `lz4`.\n\nPrior to 8.0, the `cluster.remote.<cluster_alias>.transport.compression_scheme`\nsetting defaulted to `deflate` when `cluster.remote.<cluster_alias>.transport.compress`\nwas explicitly configured. Starting in 8.0,\n`cluster.remote.<cluster_alias>.transport.compression_scheme` will fallback to\n`transport.compression_scheme` by default.\n\n*Impact* +\nThis configuration means that transport compression will produce somewhat lower\ncompression ratios in exchange for lower CPU load.\n====\n\n.The `repositories.fs.compress` node-level setting has been removed.\n[%collapsible]\n====\n*Details* +\nFor shared file system repositories (`\"type\": \"fs\"`), the node level setting `repositories.fs.compress` could\npreviously be used to enable compression for all shared file system repositories where `compress` was not specified.\nThe `repositories.fs.compress` setting has been removed.\n\n*Impact* +\nDiscontinue use of the `repositories.fs.compress` node-level setting. Use the\nrepository-specific `compress` setting to enable compression instead. Refer to\n{ref}/snapshots-filesystem-repository.html#filesystem-repository-settings[Shared\nfile system repository settings].\n====\n\n// This change is not notable because it should not have any impact on upgrades\n// However we document it here out of an abundance of caution\n[[fips-default-hash-changed]]\n.When FIPS mode is enabled the default password hash is now PBKDF2_STRETCH\n[%collapsible]\n====\n*Details* +\nIf `xpack.security.fips_mode.enabled` is true (see <<fips-140-compliance>>),\nthe value of `xpack.security.authc.password_hashing.algorithm` now defaults to\n`pbkdf2_stretch`.\n\nIn earlier versions this setting would always default to `bcrypt` and a runtime\ncheck would prevent a node from starting unless the value was explicitly set to\na \"pbkdf2\" variant.\n\nThere is no change for clusters that do not enable FIPS 140 mode.\n\n*Impact* +\nThis change should not have any impact on upgraded nodes.\nAny node with an explicitly configured value for the password hashing algorithm\nwill continue to use that configured value.\nAny node that did not have an explicitly configured password hashing algorithm in\n{es} 6.x or {es} 7.x would have failed to start.\n====\n\n.The `xpack.monitoring.history.duration` will not delete indices created by metricbeat or elastic agent\n[%collapsible]\n====\n*Details* +\n\nPrior to 8.0, Elasticsearch would internally handle removal of all monitoring indices according to the\n`xpack.monitoring.history.duration` setting.\n\nWhen using metricbeat or elastic agent >= 8.0 to collect monitoring data, indices are managed via an ILM policy. If the setting is present, the policy will be created using the `xpack.monitoring.history.duration` as an initial retention period.\n\nIf you need to customize retention settings for monitoring data collected with metricbeat, please update the `.monitoring-8-ilm-policy` ILM policy directly.\n\nThe `xpack.monitoring.history.duration` setting will only apply to monitoring indices written using (legacy) internal\ncollection, not indices created by metricbeat or agent.\n\n*Impact* +\nAfter upgrading, insure that the `.monitoring-8-ilm-policy` ILM policy aligns with your desired retention settings.\n\nIf you only use\nmetricbeat or agent to collect monitoring data, you can also remove any custom `xpack.monitoring.history.duration`\nsettings.\n\n====\n"
}