{
    "meta": {
        "timestamp": "2024-11-01T02:49:26.451066",
        "size": 2587,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-simplepattern-tokenizer.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "analysis-simplepattern-tokenizer",
        "version": "8.15"
    },
    "doc": "[[analysis-simplepattern-tokenizer]]\n=== Simple pattern tokenizer\n++++\n<titleabbrev>Simple pattern</titleabbrev>\n++++\n\nThe `simple_pattern` tokenizer uses a regular expression to capture matching\ntext as terms. The set of regular expression features it supports is more\nlimited than the <<analysis-pattern-tokenizer,`pattern`>> tokenizer, but the\ntokenization is generally faster.\n\nThis tokenizer does not support splitting the input on a pattern match, unlike\nthe <<analysis-pattern-tokenizer,`pattern`>> tokenizer. To split on pattern\nmatches using the same restricted regular expression subset, see the\n<<analysis-simplepatternsplit-tokenizer,`simple_pattern_split`>> tokenizer.\n\nThis tokenizer uses {lucene-core-javadoc}/org/apache/lucene/util/automaton/RegExp.html[Lucene regular expressions].\nFor an explanation of the supported features and syntax, see <<regexp-syntax,Regular Expression Syntax>>.\n\nThe default pattern is the empty string, which produces no terms. This\ntokenizer should always be configured with a non-default pattern.\n\n[discrete]\n=== Configuration\n\nThe `simple_pattern` tokenizer accepts the following parameters:\n\n[horizontal]\n`pattern`::\n    {lucene-core-javadoc}/org/apache/lucene/util/automaton/RegExp.html[Lucene regular expression], defaults to the empty string.\n\n[discrete]\n=== Example configuration\n\nThis example configures the `simple_pattern` tokenizer to produce terms that are\nthree-digit numbers\n\n[source,console]\n----------------------------\nPUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"my_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"simple_pattern\",\n          \"pattern\": \"[0123456789]{3}\"\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"fd-786-335-514-x\"\n}\n----------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\" : [\n    {\n      \"token\" : \"786\",\n      \"start_offset\" : 3,\n      \"end_offset\" : 6,\n      \"type\" : \"word\",\n      \"position\" : 0\n    },\n    {\n      \"token\" : \"335\",\n      \"start_offset\" : 7,\n      \"end_offset\" : 10,\n      \"type\" : \"word\",\n      \"position\" : 1\n    },\n    {\n      \"token\" : \"514\",\n      \"start_offset\" : 11,\n      \"end_offset\" : 14,\n      \"type\" : \"word\",\n      \"position\" : 2\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\nThe above example produces these terms:\n\n[source,text]\n---------------------------\n[ 786, 335, 514 ]\n---------------------------\n"
}