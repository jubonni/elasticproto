{
    "meta": {
        "timestamp": "2024-11-01T03:02:52.449579",
        "size": 6174,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-edgengram-tokenfilter.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "analysis-edgengram-tokenfilter",
        "version": "8.15"
    },
    "doc": "[[analysis-edgengram-tokenfilter]]\n=== Edge n-gram token filter\n++++\n<titleabbrev>Edge n-gram</titleabbrev>\n++++\n\nForms an {wikipedia}/N-gram[n-gram] of a specified length from\nthe beginning of a token.\n\nFor example, you can use the `edge_ngram` token filter to change `quick` to\n`qu`.\n\nWhen not customized, the filter creates 1-character edge n-grams by default.\n\nThis filter uses Lucene's\n{lucene-analysis-docs}/ngram/EdgeNGramTokenFilter.html[EdgeNGramTokenFilter].\n\n[NOTE]\n====\nThe `edge_ngram` filter is similar to the <<analysis-ngram-tokenizer,`ngram`\ntoken filter>>. However, the `edge_ngram` only outputs n-grams that start at the\nbeginning of a token. These edge n-grams are useful for\n<<search-as-you-type,search-as-you-type>> queries.\n====\n\n[[analysis-edgengram-tokenfilter-analyze-ex]]\n==== Example\n\nThe following <<indices-analyze,analyze API>> request uses the `edge_ngram`\nfilter to convert `the quick brown fox jumps` to 1-character and 2-character\nedge n-grams:\n\n[source,console]\n--------------------------------------------------\nGET _analyze\n{\n  \"tokenizer\": \"standard\",\n  \"filter\": [\n    { \"type\": \"edge_ngram\",\n      \"min_gram\": 1,\n      \"max_gram\": 2\n    }\n  ],\n  \"text\": \"the quick brown fox jumps\"\n}\n--------------------------------------------------\n\nThe filter produces the following tokens:\n\n[source,text]\n--------------------------------------------------\n[ t, th, q, qu, b, br, f, fo, j, ju ]\n--------------------------------------------------\n\n/////////////////////\n[source,console-result]\n--------------------------------------------------\n{\n  \"tokens\" : [\n    {\n      \"token\" : \"t\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 3,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 0\n    },\n    {\n      \"token\" : \"th\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 3,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 0\n    },\n    {\n      \"token\" : \"q\",\n      \"start_offset\" : 4,\n      \"end_offset\" : 9,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 1\n    },\n    {\n      \"token\" : \"qu\",\n      \"start_offset\" : 4,\n      \"end_offset\" : 9,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 1\n    },\n    {\n      \"token\" : \"b\",\n      \"start_offset\" : 10,\n      \"end_offset\" : 15,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 2\n    },\n    {\n      \"token\" : \"br\",\n      \"start_offset\" : 10,\n      \"end_offset\" : 15,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 2\n    },\n    {\n      \"token\" : \"f\",\n      \"start_offset\" : 16,\n      \"end_offset\" : 19,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 3\n    },\n    {\n      \"token\" : \"fo\",\n      \"start_offset\" : 16,\n      \"end_offset\" : 19,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 3\n    },\n    {\n      \"token\" : \"j\",\n      \"start_offset\" : 20,\n      \"end_offset\" : 25,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 4\n    },\n    {\n      \"token\" : \"ju\",\n      \"start_offset\" : 20,\n      \"end_offset\" : 25,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 4\n    }\n  ]\n}\n--------------------------------------------------\n/////////////////////\n\n[[analysis-edgengram-tokenfilter-analyzer-ex]]\n==== Add to an analyzer\n\nThe following <<indices-create-index,create index API>> request uses the\n`edge_ngram` filter to configure a new\n<<analysis-custom-analyzer,custom analyzer>>.\n\n[source,console]\n--------------------------------------------------\nPUT edge_ngram_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"standard_edge_ngram\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [ \"edge_ngram\" ]\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n[[analysis-edgengram-tokenfilter-configure-parms]]\n==== Configurable parameters\n\n`max_gram`::\n+\n--\n(Optional, integer)\nMaximum character length of a gram. For custom token filters, defaults to `2`.\nFor the built-in `edge_ngram` filter, defaults to `1`.\n\nSee <<analysis-edgengram-tokenfilter-max-gram-limits>>.\n--\n\n`min_gram`::\n(Optional, integer)\nMinimum character length of a gram. Defaults to `1`.\n\n`preserve_original`::\n(Optional, Boolean)\nEmits original token when set to `true`. Defaults to `false`.\n\n`side`::\n+\n--\n(Optional, string)\ndeprecated:[8.16.0, use <<analysis-reverse-tokenfilter,`reverse`>> token filter before and after `edge_ngram` for same results].\nIndicates whether to truncate tokens from the `front` or `back`. Defaults to `front`.\n--\n\n[[analysis-edgengram-tokenfilter-customize]]\n==== Customize\n\nTo customize the `edge_ngram` filter, duplicate it to create the basis\nfor a new custom token filter. You can modify the filter using its configurable\nparameters.\n\nFor example, the following request creates a custom `edge_ngram`\nfilter that forms n-grams between 3-5 characters.\n\n[source,console]\n--------------------------------------------------\nPUT edge_ngram_custom_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"default\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"3_5_edgegrams\" ]\n        }\n      },\n      \"filter\": {\n        \"3_5_edgegrams\": {\n          \"type\": \"edge_ngram\",\n          \"min_gram\": 3,\n          \"max_gram\": 5\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n[[analysis-edgengram-tokenfilter-max-gram-limits]]\n==== Limitations of the `max_gram` parameter\n\nThe `edge_ngram` filter's `max_gram` value limits the character length of\ntokens. When the `edge_ngram` filter is used with an index analyzer, this\nmeans search terms longer than the `max_gram` length may not match any indexed\nterms.\n\nFor example, if the `max_gram` is `3`, searches for `apple` won't match the\nindexed term `app`.\n\nTo account for this, you can use the\n<<analysis-truncate-tokenfilter,`truncate`>> filter with a search analyzer\nto shorten search terms to the `max_gram` character length. However, this could\nreturn irrelevant results.\n\nFor example, if the `max_gram` is `3` and search terms are truncated to three\ncharacters, the search term `apple` is shortened to `app`. This means searches\nfor `apple` return any indexed terms matching `app`, such as `apply`, `snapped`,\nand `apple`.\n\nWe recommend testing both approaches to see which best fits your\nuse case and desired search experience.\n"
}