{
    "meta": {
        "timestamp": "2024-11-01T03:02:53.087581",
        "size": 22009,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/paginate-search-results.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "paginate-search-results",
        "version": "8.15"
    },
    "doc": "[[paginate-search-results]]\n=== Paginate search results\n\nBy default, searches return the top 10 matching hits. To page through a larger\nset of results, you can use the <<search-search,search API>>'s `from` and `size`\nparameters. The `from` parameter defines the number of hits to skip, defaulting\nto `0`. The `size` parameter is the maximum number of hits to return. Together,\nthese two parameters define a page of results.\n\n[source,console]\n----\nGET /_search\n{\n  \"from\": 5,\n  \"size\": 20,\n  \"query\": {\n    \"match\": {\n      \"user.id\": \"kimchy\"\n    }\n  }\n}\n----\n\nAvoid using `from` and `size` to page too deeply or request too many results at\nonce. Search requests usually span multiple shards. Each shard must load its\nrequested hits and the hits for any previous pages into memory. For deep pages\nor large sets of results, these operations can significantly increase memory and\nCPU usage, resulting in degraded performance or node failures.\n\nBy default, you cannot use `from` and `size` to page through more than 10,000\nhits. This limit is a safeguard set by the\n<<index-max-result-window,`index.max_result_window`>> index setting. If you need\nto page through more than 10,000 hits, use the <<search-after,`search_after`>>\nparameter instead.\n\nWARNING: {es} uses Lucene's internal doc IDs as tie-breakers. These internal doc\nIDs can be completely different across replicas of the same data. When paging\nsearch hits, you might occasionally see that documents with the same sort values\nare not ordered consistently.\n\n[discrete]\n[[search-after]]\n=== Search after\n\nYou can use the `search_after` parameter to retrieve the next page of hits\nusing a set of <<sort-search-results,sort values>> from the previous page.\n\nUsing `search_after` requires multiple search requests with the same `query` and\n`sort` values. The first step is to run an initial request. The following\nexample sorts the results by two fields (`date` and `tie_breaker_id`):\n\n\n////\n[source,console]\n--------------------------------------------------\nPUT twitter\n{\n  \"mappings\": {\n    \"properties\": {\n      \"tie_breaker_id\": {\n        \"type\": \"keyword\"\n      },\n      \"date\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n////\n\n[source,console]\n--------------------------------------------------\nGET twitter/_search\n{\n    \"query\": {\n        \"match\": {\n            \"title\": \"elasticsearch\"\n        }\n    },\n    \"sort\": [\n        {\"date\": \"asc\"},\n        {\"tie_breaker_id\": \"asc\"}      <1>\n    ]\n}\n--------------------------------------------------\n//TEST[continued]\n\n<1> A copy of the `_id` field with `doc_values` enabled\n\nThe search response includes an array of `sort` values for each hit:\n\n[source,console-result]\n----\n{\n  \"took\" : 17,\n  \"timed_out\" : false,\n  \"_shards\" : ...,\n  \"hits\" : {\n    \"total\" : ...,\n    \"max_score\" : null,\n    \"hits\" : [\n      ...\n      {\n        \"_index\" : \"twitter\",\n        \"_id\" : \"654322\",\n        \"_score\" : null,\n        \"_source\" : ...,\n        \"sort\" : [\n          1463538855,\n          \"654322\"\n        ]\n      },\n      {\n        \"_index\" : \"twitter\",\n        \"_id\" : \"654323\",\n        \"_score\" : null,\n        \"_source\" : ...,\n        \"sort\" : [                                <1>\n          1463538857,\n          \"654323\"\n        ]\n      }\n    ]\n  }\n}\n----\n// TESTRESPONSE[skip: demo of where the sort values are]\n\n<1> Sort values for the last returned hit.\n\nTo retrieve the next page of results, repeat the request, take the `sort` values from the\nlast hit, and insert those into the `search_after` array:\n\n[source,console]\n--------------------------------------------------\nGET twitter/_search\n{\n    \"query\": {\n        \"match\": {\n            \"title\": \"elasticsearch\"\n        }\n    },\n    \"search_after\": [1463538857, \"654323\"],\n    \"sort\": [\n        {\"date\": \"asc\"},\n        {\"tie_breaker_id\": \"asc\"}\n    ]\n}\n--------------------------------------------------\n//TEST[continued]\n\nRepeat this process by updating the `search_after` array every time you retrieve a\nnew page of results. If a <<near-real-time,refresh>> occurs between these requests,\nthe order of your results may change, causing inconsistent results across pages. To\nprevent this, you can create a <<point-in-time-api,point in time (PIT)>> to\npreserve the current index state over your searches.\n\n[source,console]\n----\nPOST /my-index-000001/_pit?keep_alive=1m\n----\n// TEST[setup:my_index]\n\nThe API returns a PIT ID.\n\n[source,console-result]\n----\n{\n  \"id\": \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\",\n  \"_shards\": ...\n}\n----\n// TESTRESPONSE[s/\"id\": \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\"/\"id\": $body.id/]\n// TESTRESPONSE[s/\"_shards\": \\.\\.\\./\"_shards\": \"$body._shards\"/]\n\nTo get the first page of results, submit a search request with a `sort`\nargument. If using a PIT, specify the PIT ID in the `pit.id` parameter and omit\nthe target data stream or index from the request path.\n\nIMPORTANT: All PIT search requests add an implicit sort tiebreaker field called `_shard_doc`,\nwhich can also be provided explicitly.\nIf you cannot use a PIT, we recommend that you include a tiebreaker field\nin your `sort`. This tiebreaker field should contain a unique value for each document.\nIf you don't include a tiebreaker field, your paged results could miss or duplicate hits.\n\nNOTE: Search after requests have optimizations that make them faster when the sort\norder is `_shard_doc` and total hits are not tracked. If you want to iterate over all documents regardless of the\norder, this is the most efficient option.\n\nIMPORTANT: If the `sort` field is a <<date,`date`>> in some target data streams or indices\nbut a <<date_nanos,`date_nanos`>> field in other targets, use the `numeric_type` parameter\nto convert the values to a single resolution and the `format` parameter to specify a\n<<mapping-date-format, date format>> for the `sort` field. Otherwise, {es} won't interpret\nthe search after parameter correctly in each request.\n\n[source,console]\n----\nGET /_search\n{\n  \"size\": 10000,\n  \"query\": {\n    \"match\" : {\n      \"user.id\" : \"elkbee\"\n    }\n  },\n  \"pit\": {\n    \"id\":  \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\", <1>\n    \"keep_alive\": \"1m\"\n  },\n  \"sort\": [ <2>\n    {\"@timestamp\": {\"order\": \"asc\", \"format\": \"strict_date_optional_time_nanos\", \"numeric_type\" : \"date_nanos\" }}\n  ]\n}\n----\n// TEST[catch:unavailable]\n\n<1> PIT ID for the search.\n<2> Sorts hits for the search with an implicit tiebreak on `_shard_doc` ascending.\n\nThe search response includes an array of `sort` values for each hit. If you used\na PIT, a tiebreaker is included as the last `sort` values for each hit.\nThis tiebreaker called `_shard_doc` is added automatically on every search requests that use a PIT.\nThe `_shard_doc` value is the combination of the shard index within the PIT and the Lucene's internal doc ID,\nit is unique per document and constant within a PIT.\nYou can also add the tiebreaker explicitly in the search request to customize the order:\n\n[source,console]\n----\nGET /_search\n{\n  \"size\": 10000,\n  \"query\": {\n    \"match\" : {\n      \"user.id\" : \"elkbee\"\n    }\n  },\n  \"pit\": {\n    \"id\":  \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\", <1>\n    \"keep_alive\": \"1m\"\n  },\n  \"sort\": [ <2>\n    {\"@timestamp\": {\"order\": \"asc\", \"format\": \"strict_date_optional_time_nanos\"}},\n    {\"_shard_doc\": \"desc\"}\n  ]\n}\n----\n// TEST[catch:unavailable]\n\n<1> PIT ID for the search.\n<2> Sorts hits for the search with an explicit tiebreak on `_shard_doc` descending.\n\n\n[source,console-result]\n----\n{\n  \"pit_id\" : \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\", <1>\n  \"took\" : 17,\n  \"timed_out\" : false,\n  \"_shards\" : ...,\n  \"hits\" : {\n    \"total\" : ...,\n    \"max_score\" : null,\n    \"hits\" : [\n      ...\n      {\n        \"_index\" : \"my-index-000001\",\n        \"_id\" : \"FaslK3QBySSL_rrj9zM5\",\n        \"_score\" : null,\n        \"_source\" : ...,\n        \"sort\" : [                                <2>\n          \"2021-05-20T05:30:04.832Z\",\n          4294967298                              <3>\n        ]\n      }\n    ]\n  }\n}\n----\n// TESTRESPONSE[skip: unable to access PIT ID]\n\n<1> Updated `id` for the point in time.\n<2> Sort values for the last returned hit.\n<3> The tiebreaker value, unique per document within the `pit_id`.\n\nTo get the next page of results, rerun the previous search using the last hit's\nsort values (including the tiebreaker) as the `search_after` argument. If using a PIT, use the latest PIT\nID in the `pit.id` parameter. The search's `query` and `sort` arguments must\nremain unchanged. If provided, the `from` argument must be `0` (default) or `-1`.\n\n[source,console]\n----\nGET /_search\n{\n  \"size\": 10000,\n  \"query\": {\n    \"match\" : {\n      \"user.id\" : \"elkbee\"\n    }\n  },\n  \"pit\": {\n    \"id\":  \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\", <1>\n    \"keep_alive\": \"1m\"\n  },\n  \"sort\": [\n    {\"@timestamp\": {\"order\": \"asc\", \"format\": \"strict_date_optional_time_nanos\"}}\n  ],\n  \"search_after\": [                                <2>\n    \"2021-05-20T05:30:04.832Z\",\n    4294967298\n  ],\n  \"track_total_hits\": false                        <3>\n}\n----\n// TEST[catch:unavailable]\n\n<1> PIT ID returned by the previous search.\n<2> Sort values from the previous search's last hit.\n<3> Disable the tracking of total hits to speed up pagination.\n\nYou can repeat this process to get additional pages of results. If using a PIT,\nyou can extend the PIT's retention period using the\n`keep_alive` parameter of each search request.\n\nWhen you're finished, you should delete your PIT.\n\n[source,console]\n----\nDELETE /_pit\n{\n    \"id\" : \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\"\n}\n----\n// TEST[catch:missing]\n\n[discrete]\n[[scroll-search-results]]\n=== Scroll search results\n\nIMPORTANT: We no longer recommend using the scroll API for deep pagination. If\nyou need to preserve the index state while paging through more than 10,000 hits,\nuse the <<search-after,`search_after`>> parameter with a point in time (PIT).\n\nWhile a `search` request returns a single ``page'' of results, the `scroll`\nAPI can be used to retrieve large numbers of results (or even all results)\nfrom a single search request, in much the same way as you would use a cursor\non a traditional database.\n\nScrolling is not intended for real time user requests, but rather for\nprocessing large amounts of data, e.g. in order to reindex the contents of one\ndata stream or index into a new data stream or index with a different\nconfiguration.\n\n.Client support for scrolling and reindexing\n*********************************************\n\nSome of the officially supported clients provide helpers to assist with\nscrolled searches and reindexing:\n\nPerl::\n\n    See https://metacpan.org/pod/Search::Elasticsearch::Client::5_0::Bulk[Search::Elasticsearch::Client::5_0::Bulk]\n    and https://metacpan.org/pod/Search::Elasticsearch::Client::5_0::Scroll[Search::Elasticsearch::Client::5_0::Scroll]\n\nPython::\n\n    See https://elasticsearch-py.readthedocs.io/en/stable/helpers.html[elasticsearch.helpers.*]\n\nJavaScript::\n\n    See {jsclient-current}/client-helpers.html[client.helpers.*]\n\n*********************************************\n\nNOTE: The results that are returned from a scroll request reflect the state of\nthe data stream or index at the time that the initial `search` request was made, like a\nsnapshot in time. Subsequent changes to documents (index, update or delete)\nwill only affect later search requests.\n\nIn order to use scrolling, the initial search request should specify the\n`scroll` parameter in the query string, which tells Elasticsearch how long it\nshould keep the ``search context'' alive (see <<scroll-search-context>>), eg `?scroll=1m`.\n\n[source,console]\n--------------------------------------------------\nPOST /my-index-000001/_search?scroll=1m\n{\n  \"size\": 100,\n  \"query\": {\n    \"match\": {\n      \"message\": \"foo\"\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:my_index]\n\nThe result from the above request includes a `_scroll_id`, which should\nbe passed to the `scroll` API in order to retrieve the next batch of\nresults.\n\n[source,console]\n--------------------------------------------------\nPOST /_search/scroll                                                               <1>\n{\n  \"scroll\" : \"1m\",                                                                 <2>\n  \"scroll_id\" : \"DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAD4WYm9laVYtZndUQlNsdDcwakFMNjU1QQ==\" <3>\n}\n--------------------------------------------------\n// TEST[continued s/DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAD4WYm9laVYtZndUQlNsdDcwakFMNjU1QQ==/$body._scroll_id/]\n\n<1> `GET` or `POST` can be used and the URL should not include the `index`\n    name -- this is specified in the original `search` request instead.\n<2> The `scroll` parameter tells Elasticsearch to keep the search context open\n    for another `1m`.\n<3> The `scroll_id` parameter\n\nThe `size` parameter allows you to configure the maximum number of hits to be\nreturned with each batch of results. Each call to the `scroll` API returns the\nnext batch of results until there are no more results left to return, ie the\n`hits` array is empty.\n\nIMPORTANT: The initial search request and each subsequent scroll request each\nreturn a `_scroll_id`. While the `_scroll_id` may change between requests, it doesn\u2019t\nalways change\u2009\u2014\u2009in any case, only the most recently received `_scroll_id` should be used.\n\nNOTE: If the request specifies aggregations, only the initial search response\nwill contain the aggregations results.\n\nNOTE: Scroll requests have optimizations that make them faster when the sort\norder is `_doc`. If you want to iterate over all documents regardless of the\norder, this is the most efficient option:\n\n[source,console]\n--------------------------------------------------\nGET /_search?scroll=1m\n{\n  \"sort\": [\n    \"_doc\"\n  ]\n}\n--------------------------------------------------\n// TEST[setup:my_index]\n\n[discrete]\n[[scroll-search-context]]\n==== Keeping the search context alive\n\nA scroll returns all the documents which matched the search at the time of the\ninitial search request. It ignores any subsequent changes to these documents.\nThe `scroll_id` identifies a _search context_ which keeps track of everything\nthat {es} needs to return the correct documents. The search context is created\nby the initial request and kept alive by subsequent requests.\n\nThe `scroll` parameter (passed to the `search` request and to every `scroll`\nrequest) tells Elasticsearch how long it should keep the search context alive.\nIts value (e.g. `1m`, see <<time-units>>) does not need to be long enough to\nprocess all data -- it just needs to be long enough to process the previous\nbatch of results. Each `scroll` request (with the `scroll` parameter) sets a\nnew expiry time. If a `scroll` request doesn't pass in the `scroll`\nparameter, then the search context will be freed as part of _that_ `scroll`\nrequest.\n\nNormally, the background merge process optimizes the index by merging together\nsmaller segments to create new, bigger segments. Once the smaller segments are\nno longer needed they are deleted. This process continues during scrolling, but\nan open search context prevents the old segments from being deleted since they\nare still in use.\n\nTIP: Keeping older segments alive means that more disk space and file handles\nare needed. Ensure that you have configured your nodes to have ample free file\nhandles. See <<file-descriptors>>.\n\nAdditionally, if a segment contains deleted or updated documents then the\nsearch context must keep track of whether each document in the segment was live\nat the time of the initial search request. Ensure that your nodes have\nsufficient heap space if you have many open scrolls on an index that is subject\nto ongoing deletes or updates.\n\nNOTE: To prevent against issues caused by having too many scrolls open, the\nuser is not allowed to open scrolls past a certain limit. By default, the\nmaximum number of open scrolls is 500. This limit can be updated with the\n`search.max_open_scroll_context` cluster setting.\n\nYou can check how many search contexts are open with the\n<<cluster-nodes-stats,nodes stats API>>:\n\n[source,console]\n---------------------------------------\nGET /_nodes/stats/indices/search\n---------------------------------------\n\n[discrete]\n[[clear-scroll]]\n==== Clear scroll\n\nSearch context are automatically removed when the `scroll` timeout has been\nexceeded. However keeping scrolls open has a cost, as discussed in the\n<<scroll-search-context,previous section>> so scrolls should be explicitly\ncleared as soon as the scroll is not being used anymore using the\n`clear-scroll` API:\n\n[source,console]\n---------------------------------------\nDELETE /_search/scroll\n{\n  \"scroll_id\" : \"DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAD4WYm9laVYtZndUQlNsdDcwakFMNjU1QQ==\"\n}\n---------------------------------------\n// TEST[catch:missing]\n\nMultiple scroll IDs can be passed as array:\n\n[source,console]\n---------------------------------------\nDELETE /_search/scroll\n{\n  \"scroll_id\" : [\n    \"DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAD4WYm9laVYtZndUQlNsdDcwakFMNjU1QQ==\",\n    \"DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAABFmtSWWRRWUJrU2o2ZExpSGJCVmQxYUEAAAAAAAAAAxZrUllkUVlCa1NqNmRMaUhiQlZkMWFBAAAAAAAAAAIWa1JZZFFZQmtTajZkTGlIYkJWZDFhQQAAAAAAAAAFFmtSWWRRWUJrU2o2ZExpSGJCVmQxYUEAAAAAAAAABBZrUllkUVlCa1NqNmRMaUhiQlZkMWFB\"\n  ]\n}\n---------------------------------------\n// TEST[catch:missing]\n\nAll search contexts can be cleared with the `_all` parameter:\n\n[source,console]\n---------------------------------------\nDELETE /_search/scroll/_all\n---------------------------------------\n\nThe `scroll_id` can also be passed as a query string parameter or in the request body.\nMultiple scroll IDs can be passed as comma separated values:\n\n[source,console]\n---------------------------------------\nDELETE /_search/scroll/DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAD4WYm9laVYtZndUQlNsdDcwakFMNjU1QQ==,DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAABFmtSWWRRWUJrU2o2ZExpSGJCVmQxYUEAAAAAAAAAAxZrUllkUVlCa1NqNmRMaUhiQlZkMWFBAAAAAAAAAAIWa1JZZFFZQmtTajZkTGlIYkJWZDFhQQAAAAAAAAAFFmtSWWRRWUJrU2o2ZExpSGJCVmQxYUEAAAAAAAAABBZrUllkUVlCa1NqNmRMaUhiQlZkMWFB\n---------------------------------------\n// TEST[catch:missing]\n\n[discrete]\n[[slice-scroll]]\n==== Sliced scroll\n\nWhen paging through a large number of documents, it can be helpful to split the search into multiple slices\nto consume them independently:\n\n[source,console]\n--------------------------------------------------\nGET /my-index-000001/_search?scroll=1m\n{\n  \"slice\": {\n    \"id\": 0,                      <1>\n    \"max\": 2                      <2>\n  },\n  \"query\": {\n    \"match\": {\n      \"message\": \"foo\"\n    }\n  }\n}\nGET /my-index-000001/_search?scroll=1m\n{\n  \"slice\": {\n    \"id\": 1,\n    \"max\": 2\n  },\n  \"query\": {\n    \"match\": {\n      \"message\": \"foo\"\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:my_index_big]\n\n<1> The id of the slice\n<2> The maximum number of slices\n\nThe result from the first request returned documents that belong to the first slice (id: 0) and\nthe result from the second request returned documents that belong to the second slice. Since the\nmaximum number of slices is set to 2 the union of the results of the two requests is equivalent\nto the results of a scroll query without slicing. By default the splitting is done first on the\nshards, then locally on each shard using the `_id` field. The local splitting follows the formula\n`slice(doc) = floorMod(hashCode(doc._id), max))`.\n\nEach scroll is independent and can be processed in parallel like any scroll request.\n\nNOTE: If the number of slices is bigger than the number of shards the slice filter is very slow on\nthe first calls, it has a complexity of O(N) and a memory cost equals to N bits per slice where N\nis the total number of documents in the shard. After few calls the filter should be cached and\nsubsequent calls should be faster but you should limit the number of sliced query you perform in\nparallel to avoid the memory explosion.\n\nThe <<point-in-time-api,point-in-time>> API supports a more efficient partitioning strategy and\ndoes not suffer from this problem. When possible, it's recommended to use a point-in-time search\nwith slicing instead of a scroll.\n\nAnother way to avoid this high cost is to use the `doc_values` of another field to do the slicing.\nThe field must have the following properties:\n\n    * The field is numeric.\n\n    * `doc_values` are enabled on that field\n\n    * Every document should contain a single value. If a document has multiple values for the specified field, the first value is used.\n\n    * The value for each document should be set once when the document is created and never updated. This ensures that each\nslice gets deterministic results.\n\n    * The cardinality of the field should be high. This ensures that each slice gets approximately the same amount of documents.\n\n[source,console]\n--------------------------------------------------\nGET /my-index-000001/_search?scroll=1m\n{\n  \"slice\": {\n    \"field\": \"@timestamp\",\n    \"id\": 0,\n    \"max\": 10\n  },\n  \"query\": {\n    \"match\": {\n      \"message\": \"foo\"\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:my_index_big]\n\nFor append only time-based indices, the `timestamp` field can be used safely.\n"
}