{
    "meta": {
        "size": 11346,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/disk-based-shard-allocation.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "disk-based-shard-allocation",
        "version": "8.15"
    },
    "doc": "[[disk-based-shard-allocation]]\n==== Disk-based shard allocation settings\n[[disk-based-shard-allocation-description]]\n// tag::disk-based-shard-allocation-description-tag[]\n\nThe disk-based shard allocator ensures that all nodes have enough disk space\nwithout performing more shard movements than necessary. It allocates shards\nbased on a pair of thresholds known as the _low watermark_ and the _high\nwatermark_. Its primary goal is to ensure that no node exceeds the high\nwatermark, or at least that any such overage is only temporary. If a node\nexceeds the high watermark then {es} will solve this by moving some of its\nshards onto other nodes in the cluster.\n\nNOTE: It is normal for nodes to temporarily exceed the high watermark from time\nto time.\n\nThe allocator also tries to keep nodes clear of the high watermark by\nforbidding the allocation of more shards to a node that exceeds the low\nwatermark. Importantly, if all of your nodes have exceeded the low watermark\nthen no new shards can be allocated and {es} will not be able to move any\nshards between nodes in order to keep the disk usage below the high watermark.\nYou must ensure that your cluster has enough disk space in total and that there\nare always some nodes below the low watermark.\n\nShard movements triggered by the disk-based shard allocator must also satisfy\nall other shard allocation rules such as\n<<cluster-shard-allocation-filtering,allocation filtering>> and\n<<forced-awareness,forced awareness>>. If these rules are too strict then they\ncan also prevent the shard movements needed to keep the nodes' disk usage under\ncontrol. If you are using <<data-tiers,data tiers>> then {es} automatically\nconfigures allocation filtering rules to place shards within the appropriate\ntier, which means that the disk-based shard allocator works independently\nwithin each tier.\n\nIf a node is filling up its disk faster than {es} can move shards elsewhere\nthen there is a risk that the disk will completely fill up. To prevent this, as\na last resort, once the disk usage reaches the _flood-stage_ watermark {es}\nwill block writes to indices with a shard on the affected node. It will also\ncontinue to move shards onto the other nodes in the cluster. When disk usage\non the affected node drops below the high watermark, {es} automatically removes\nthe write block. Refer to <<fix-watermark-errors,Fix watermark errors>> to \nresolve persistent watermark errors.\n\n[[disk-based-shard-allocation-does-not-balance]]\n[TIP]\n====\nIt is normal for the nodes in your cluster to be using very different amounts\nof disk space. The <<shards-rebalancing-settings,balance>> of the cluster\ndepends on a combination of factors which includes the number of shards on each\nnode, the indices to which those shards belong, and the resource needs of each\nshard in terms of its size on disk and its CPU usage. {es} must trade off all\nof these factors against each other, and a cluster which is balanced when\nlooking at the combination of all of these factors may not appear to be\nbalanced if you focus attention on just one of them.\n====\n\nYou can use the following settings to control disk-based allocation:\n\n[[cluster-routing-disk-threshold]]\n// tag::cluster-routing-disk-threshold-tag[]\n`cluster.routing.allocation.disk.threshold_enabled`::\n(<<dynamic-cluster-setting,Dynamic>>)\nDefaults to `true`. Set to `false` to disable the disk allocation decider. Upon disabling, it will also remove any existing `index.blocks.read_only_allow_delete` index blocks.\n// end::cluster-routing-disk-threshold-tag[]\n\n[[cluster-routing-watermark-low]]\n// tag::cluster-routing-watermark-low-tag[]\n`cluster.routing.allocation.disk.watermark.low` {ess-icon}::\n(<<dynamic-cluster-setting,Dynamic>>)\nControls the low watermark for disk usage. It defaults to `85%`, meaning that {es} will not allocate shards to nodes that have more than 85% disk used. It can alternatively be set to a ratio value, e.g., `0.85`. It can also be set to an absolute byte value (like `500mb`) to prevent {es} from allocating shards if less than the specified amount of space is available. This setting has no effect on the primary shards of newly-created indices but will prevent their replicas from being allocated.\n// end::cluster-routing-watermark-low-tag[]\n\n`cluster.routing.allocation.disk.watermark.low.max_headroom`::\n(<<dynamic-cluster-setting,Dynamic>>) Controls the max headroom for the low watermark (in case of a percentage/ratio value).\nDefaults to 200GB when `cluster.routing.allocation.disk.watermark.low` is not explicitly set.\nThis caps the amount of free space required.\n\n[[cluster-routing-watermark-high]]\n// tag::cluster-routing-watermark-high-tag[]\n`cluster.routing.allocation.disk.watermark.high` {ess-icon}::\n(<<dynamic-cluster-setting,Dynamic>>)\nControls the high watermark. It defaults to `90%`, meaning that {es} will attempt to relocate shards away from a node whose disk usage is above 90%. It can alternatively be set to a ratio value, e.g., `0.9`. It can also be set to an absolute byte value (similarly to the low watermark) to relocate shards away from a node if it has less than the specified amount of free space. This setting affects the allocation of all shards, whether previously allocated or not.\n// end::cluster-routing-watermark-high-tag[]\n\n`cluster.routing.allocation.disk.watermark.high.max_headroom`::\n(<<dynamic-cluster-setting,Dynamic>>) Controls the max headroom for the high watermark (in case of a percentage/ratio value).\nDefaults to 150GB when `cluster.routing.allocation.disk.watermark.high` is not explicitly set.\nThis caps the amount of free space required.\n\n`cluster.routing.allocation.disk.watermark.enable_for_single_data_node`::\n    (<<static-cluster-setting,Static>>)\nIn earlier releases, the default behaviour was to disregard disk watermarks for a single\ndata node cluster when making an allocation decision. This is deprecated behavior\nsince 7.14 and has been removed in 8.0. The only valid value for this setting\nis now `true`. The setting will be removed in a future release.\n\n[[cluster-routing-flood-stage]]\n// tag::cluster-routing-flood-stage-tag[]\n`cluster.routing.allocation.disk.watermark.flood_stage` {ess-icon}::\n+\n--\n(<<dynamic-cluster-setting,Dynamic>>)\nControls the flood stage watermark, which defaults to 95%. {es} enforces a read-only index block (`index.blocks.read_only_allow_delete`) on every index that has one or more shards allocated on the node, and that has at least one disk exceeding the flood stage. This setting is a last resort to prevent nodes from running out of disk space. The index block is automatically released when the disk utilization falls below the high watermark. Similarly to the low and high watermark values, it can alternatively be set to a ratio value, e.g., `0.95`, or an absolute byte value.\n\nAn example of resetting the read-only index block on the `my-index-000001` index:\n\n[source,console]\n--------------------------------------------------\nPUT /my-index-000001/_settings\n{\n  \"index.blocks.read_only_allow_delete\": null\n}\n--------------------------------------------------\n// TEST[setup:my_index]\n--\n// end::cluster-routing-flood-stage-tag[]\n\n`cluster.routing.allocation.disk.watermark.flood_stage.max_headroom`::\n(<<dynamic-cluster-setting,Dynamic>>) Controls the max headroom for the flood stage watermark (in case of a percentage/ratio value).\nDefaults to 100GB when\n`cluster.routing.allocation.disk.watermark.flood_stage` is not explicitly set.\nThis caps the amount of free space required.\n\nNOTE: You cannot mix the usage of percentage/ratio values and byte values across\nthe `cluster.routing.allocation.disk.watermark.low`, `cluster.routing.allocation.disk.watermark.high`,\nand `cluster.routing.allocation.disk.watermark.flood_stage` settings. Either all values\nare set to percentage/ratio values, or all are set to byte values. This enforcement is\nso that {es} can validate that the settings are internally consistent, ensuring that the\nlow disk threshold is less than the high disk threshold, and the high disk threshold is\nless than the flood stage threshold. A similar comparison check is done for the max\nheadroom values.\n\n[[cluster-routing-flood-stage-frozen]]\n// tag::cluster-routing-flood-stage-tag[]\n`cluster.routing.allocation.disk.watermark.flood_stage.frozen` {ess-icon}::\n(<<dynamic-cluster-setting,Dynamic>>)\nControls the flood stage watermark for dedicated frozen nodes, which defaults to\n95%.\n\n`cluster.routing.allocation.disk.watermark.flood_stage.frozen.max_headroom` {ess-icon}::\n(<<dynamic-cluster-setting,Dynamic>>)\nControls the max headroom for the flood stage watermark (in case of a\npercentage/ratio value) for dedicated frozen nodes. Defaults to 20GB when\n`cluster.routing.allocation.disk.watermark.flood_stage.frozen` is not explicitly\nset. This caps the amount of free space required on dedicated frozen nodes.\n\n`cluster.info.update.interval`::\n    (<<dynamic-cluster-setting,Dynamic>>)\n    How often {es} should check on disk usage for each node in the\n    cluster. Defaults to `30s`.\n\nNOTE: Percentage values refer to used disk space, while byte values refer to\nfree disk space. This can be confusing, since it flips the meaning of high and\nlow. For example, it makes sense to set the low watermark to 10gb and the high\nwatermark to 5gb, but not the other way around.\n\nAn example of updating the low watermark to at least 100 gigabytes free, a high\nwatermark of at least 50 gigabytes free, and a flood stage watermark of 10\ngigabytes free, and updating the information about the cluster every minute:\n\n[source,console]\n--------------------------------------------------\nPUT _cluster/settings\n{\n  \"persistent\": {\n    \"cluster.routing.allocation.disk.watermark.low\": \"100gb\",\n    \"cluster.routing.allocation.disk.watermark.high\": \"50gb\",\n    \"cluster.routing.allocation.disk.watermark.flood_stage\": \"10gb\",\n    \"cluster.info.update.interval\": \"1m\"\n  }\n}\n--------------------------------------------------\n\nConcerning the max headroom settings for the watermarks, please note\nthat these apply only in the case that the watermark settings are percentages/ratios.\nThe aim of a max headroom value is to cap the required free disk space before hitting\nthe respective watermark. This is especially useful for servers with larger\ndisks, where a percentage/ratio watermark could translate to a big free disk space requirement,\nand the max headroom can be used to cap the required free disk space amount.\nAs an example, let us take the default settings for the flood watermark.\nIt has a 95% default value, and the flood max headroom setting has a default value of 100GB.\nThis means that:\n\n* For a smaller disk, e.g., of 100GB, the flood watermark will hit at 95%, meaning at 5GB\nof free space, since 5GB is smaller than the 100GB max headroom value.\n* For a larger disk, e.g., of 100TB, the flood watermark will hit at 100GB of free space.\nThat is because the 95% flood watermark alone would require 5TB of free disk space, but\nthat is capped by the max headroom setting to 100GB.\n\nFinally, the max headroom settings have their default values only if their respective watermark\nsettings are not explicitly set (thus, they have their default percentage values).\nIf watermarks are explicitly set, then the max headroom settings do not have their default values,\nand would need to be explicitly set if they are desired.\n"
}