{
    "meta": {
        "size": 7671,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-voting.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "modules-discovery-voting",
        "version": "8.15"
    },
    "doc": "[[modules-discovery-voting]]\n=== Voting configurations\n\nEach {es} cluster has a _voting configuration_, which is the set of\n<<master-node,master-eligible nodes>> whose responses are counted when making\ndecisions such as electing a new master or committing a new cluster state.\nDecisions are made only after a majority (more than half) of the nodes in the\nvoting configuration respond.\n\nUsually the voting configuration is the same as the set of all the \nmaster-eligible nodes that are currently in the cluster. However, there are some\nsituations in which they may be different.\n\ninclude::quorums.asciidoc[tag=quorums-and-availability]\n\nAfter a node joins or leaves the cluster, {es} reacts by automatically making\ncorresponding changes to the voting configuration in order to ensure that the\ncluster is as resilient as possible. It is important to wait for this adjustment\nto complete before you remove more nodes from the cluster. For more information,\nsee <<add-elasticsearch-nodes>>.\n\nThe current voting configuration is stored in the cluster state so you can\ninspect its current contents as follows:\n\n[source,console]\n--------------------------------------------------\nGET /_cluster/state?filter_path=metadata.cluster_coordination.last_committed_config\n--------------------------------------------------\n\nNOTE: The current voting configuration is not necessarily the same as the set of\nall available master-eligible nodes in the cluster. Altering the voting\nconfiguration involves taking a vote, so it takes some time to adjust the\nconfiguration as nodes join or leave the cluster. Also, there are situations\nwhere the most resilient configuration includes unavailable nodes or does not\ninclude some available nodes. In these situations, the voting configuration\ndiffers from the set of available master-eligible nodes in the cluster.\n\nLarger voting configurations are usually more resilient, so Elasticsearch\nnormally prefers to add master-eligible nodes to the voting configuration after\nthey join the cluster. Similarly, if a node in the voting configuration\nleaves the cluster and there is another master-eligible node in the cluster that\nis not in the voting configuration then it is preferable to swap these two nodes\nover. The size of the voting configuration is thus unchanged but its\nresilience increases.\n\nIt is not so straightforward to automatically remove nodes from the voting\nconfiguration after they have left the cluster. Different strategies have\ndifferent benefits and drawbacks, so the right choice depends on how the cluster\nwill be used. You can control whether the voting configuration automatically\nshrinks by using the\n<<modules-discovery-settings,`cluster.auto_shrink_voting_configuration` setting>>.\n\nNOTE: If `cluster.auto_shrink_voting_configuration` is set to `true` (which is\nthe default and recommended value) and there are at least three master-eligible\nnodes in the cluster, Elasticsearch remains capable of processing cluster state\nupdates as long as all but one of its master-eligible nodes are healthy.\n\nThere are situations in which Elasticsearch might tolerate the loss of multiple\nnodes, but this is not guaranteed under all sequences of failures. If the\n`cluster.auto_shrink_voting_configuration` setting is `false`, you must remove\ndeparted nodes from the voting configuration manually. Use the\n<<voting-config-exclusions,voting exclusions API>> to achieve the desired level\nof resilience.\n\nNo matter how it is configured, Elasticsearch will not suffer from a \n\"{wikipedia}/Split-brain_(computing)[split-brain]\" inconsistency. \nThe `cluster.auto_shrink_voting_configuration`\nsetting affects only its availability in the event of the failure of some of its\nnodes and the administrative tasks that must be performed as nodes join and\nleave the cluster.\n\n[discrete]\n==== Even numbers of master-eligible nodes\n\nThere should normally be an odd number of master-eligible nodes in a cluster.\nIf there is an even number, Elasticsearch leaves one of them out of the voting\nconfiguration to ensure that it has an odd size. This omission does not decrease\nthe failure-tolerance of the cluster. In fact, improves it slightly: if the\ncluster suffers from a network partition that divides it into two equally-sized\nhalves then one of the halves will contain a majority of the voting\nconfiguration and will be able to keep operating. If all of the votes from\nmaster-eligible nodes were counted, neither side would contain a strict majority\nof the nodes and so the cluster would not be able to make any progress.\n\nFor instance if there are four master-eligible nodes in the cluster and the\nvoting configuration contained all of them, any quorum-based decision would\nrequire votes from at least three of them. This situation means that the cluster\ncan tolerate the loss of only a single master-eligible node. If this cluster\nwere split into two equal halves, neither half would contain three\nmaster-eligible nodes and the cluster would not be able to make any progress.\nIf the voting configuration contains only three of the four master-eligible\nnodes, however, the cluster is still only fully tolerant to the loss of one\nnode, but quorum-based decisions require votes from two of the three voting\nnodes. In the event of an even split, one half will contain two of the three\nvoting nodes so that half will remain available.\n\n[discrete]\n==== Setting the initial voting configuration\n\nWhen a brand-new cluster starts up for the first time, it must elect its first\nmaster node. To do this election, it needs to know the set of master-eligible\nnodes whose votes should count. This initial voting configuration is known as\nthe _bootstrap configuration_ and is set in the\n<<modules-discovery-bootstrap-cluster,cluster bootstrapping process>>.\n\nIt is important that the bootstrap configuration identifies exactly which nodes\nshould vote in the first election. It is not sufficient to configure each node\nwith an expectation of how many nodes there should be in the cluster. It is also\nimportant to note that the bootstrap configuration must come from outside the\ncluster: there is no safe way for the cluster to determine the bootstrap\nconfiguration correctly on its own.\n\nIf the bootstrap configuration is not set correctly, when you start a brand-new\ncluster there is a risk that you will accidentally form two separate clusters\ninstead of one. This situation can lead to data loss: you might start using both\nclusters before you notice that anything has gone wrong and it is impossible to\nmerge them together later.\n\nNOTE: To illustrate the problem with configuring each node to expect a certain\ncluster size, imagine starting up a three-node cluster in which each node knows\nthat it is going to be part of a three-node cluster. A majority of three nodes\nis two, so normally the first two nodes to discover each other form a cluster\nand the third node joins them a short time later. However, imagine that four\nnodes were erroneously started instead of three. In this case, there are enough\nnodes to form two separate clusters. Of course if each node is started manually\nthen it's unlikely that too many nodes are started. If you're using an automated\norchestrator, however, it's certainly possible to get into this situation--\nparticularly if the orchestrator is not resilient to failures such as network\npartitions.\n\nThe initial quorum is only required the very first time a whole cluster starts\nup. New nodes joining an established cluster can safely obtain all the\ninformation they need from the elected master. Nodes that have previously been\npart of a cluster will have stored to disk all the information that is required\nwhen they restart.\n"
}