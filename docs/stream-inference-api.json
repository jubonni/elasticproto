{
    "meta": {
        "timestamp": "2024-11-01T03:07:10.348282",
        "size": 2830,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/stream-inference-api.html",
        "type": "documentation",
        "role": [
            "xpack"
        ],
        "has_code": true,
        "title": "stream-inference-api",
        "version": "8.15"
    },
    "doc": "[role=\"xpack\"]\n[[stream-inference-api]]\n=== Stream inference API\n\nStreams a chat completion response.\n\nIMPORTANT: The {infer} APIs enable you to use certain services, such as built-in {ml} models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.\nFor built-in models and models uploaded through Eland, the {infer} APIs offer an alternative way to use and manage trained models.\nHowever, if you do not plan to use the {infer} APIs to use these models or if you want to use non-NLP models, use the <<ml-df-trained-models-apis>>.\n\n\n[discrete]\n[[stream-inference-api-request]]\n==== {api-request-title}\n\n`POST /_inference/<inference_id>/_stream`\n\n`POST /_inference/<task_type>/<inference_id>/_stream`\n\n\n[discrete]\n[[stream-inference-api-prereqs]]\n==== {api-prereq-title}\n\n* Requires the `monitor_inference` <<privileges-list-cluster,cluster privilege>>\n(the built-in `inference_admin` and `inference_user` roles grant this privilege)\n* You must use a client that supports streaming.\n\n\n[discrete]\n[[stream-inference-api-desc]]\n==== {api-description-title}\n\nThe stream {infer} API enables real-time responses for completion tasks by delivering answers incrementally, reducing response times during computation.\nIt only works with the `completion` task type.\n\n\n[discrete]\n[[stream-inference-api-path-params]]\n==== {api-path-parms-title}\n\n`<inference_id>`::\n(Required, string)\nThe unique identifier of the {infer} endpoint.\n\n\n`<task_type>`::\n(Optional, string)\nThe type of {infer} task that the model performs.\n\n\n[discrete]\n[[stream-inference-api-request-body]]\n==== {api-request-body-title}\n\n`input`::\n(Required, string or array of strings)\nThe text on which you want to perform the {infer} task.\n`input` can be a single string or an array.\n+\n--\n[NOTE]\n====\nInference endpoints for the `completion` task type currently only support a\nsingle string as input.\n====\n--\n\n\n[discrete]\n[[stream-inference-api-example]]\n==== {api-examples-title}\n\nThe following example performs a completion on the example question with streaming.\n\n\n[source,console]\n------------------------------------------------------------\nPOST _inference/completion/openai-completion/_stream\n{\n  \"input\": \"What is Elastic?\"\n}\n------------------------------------------------------------\n// TEST[skip:TBD]\n\n\nThe API returns the following response:\n\n\n[source,txt]\n------------------------------------------------------------\nevent: message\ndata: {\n  \"completion\":[{\n    \"delta\":\"Elastic\"\n  }]\n}\n\nevent: message\ndata: {\n  \"completion\":[{\n    \"delta\":\" is\"\n    },\n    {\n    \"delta\":\" a\"\n    }\n  ]\n}\n\nevent: message\ndata: {\n  \"completion\":[{\n    \"delta\":\" software\"\n  },\n  {\n    \"delta\":\" company\"\n  }]\n}\n\n(...)\n------------------------------------------------------------\n// NOTCONSOLE\n"
}