{
    "meta": {
        "timestamp": "2024-11-01T03:02:53.728583",
        "size": 8389,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "modules-threadpool",
        "version": "8.15"
    },
    "doc": "[[modules-threadpool]]\n=== Thread pools\n\nA node uses several thread pools to manage memory consumption.\nQueues associated with many of the thread pools enable pending requests\nto be held instead of discarded.\n\nThere are several thread pools, but the important ones include:\n\n`generic`::\n    For generic operations (for example, background node discovery).\n    Thread pool type is `scaling`.\n\n[[search-threadpool]]\n`search`::\n    For count/search operations at the shard level. Used also by fetch and other search\n    related operations  Thread pool type is `fixed` with a size of `int((`<<node.processors,\n    `# of allocated processors`>>`pass:[ * ]3) / 2) + 1`, and queue_size of `1000`.\n\n[[search-throttled]]`search_throttled`::\n    For count/search/suggest/get operations on `search_throttled indices`.\n    Thread pool type is `fixed` with a size of `1`, and queue_size of `100`.\n\n`search_coordination`::\n    For lightweight search-related coordination operations. Thread pool type is\n    `fixed` with a size of `(`<<node.processors, `# of allocated processors`>>`) / 2`,\n    and queue_size of `1000`.\n\n`get`::\n    For get operations. Thread pool type is\n    `fixed` with a size of `int((`<<node.processors,\n    `# of allocated processors`>>`pass:[ * ]3) / 2) + 1`, and queue_size of `1000`.\n\n`analyze`::\n    For analyze requests. Thread pool type is `fixed` with a size of `1`, queue\n    size of `16`.\n\n`write`::\n    For single-document index/delete/update, ingest processors, and bulk requests. Thread pool type\n    is `fixed` with a size of <<node.processors, `# of allocated processors`>>,\n    queue_size of `10000`. The maximum size for this pool is\n    `pass:[1 + ]`<<node.processors, `# of allocated processors`>>.\n\n`snapshot`::\n    For snapshot/restore operations. Thread pool type is `scaling` with a\n    keep-alive of `5m`. On nodes with at least 750MB of heap the maximum size\n    of this pool is `10` by default. On nodes with less than 750MB of heap the\n    maximum size of this pool is `min(5, (`<<node.processors,\n    `# of allocated processors`>>`) / 2)` by default.\n\n`snapshot_meta`::\n    For snapshot repository metadata read operations. Thread pool type is `scaling` with a\n    keep-alive of `5m` and a max of `min(50, (`<<node.processors,\n    `# of allocated processors`>>`* 3))`.\n\n`warmer`::\n    For segment warm-up operations. Thread pool type is `scaling` with a\n    keep-alive of `5m` and a max of `min(5, (`<<node.processors,\n    `# of allocated processors`>>`) / 2)`.\n\n`refresh`::\n    For refresh operations. Thread pool type is `scaling` with a\n    keep-alive of `5m` and a max of `min(10, (`<<node.processors,\n    `# of allocated processors`>>`) / 2)`.\n\n`fetch_shard_started`::\n    For listing shard states.\n    Thread pool type is `scaling` with keep-alive of `5m` and a default maximum\n    size of `pass:[2 * ]`<<node.processors, `# of allocated processors`>>.\n\n`fetch_shard_store`::\n    For listing shard stores.\n    Thread pool type is `scaling` with keep-alive of `5m` and a default maximum\n    size of `pass:[2 * ]`<<node.processors, `# of allocated processors`>>.\n\n`flush`::\n    For <<indices-flush,flush>> and <<index-modules-translog, translog>> `fsync`\n    operations. Thread pool type is `scaling` with a keep-alive of `5m` and a\n    default maximum size of `min(5, (`<<node.processors,\n    `# of allocated processors`>>`) / 2)`.\n\n`force_merge`::\n    For <<indices-forcemerge,force merge>> operations.\n    Thread pool type is `fixed` with a size of `max(1, (`<<node.processors,\n`# of allocated processors`>>`) / 8)` and an unbounded queue size.\n\n`management`::\n    For cluster management.\n    Thread pool type is `scaling` with a keep-alive of `5m` and a default\n    maximum size of `5`.\n\n`system_read`::\n    For read operations on system indices.\n    Thread pool type is `fixed` with a default maximum size of\n    `min(5, (`<<node.processors, `# of allocated processors`>>`) / 2)`.\n\n`system_write`::\n    For write operations on system indices.\n    Thread pool type is `fixed` with a default maximum size of\n    `min(5, (`<<node.processors, `# of allocated processors`>>`) / 2)`.\n\n`system_critical_read`::\n    For critical read operations on system indices.\n    Thread pool type is `fixed` with a default maximum size of\n    `min(5, (`<<node.processors, `# of allocated processors`>>`) / 2)`.\n\n`system_critical_write`::\n    For critical write operations on system indices.\n    Thread pool type is `fixed` with a default maximum size of\n    `min(5, (`<<node.processors, `# of allocated processors`>>`) / 2)`.\n\n`watcher`::\n    For <<xpack-alerting,watch executions>>.\n    Thread pool type is `fixed` with a default maximum size of\n    `min(5 * (`<<node.processors, `# of allocated processors`>>`), 50)`\n    and queue_size of `1000`.\n\n[[modules-threadpool-esql]]`esql_worker`::\n    Executes <<esql>> operations. Thread pool type is `fixed` with a\n    size of `int((`<<node.processors, `# of allocated processors`>>\n    `pass:[ * ]3) / 2) + 1`, and queue_size of `1000`.\n\nThread pool settings are <<static-cluster-setting,static>> and can be changed by\nediting `elasticsearch.yml`. Changing a specific thread pool can be done by\nsetting its type-specific parameters; for example, changing the number of\nthreads in the `write` thread pool:\n\n[source,yaml]\n--------------------------------------------------\nthread_pool:\n    write:\n        size: 30\n--------------------------------------------------\n\n[[thread-pool-types]]\n==== Thread pool types\n\nThe following are the types of thread pools and their respective parameters:\n\n[[fixed-thread-pool]]\n===== `fixed`\n\nThe `fixed` thread pool holds a fixed size of threads to handle the\nrequests with a queue (optionally bounded) for pending requests that\nhave no threads to service them.\n\nThe `size` parameter controls the number of threads.\n\nThe `queue_size` allows to control the size of the queue of pending\nrequests that have no threads to execute them. By default, it is set to\n`-1` which means its unbounded. When a request comes in and the queue is\nfull, it will abort the request.\n\n[source,yaml]\n--------------------------------------------------\nthread_pool:\n    write:\n        size: 30\n        queue_size: 1000\n--------------------------------------------------\n\n[[scaling-thread-pool]]\n===== `scaling`\n\nThe `scaling` thread pool holds a dynamic number of threads. This\nnumber is proportional to the workload and varies between the value of\nthe `core` and `max` parameters.\n\nThe `keep_alive` parameter determines how long a thread should be kept\naround in the thread pool without it doing any work.\n\n[source,yaml]\n--------------------------------------------------\nthread_pool:\n    warmer:\n        core: 1\n        max: 8\n        keep_alive: 2m\n--------------------------------------------------\n\n[[node.processors]]\n==== Allocated processors setting\n\nThe number of processors is automatically detected, and the thread pool settings\nare automatically set based on it. In some cases it can be useful to override\nthe number of detected processors. This can be done by explicitly setting the\n`node.processors` setting. This setting is bounded by the number of available\nprocessors and accepts floating point numbers, which can be useful in environments\nwhere the {es} nodes are configured to run with CPU limits, such as cpu\nshares or quota under `Cgroups`.\n\n[source,yaml]\n--------------------------------------------------\nnode.processors: 2\n--------------------------------------------------\n\nThere are a few use-cases for explicitly overriding the `node.processors`\nsetting:\n\n. If you are running multiple instances of {es} on the same host but want\n{es} to size its thread pools as if it only has a fraction of the CPU, you\nshould override the `node.processors` setting to the desired fraction, for\nexample, if you're running two instances of {es} on a 16-core machine, set\n`node.processors` to 8. Note that this is an expert-level use case and there's\na lot more involved than just setting the `node.processors` setting as there are\nother considerations like changing the number of garbage collector threads,\npinning processes to cores, and so on.\n. Sometimes the number of processors is wrongly detected and in such cases\nexplicitly setting the `node.processors` setting will workaround such issues.\n\nIn order to check the number of processors detected, use the nodes info\nAPI with the `os` flag.\n"
}