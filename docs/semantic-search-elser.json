{
    "meta": {
        "timestamp": "2024-11-01T02:49:26.371066",
        "size": 13295,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/semantic-search-elser.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "semantic-search-elser",
        "version": "8.15"
    },
    "doc": "[[semantic-search-elser]]\n=== Tutorial: semantic search with ELSER\n\n++++\n<titleabbrev>Semantic search with ELSER</titleabbrev>\n++++\n\nElastic Learned Sparse EncodeR - or ELSER - is an NLP model trained by Elastic that enables you to perform semantic search by using sparse vector representation.\nInstead of literal matching on search terms, semantic search retrieves results based on the intent and the contextual meaning of a search query.\n\nThe instructions in this tutorial shows you how to use ELSER to perform semantic search on your data.\n\nIMPORTANT: For the easiest way to perform semantic search in the {stack}, refer to the <<semantic-search-semantic-text, `semantic_text`>> end-to-end tutorial.\n\nNOTE: Only the first 512 extracted tokens per field are considered during semantic search with ELSER.\nRefer to {ml-docs}/ml-nlp-limitations.html#ml-nlp-elser-v1-limit-512[this page] for more information.\n\n[discrete]\n[[requirements]]\n==== Requirements\n\nTo perform semantic search by using ELSER, you must have the NLP model deployed in your cluster.\nRefer to the {ml-docs}/ml-nlp-elser.html[ELSER documentation] to learn how to download and deploy the model.\n\nNOTE: The minimum dedicated ML node size for deploying and using the ELSER model is 4 GB in Elasticsearch Service if\n{cloud}/ec-autoscaling.html[deployment autoscaling] is turned off.\nTurning on autoscaling is recommended because it allows your deployment to dynamically adjust resources based on demand.\nBetter performance can be achieved by using more allocations or more threads per allocation, which requires bigger ML nodes.\nAutoscaling provides bigger nodes when required.\nIf autoscaling is turned off, you must provide suitably sized nodes yourself.\n\n[discrete]\n[[elser-mappings]]\n==== Create the index mapping\n\nFirst, the mapping of the destination index - the index that contains the tokens that the model created based on your text - must be created.\nThe destination index must have a field with the <<sparse-vector, `sparse_vector`>> or <<rank-features,`rank_features`>> field type to index the ELSER output.\n\nNOTE: ELSER output must be ingested into a field with the `sparse_vector` or `rank_features` field type.\nOtherwise, {es} interprets the token-weight pairs as a massive amount of fields in a document.\nIf you get an error similar to this: `\"Limit of total fields [1000] has been exceeded while adding new fields\"` then the ELSER output field is not mapped properly and it has a field type different than `sparse_vector` or `rank_features`.\n\n[source,console]\n----\nPUT my-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"content_embedding\": { <1>\n        \"type\": \"sparse_vector\" <2>\n      },\n      \"content\": { <3>\n        \"type\": \"text\" <4>\n      }\n    }\n  }\n}\n----\n// TEST[skip:TBD]\n<1> The name of the field to contain the generated tokens.\nIt must be referenced in the {infer} pipeline configuration in the next step.\n<2> The field to contain the tokens is a `sparse_vector` field.\n<3> The name of the field from which to create the sparse vector representation.\nIn this example, the name of the field is `content`.\nIt must be referenced in the {infer} pipeline configuration in the next step.\n<4> The field type which is text in this example.\n\nTo learn how to optimize space, refer to the <<save-space>> section.\n\n[discrete]\n[[inference-ingest-pipeline]]\n==== Create an ingest pipeline with an inference processor\n\nCreate an <<ingest,ingest pipeline>> with an\n<<inference-processor,{infer} processor>> to use ELSER to infer against the data that is being ingested in the pipeline.\n\n[source,console]\n----\nPUT _ingest/pipeline/elser-v2-test\n{\n  \"processors\": [\n    {\n      \"inference\": {\n        \"model_id\": \".elser_model_2\",\n        \"input_output\": [ <1>\n          {\n            \"input_field\": \"content\",\n            \"output_field\": \"content_embedding\"\n          }\n        ]\n      }\n    }\n  ]\n}\n----\n\n<1> Configuration object that defines the `input_field` for the {infer} process and the `output_field` that will contain the {infer} results.\n\n////\n[source,console]\n----\nDELETE _ingest/pipeline/elser-v2-test\n----\n// TEST[continued]\n////\n\n\n[discrete]\n[[load-data]]\n==== Load data\n\nIn this step, you load the data that you later use in the {infer} ingest pipeline to extract tokens from it.\n\nUse the `msmarco-passagetest2019-top1000` data set, which is a subset of the MS MARCO Passage Ranking data set.\nIt consists of 200 queries, each accompanied by a list of relevant text passages.\nAll unique passages, along with their IDs, have been extracted from that data set and compiled into a\nhttps://github.com/elastic/stack-docs/blob/main/docs/en/stack/ml/nlp/data/msmarco-passagetest2019-unique.tsv[tsv file].\n\nIMPORTANT: The `msmarco-passagetest2019-top1000` dataset was not utilized to train the model.\nWe use this sample dataset in the tutorial because is easily accessible for demonstration purposes.\nYou can use a different data set to test the workflow and become familiar with it.\n\nDownload the file and upload it to your cluster using the {kibana-ref}/connect-to-elasticsearch.html#upload-data-kibana[File Uploader] in the UI.\nAfter your data is analyzed, click **Override settings**.\nUnder **Edit field names**, assign `id` to the first column and `content` to the second.\nClick **Apply**, then **Import**.\nName the index `test-data`, and click **Import**.\nAfter the upload is complete, you will see an index named `test-data` with 182,469 documents.\n\n[discrete]\n[[reindexing-data-elser]]\n==== Ingest the data through the {infer} ingest pipeline\n\nCreate the tokens from the text by reindexing the data throught the {infer}\npipeline that uses ELSER as the inference model.\n\n[source,console]\n----\nPOST _reindex?wait_for_completion=false\n{\n  \"source\": {\n    \"index\": \"test-data\",\n    \"size\": 50 <1>\n  },\n  \"dest\": {\n    \"index\": \"my-index\",\n    \"pipeline\": \"elser-v2-test\"\n  }\n}\n----\n// TEST[skip:TBD]\n<1> The default batch size for reindexing is 1000. Reducing `size` to a smaller number makes the update of the reindexing process quicker which enables you to follow the progress closely and detect errors early.\n\nThe call returns a task ID to monitor the progress:\n\n[source,console]\n----\nGET _tasks/<task_id>\n----\n// TEST[skip:TBD]\n\nYou can also open the Trained Models UI, select the Pipelines tab under ELSER to follow the progress.\n\nReindexing large datasets can take a long time.\nYou can test this workflow using only a subset of the dataset.\nDo this by cancelling the reindexing process, and only generating embeddings for the subset that was reindexed.\nThe following API request will cancel the reindexing task:\n\n[source,console]\n----\nPOST _tasks/<task_id>/_cancel\n----\n// TEST[skip:TBD]\n\n\n[discrete]\n[[text-expansion-query]]\n==== Semantic search by using the `sparse_vector` query\n\nTo perform semantic search, use the <<query-dsl-sparse-vector-query, `sparse_vector` query>>, and provide the query text and the inference ID associated with your ELSER model.\nThe example below uses the query text \"How to avoid muscle soreness after running?\", the `content_embedding` field contains the generated ELSER output:\n\n[source,console]\n----\nGET my-index/_search\n{\n   \"query\":{\n      \"sparse_vector\":{\n         \"field\": \"content_embedding\",\n         \"inference_id\": \"my-elser-endpoint\",\n         \"query\": \"How to avoid muscle soreness after running?\"\n      }\n   }\n}\n----\n// TEST[skip:TBD]\n\nThe result is the top 10 documents that are closest in meaning to your query text from the `my-index` index sorted by their relevancy.\nThe result also contains the extracted tokens for each of the relevant search results with their weights.\nTokens are learned associations capturing relevance, they are not synonyms.\nTo learn more about what tokens are, refer to {ml-docs}/ml-nlp-elser.html#elser-tokens[this page].\nIt is possible to exclude tokens from source, refer to <<save-space,this section>> to learn more.\n\n[source,consol-result]\n----\n\"hits\": {\n  \"total\": {\n    \"value\": 10000,\n    \"relation\": \"gte\"\n  },\n  \"max_score\": 26.199875,\n  \"hits\": [\n    {\n      \"_index\": \"my-index\",\n      \"_id\": \"FPr9HYsBag9jXmT8lEpI\",\n      \"_score\": 26.199875,\n      \"_source\": {\n        \"content_embedding\": {\n          \"muscular\": 0.2821541,\n          \"bleeding\": 0.37929374,\n          \"foods\": 1.1718726,\n          \"delayed\": 1.2112266,\n          \"cure\": 0.6848574,\n          \"during\": 0.5886185,\n          \"fighting\": 0.35022718,\n          \"rid\": 0.2752442,\n          \"soon\": 0.2967024,\n          \"leg\": 0.37649947,\n          \"preparation\": 0.32974035,\n          \"advance\": 0.09652356,\n          (...)\n        },\n        \"id\": 1713868,\n        \"model_id\": \".elser_model_2\",\n        \"content\": \"For example, if you go for a run, you will mostly use the muscles in your lower body. Give yourself 2 days to rest those muscles so they have a chance to heal before you exercise them again. Not giving your muscles enough time to rest can cause muscle damage, rather than muscle development.\"\n      }\n    },\n    (...)\n  ]\n}\n----\n// NOTCONSOLE\n\n\n[discrete]\n[[text-expansion-compound-query]]\n==== Combining semantic search with other queries\n\nYou can combine <<query-dsl-sparse-vector-query, `sparse_vector`>> with other queries in a <<compound-queries,compound query>>.\nFor example, use a filter clause in a <<query-dsl-bool-query>> or a full text query with the same (or different) query text as the `sparse_vector` query.\nThis enables you to combine the search results from both queries.\n\nThe search hits from the `sparse_vector` query tend to score higher than other\n{es} queries.\nThose scores can be regularized by increasing or decreasing the relevance scores of each query by using the `boost` parameter.\nRecall on the `sparse_vector` query can be high where there is a long tail of less relevant results.\nUse the `min_score` parameter to prune those less relevant documents.\n\n[source,console]\n----\nGET my-index/_search\n{\n  \"query\": {\n    \"bool\": { <1>\n      \"should\": [\n        {\n          \"sparse_vector\": {\n            \"field\": \"content_embedding\",\n            \"inference_id\": \"my-elser-endpoint\",\n            \"query\": \"How to avoid muscle soreness after running?\",\n            \"boost\": 1 <2>\n          }\n        },\n        {\n          \"query_string\": {\n            \"query\": \"toxins\",\n            \"boost\": 4 <3>\n          }\n        }\n      ]\n    }\n  },\n  \"min_score\": 10 <4>\n}\n----\n// TEST[skip:TBD]\n<1> Both the `sparse_vector` and the `query_string` queries are in a `should` clause of a `bool` query.\n<2> The `boost` value is `1` for the `sparse_vector` query which is the default value.\nThis means that the relevance score of the results of this query are not boosted.\n<3> The `boost` value is `4` for the `query_string` query.\nThe relevance score of the results of this query is increased causing them to rank higher in the search results.\n<4> Only the results with a score equal to or higher than `10` are displayed.\n\n[discrete]\n[[optimization]]\n=== Optimizing performance\n\n[discrete]\n[[save-space]]\n==== Saving disk space by excluding the ELSER tokens from document source\n\nThe tokens generated by ELSER must be indexed for use in the <<query-dsl-sparse-vector-query, sparse_vector query>>.\nHowever, it is not necessary to retain those terms in the document source.\nYou can save disk space by using the <<include-exclude,source exclude>> mapping to remove the ELSER terms from the document source.\n\nWARNING: Reindex uses the document source to populate the destination index.\n**Once the ELSER terms have been excluded from the source, they cannot be recovered through reindexing.**\nExcluding the tokens from the source is a space-saving optimization that should only be applied if you are certain that reindexing will not be required in the future!\nIt's important to carefully consider this trade-off and make sure that excluding the ELSER terms from the source aligns with your specific requirements and use case.\nReview the\n<<disable-source-field>> and <<include-exclude>> sections carefully to learn more about the possible consequences of excluding the tokens from the `_source`.\n\nThe mapping that excludes `content_embedding` from the  `_source` field can be created by the following API call:\n\n[source,console]\n----\nPUT my-index\n{\n  \"mappings\": {\n    \"_source\": {\n      \"excludes\": [\n        \"content_embedding\"\n      ]\n    },\n    \"properties\": {\n      \"content_embedding\": {\n        \"type\": \"sparse_vector\"\n      },\n      \"content\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\n----\n// TEST[skip:TBD]\n\n[NOTE]\n====\nDepending on your data, the `sparse_vector` query may be faster with `track_total_hits: false`.\n====\n\n[discrete]\n[[further-reading]]\n==== Further reading\n\n* {ml-docs}/ml-nlp-elser.html[How to download and deploy ELSER]\n* {ml-docs}/ml-nlp-limitations.html#ml-nlp-elser-v1-limit-512[ELSER limitation]\n* https://www.elastic.co/blog/may-2023-launch-information-retrieval-elasticsearch-ai-model[Improving information retrieval in the Elastic Stack: Introducing Elastic Learned Sparse Encoder, our new retrieval model]\n\n[discrete]\n[[interactive-example]]\n==== Interactive example\n\n* The `elasticsearch-labs` repo has an interactive example of running https://github.com/elastic/elasticsearch-labs/blob/main/notebooks/search/03-ELSER.ipynb[ELSER-powered semantic search] using the {es} Python client.\n"
}