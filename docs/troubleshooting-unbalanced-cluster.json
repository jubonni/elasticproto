{
    "meta": {
        "size": 3983,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/troubleshooting-unbalanced-cluster.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "troubleshooting-unbalanced-cluster",
        "version": "8.15"
    },
    "doc": "[[troubleshooting-unbalanced-cluster]]\n== Troubleshooting an unbalanced cluster\n\nElasticsearch balances shards across data tiers to achieve a good compromise between:\n\n* shard count\n* disk usage\n* write load (for indices in data streams)\n\nElasticsearch does not take into account the amount or complexity of search queries when rebalancing shards.\nThis is indirectly achieved by balancing shard count and disk usage.\n\nThere is no guarantee that individual components will be evenly spread across the nodes.\nThis could happen if some nodes have fewer shards, or are using less disk space,\nbut are assigned shards with higher write loads.\n\nUse the <<cat-allocation,cat allocation command>> to list workloads per node:\n\n[source,console]\n--------------------------------------------------\nGET /_cat/allocation?v\n--------------------------------------------------\n// TEST[s/^/PUT test\\n{\"settings\": {\"number_of_replicas\": 0}}\\n/]\n\nThe API returns the following response:\n\n[source,text]\n--------------------------------------------------\nshards shards.undesired write_load.forecast disk.indices.forecast disk.indices disk.used disk.avail disk.total disk.percent host      ip        node    node.role\n     1                0                 0.0                  260b         260b    47.3gb     43.4gb    100.7gb           46 127.0.0.1 127.0.0.1 CSUXak2 himrst\n--------------------------------------------------\n// TESTRESPONSE[s/\\d+(\\.\\d+)?[tgmk]?b/\\\\d+(\\\\.\\\\d+)?[tgmk]?b/ s/46/\\\\d+/]\n// TESTRESPONSE[s/CSUXak2 himrst/.+/ non_json]\n\nThis response contains the following information that influences balancing:\n\n* `shards` is the current number of shards allocated to the node\n* `shards.undesired` is the number of shards that needs to be moved to other nodes to finish balancing\n* `disk.indices.forecast` is the expected disk usage according to projected shard growth\n* `write_load.forecast` is the projected total write load associated with this node\n\nA cluster is considered balanced when all shards are in their desired locations,\nwhich means that no further shard movements are planned (all `shards.undesired` values are equal to 0).\n\nSome operations such as node restarting, decommissioning, or changing cluster allocation settings\nare disruptive and might require multiple shards to move in order to rebalance the cluster.\n\nShard movement order is not deterministic and mostly determined by the source and target node readiness to move a shard.\nWhile rebalancing is in progress some nodes might appear busier than others.\n\nWhen a shard is allocated to an undesired node it uses the resources of the current node instead of the target.\nThis might cause a hotspot (disk or CPU) when multiple shards reside on the current node that have not been\nmoved to their corresponding targets yet.\n\nIf a cluster takes a long time to finish rebalancing you might find the following log entries:\n[source,text]\n--------------------------------------------------\n[WARN][o.e.c.r.a.a.DesiredBalanceReconciler] [10%] of assigned shards (10/100) are not on their desired nodes, which exceeds the warn threshold of [10%]\n--------------------------------------------------\nThis is not concerning as long as the number of such shards is decreasing and this warning appears occasionally,\nfor example after rolling restarts or changing allocation settings.\n\nIf the cluster has this warning repeatedly for an extended period of time (multiple hours),\nit is possible that the desired balance is diverging too far from the current state.\n\nIf so, increase the <<shards-rebalancing-heuristics,`cluster.routing.allocation.balance.threshold`>>\nto reduce the sensitivity of the algorithm that tries to level up the shard count and disk usage within the cluster.\n\nAnd reset the desired balance using the following API call:\n\n[source,console,id=delete-desired-balance-request-example]\n--------------------------------------------------\nDELETE /_internal/desired_balance\n--------------------------------------------------\n"
}