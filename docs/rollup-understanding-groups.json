{
    "meta": {
        "timestamp": "2024-11-01T03:07:10.482271",
        "size": 8383,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/rollup-understanding-groups.html",
        "type": "documentation",
        "role": [
            "xpack"
        ],
        "has_code": true,
        "title": "rollup-understanding-groups",
        "version": "8.15"
    },
    "doc": "[role=\"xpack\"]\n[[rollup-understanding-groups]]\n=== Understanding groups\n\ndeprecated::[8.11.0,\"Rollups will be removed in a future version. Please <<rollup-migrating-to-downsampling,migrate>> to <<downsampling,downsampling>> instead.\"]\n\nTo preserve flexibility, Rollup Jobs are defined based on how future queries may need to use the data. Traditionally, systems force\nthe admin to make decisions about what metrics to rollup and on what interval. E.g. The average of `cpu_time` on an hourly basis. This\nis limiting; if, in the future, the admin wishes to see the average of `cpu_time` on an hourly basis _and_ partitioned by `host_name`,\nthey are out of luck.\n\nOf course, the admin can decide to rollup the `[hour, host]` tuple on an hourly basis, but as the number of grouping keys grows, so do the\nnumber of tuples the admin needs to configure. Furthermore, these `[hours, host]` tuples are only useful for hourly rollups... daily, weekly,\nor monthly rollups all require new configurations.\n\nRather than force the admin to decide ahead of time which individual tuples should be rolled up, Elasticsearch's Rollup jobs are configured\nbased on which groups are potentially useful to future queries. For example, this configuration:\n\n[source,js]\n--------------------------------------------------\n\"groups\" : {\n  \"date_histogram\": {\n    \"field\": \"timestamp\",\n    \"fixed_interval\": \"1h\",\n    \"delay\": \"7d\"\n  },\n  \"terms\": {\n    \"fields\": [\"hostname\", \"datacenter\"]\n  },\n  \"histogram\": {\n    \"fields\": [\"load\", \"net_in\", \"net_out\"],\n    \"interval\": 5\n  }\n}\n--------------------------------------------------\n// NOTCONSOLE\n\nAllows `date_histogram` to be used on the `\"timestamp\"` field, `terms` aggregations to be used on the `\"hostname\"` and `\"datacenter\"`\nfields, and `histograms` to be used on any of `\"load\"`, `\"net_in\"`, `\"net_out\"` fields.\n\nImportantly, these aggs/fields can be used in any combination. This aggregation:\n\n[source,js]\n--------------------------------------------------\n\"aggs\" : {\n  \"hourly\": {\n    \"date_histogram\": {\n      \"field\": \"timestamp\",\n      \"fixed_interval\": \"1h\"\n    },\n    \"aggs\": {\n      \"host_names\": {\n        \"terms\": {\n          \"field\": \"hostname\"\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n// NOTCONSOLE\n\nis just as valid as this aggregation:\n\n[source,js]\n--------------------------------------------------\n\"aggs\" : {\n  \"hourly\": {\n    \"date_histogram\": {\n      \"field\": \"timestamp\",\n      \"fixed_interval\": \"1h\"\n    },\n    \"aggs\": {\n      \"data_center\": {\n        \"terms\": {\n          \"field\": \"datacenter\"\n        }\n      },\n      \"aggs\": {\n        \"host_names\": {\n          \"terms\": {\n            \"field\": \"hostname\"\n          }\n        },\n        \"aggs\": {\n          \"load_values\": {\n            \"histogram\": {\n              \"field\": \"load\",\n              \"interval\": 5\n            }\n          }\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n// NOTCONSOLE\n\n\nYou'll notice that the second aggregation is not only substantially larger, it also swapped the position of the terms aggregation on\n`\"hostname\"`, illustrating how the order of aggregations does not matter to rollups. Similarly, while the `date_histogram` is required\nfor rolling up data, it isn't required while querying (although often used). For example, this is a valid aggregation for\nRollup Search to execute:\n\n\n[source,js]\n--------------------------------------------------\n\"aggs\" : {\n  \"host_names\": {\n    \"terms\": {\n      \"field\": \"hostname\"\n    }\n  }\n}\n--------------------------------------------------\n// NOTCONSOLE\n\nUltimately, when configuring `groups` for a job, think in terms of how you might wish to partition data in a query at a future date...\nthen include those in the config. Because Rollup Search allows any order or combination of the grouped fields, you just need to decide\nif a field is useful for aggregating later, and how you might wish to use it (terms, histogram, etc).\n\n[[rollup-understanding-group-intervals]]\n==== Calendar vs fixed time intervals\n\nEach rollup-job must have a date histogram group with a defined interval. {es}\nunderstands both\n<<calendar_and_fixed_intervals,calendar and fixed time intervals>>. Fixed time\nintervals are fairly easy to understand; `60s` means sixty seconds. But what\ndoes `1M` mean? One month of time depends on which month we are talking about,\nsome months are longer or shorter than others. This is an example of calendar\ntime and the duration of that unit depends on context. Calendar units are also\naffected by leap-seconds, leap-years, etc.\n\nThis is important because the buckets generated by rollup are in either calendar\nor fixed intervals and this limits how you can query them later. See\n<<rollup-search-limitations-intervals>>.\n\nWe recommend sticking with fixed time intervals, since they are easier to\nunderstand and are more flexible at query time. It will introduce some drift in\nyour data during leap-events and you will have to think about months in a fixed\nquantity (30 days) instead of the actual calendar length. However, it is often\neasier than dealing with calendar units at query time.\n\nMultiples of units are always \"fixed\". For example, `2h` is always the fixed\nquantity `7200` seconds. Single units can be fixed or calendar depending on the\nunit:\n\n[options=\"header\"]\n|=======\n|Unit |Calendar |Fixed\n|millisecond |NA |`1ms`, `10ms`, etc\n|second |NA |`1s`, `10s`, etc\n|minute |`1m` |`2m`, `10m`, etc\n|hour |`1h` |`2h`, `10h`, etc\n|day |`1d` |`2d`, `10d`, etc\n|week |`1w` |NA\n|month |`1M` |NA\n|quarter |`1q` |NA\n|year |`1y` |NA\n|=======\n\nFor some units where there are both fixed and calendar, you may need to express\nthe quantity in terms of the next smaller unit. For example, if you want a fixed\nday (not a calendar day), you should specify `24h` instead of `1d`. Similarly,\nif you want fixed hours, specify `60m` instead of `1h`. This is because the\nsingle quantity entails calendar time, and limits you to querying by calendar\ntime in the future.\n\n==== Grouping limitations with heterogeneous indices\n\nThere was previously a limitation in how Rollup could handle indices that had heterogeneous mappings (multiple, unrelated/non-overlapping\nmappings). The recommendation at the time was to configure a separate job per data \"type\". For example, you might configure a separate\njob for each Beats module that you had enabled (one for `process`, another for `filesystem`, etc).\n\nThis recommendation was driven by internal implementation details that caused document counts to be potentially incorrect if a single \"merged\"\njob was used.\n\nThis limitation has since been alleviated. As of 6.4.0, it is now considered best practice to combine all rollup configurations\ninto a single job.\n\nAs an example, if your index has two types of documents:\n\n[source,js]\n--------------------------------------------------\n{\n  \"timestamp\": 1516729294000,\n  \"temperature\": 200,\n  \"voltage\": 5.2,\n  \"node\": \"a\"\n}\n--------------------------------------------------\n// NOTCONSOLE\n\nand\n\n[source,js]\n--------------------------------------------------\n{\n  \"timestamp\": 1516729294000,\n  \"price\": 123,\n  \"title\": \"Foo\"\n}\n--------------------------------------------------\n// NOTCONSOLE\n\nthe best practice is to combine them into a single rollup job which covers both of these document types, like this:\n\n[source,js]\n--------------------------------------------------\nPUT _rollup/job/combined\n{\n  \"index_pattern\": \"data-*\",\n  \"rollup_index\": \"data_rollup\",\n  \"cron\": \"*/30 * * * * ?\",\n  \"page_size\": 1000,\n  \"groups\": {\n    \"date_histogram\": {\n      \"field\": \"timestamp\",\n      \"fixed_interval\": \"1h\",\n      \"delay\": \"7d\"\n    },\n    \"terms\": {\n      \"fields\": [ \"node\", \"title\" ]\n    }\n  },\n  \"metrics\": [\n    {\n      \"field\": \"temperature\",\n      \"metrics\": [ \"min\", \"max\", \"sum\" ]\n    },\n    {\n      \"field\": \"price\",\n      \"metrics\": [ \"avg\" ]\n    }\n  ]\n}\n--------------------------------------------------\n// NOTCONSOLE\n\n==== Doc counts and overlapping jobs\n\nThere was previously an issue with document counts on \"overlapping\" job configurations, driven by the same internal implementation detail.\nIf there were two Rollup jobs saving to the same index, where one job is a \"subset\" of another job, it was possible that document counts\ncould be incorrect for certain aggregation arrangements.\n\nThis issue has also since been eliminated in 6.4.0.\n"
}