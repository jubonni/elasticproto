{
    "meta": {
        "timestamp": "2024-11-01T03:07:08.758287",
        "size": 2368,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer-anatomy.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "analyzer-anatomy",
        "version": "8.15"
    },
    "doc": "[[analyzer-anatomy]]\n=== Anatomy of an analyzer\n\nAn _analyzer_  -- whether built-in or custom -- is just a package which\ncontains three lower-level building blocks: _character filters_,\n_tokenizers_, and _token filters_.\n\nThe built-in <<analysis-analyzers,analyzers>> pre-package these building\nblocks into analyzers suitable for different languages and types of text.\nElasticsearch also exposes the individual building blocks so that they can be\ncombined to define new <<analysis-custom-analyzer,`custom`>> analyzers.\n\n[[analyzer-anatomy-character-filters]]\n==== Character filters\n\nA _character filter_ receives the original text as a stream of characters and\ncan transform the stream by adding, removing, or changing characters. For\ninstance, a character filter could be used to convert Hindu-Arabic numerals\n(\u0660\u200e\u0661\u0662\u0663\u0664\u0665\u0666\u0667\u0668\u200e\u0669\u200e) into their Arabic-Latin equivalents (0123456789), or to strip HTML\nelements like `<b>` from the stream.\n\nAn analyzer may have *zero or more* <<analysis-charfilters,character filters>>,\nwhich are applied in order.\n\n[[analyzer-anatomy-tokenizer]]\n==== Tokenizer\n\nA _tokenizer_ receives a stream of characters, breaks it up into individual\n_tokens_ (usually individual words), and outputs a stream of _tokens_. For\ninstance, a <<analysis-whitespace-tokenizer,`whitespace`>> tokenizer breaks\ntext into tokens whenever it sees any whitespace. It would convert the text\n`\"Quick brown fox!\"` into the terms `[Quick, brown, fox!]`.\n\nThe tokenizer is also responsible for recording the order or _position_ of\neach term and the start and end _character offsets_ of the original word which\nthe term represents.\n\nAn analyzer must have *exactly one* <<analysis-tokenizers,tokenizer>>.\n\n[[analyzer-anatomy-token-filters]]\n==== Token filters\n\nA _token filter_ receives the token stream and may add, remove, or change\ntokens. For example, a <<analysis-lowercase-tokenfilter,`lowercase`>> token\nfilter converts all tokens to lowercase, a\n<<analysis-stop-tokenfilter,`stop`>> token filter removes common words\n(_stop words_) like `the` from the token stream, and a\n<<analysis-synonym-tokenfilter,`synonym`>> token filter introduces synonyms\ninto the token stream.\n\nToken filters are not allowed to change the position or character offsets of\neach token.\n\nAn analyzer may have *zero or more* <<analysis-tokenfilters,token filters>>,\nwhich are applied in order."
}