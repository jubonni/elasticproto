{
    "meta": {
        "timestamp": "2024-11-01T03:02:52.493578",
        "size": 11072,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/esql-process-data-with-dissect-and-grok.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "esql-process-data-with-dissect-and-grok",
        "version": "8.15"
    },
    "doc": "[[esql-process-data-with-dissect-and-grok]]\n=== Data processing with DISSECT and GROK\n\n++++\n<titleabbrev>Data processing with DISSECT and GROK</titleabbrev>\n++++\n\nYour data may contain unstructured strings that you want to structure. This\nmakes it easier to analyze the data. For example, log messages may contain IP\naddresses that you want to extract so you can find the most active IP addresses.\n\nimage::images/esql/unstructured-data.png[align=\"center\",width=75%]\n\n{es} can structure your data at index time or query time. At index time, you can\nuse the <<dissect-processor,Dissect>> and <<grok-processor,Grok>> ingest\nprocessors, or the {ls} {logstash-ref}/plugins-filters-dissect.html[Dissect] and\n{logstash-ref}/plugins-filters-grok.html[Grok] filters. At query time, you can\nuse the {esql} <<esql-dissect>> and <<esql-grok>> commands.\n\n[[esql-grok-or-dissect]]\n==== `DISSECT` or `GROK`? Or both?\n\n`DISSECT` works by breaking up a string using a delimiter-based pattern. `GROK`\nworks similarly, but uses regular expressions. This makes `GROK` more powerful,\nbut generally also slower. `DISSECT` works well when data is reliably repeated.\n`GROK` is a better choice when you really need the power of regular expressions,\nfor example when the structure of your text varies from row to row.\n\nYou can use both `DISSECT` and `GROK` for hybrid use cases. For example when a\nsection of the line is reliably repeated, but the entire line is not. `DISSECT`\ncan deconstruct the section of the line that is repeated. `GROK` can process the\nremaining field values using regular expressions.\n\n[[esql-process-data-with-dissect]]\n==== Process data with `DISSECT`\n\nThe <<esql-dissect>> processing command matches a string against a\ndelimiter-based pattern, and extracts the specified keys as columns.\n\nFor example, the following pattern:\n[source,txt]\n----\n%{clientip} [%{@timestamp}] %{status}\n----\n\nmatches a log line of this format:\n[source,txt]\n----\n1.2.3.4 [2023-01-23T12:15:00.000Z] Connected\n----\n\nand results in adding the following columns to the input table:\n\n[%header.monospaced.styled,format=dsv,separator=|]\n|===\nclientip:keyword | @timestamp:keyword | status:keyword\n1.2.3.4 | 2023-01-23T12:15:00.000Z | Connected\n|===\n\n[[esql-dissect-patterns]]\n===== Dissect patterns\n\ninclude::../ingest/processors/dissect.asciidoc[tag=intro-example-explanation]\n\nAn empty key (`%{}`) or <<esql-named-skip-key,named skip key>> can be used to\nmatch values, but exclude the value from the output.\n\nAll matched values are output as keyword string data types. Use the\n<<esql-type-conversion-functions>> to convert to another data type.\n\nDissect also supports <<esql-dissect-key-modifiers,key modifiers>> that can\nchange dissect's default behavior. For example, you can instruct dissect to\nignore certain fields, append fields, skip over padding, etc.\n\n[[esql-dissect-terminology]]\n===== Terminology\n\ndissect pattern::\nthe set of fields and delimiters describing the textual\nformat. Also known as a dissection.\nThe dissection is described using a set of `%{}` sections:\n`%{a} - %{b} - %{c}`\n\nfield::\nthe text from `%{` to `}` inclusive.\n\ndelimiter::\nthe text between `}` and the next `%{` characters.\nAny set of characters other than `%{`, `'not }'`, or `}` is a delimiter.\n\nkey::\n+\n--\nthe text between the `%{` and `}`, exclusive of the `?`, `+`, `&` prefixes\nand the ordinal suffix.\n\nExamples:\n\n* `%{?aaa}` - the key is `aaa`\n* `%{+bbb/3}` - the key is `bbb`\n* `%{&ccc}` - the key is `ccc`\n--\n\n[[esql-dissect-examples]]\n===== Examples\n\ninclude::processing-commands/dissect.asciidoc[tag=examples]\n\n[[esql-dissect-key-modifiers]]\n===== Dissect key modifiers\n\ninclude::../ingest/processors/dissect.asciidoc[tag=dissect-key-modifiers]\n\n[[esql-dissect-key-modifiers-table]]\n.Dissect key modifiers\n[options=\"header\",role=\"styled\"]\n|======\n| Modifier      | Name               | Position       | Example                      | Description                                                  | Details\n| `->`          | Skip right padding | (far) right    | `%{keyname1->}`  | Skips any repeated characters to the right                   | <<esql-dissect-modifier-skip-right-padding,link>>\n| `+`           | Append             | left           | `%{+keyname} %{+keyname}`    | Appends two or more fields together                          | <<esql-append-modifier,link>>\n| `+` with `/n` | Append with order  | left and right | `%{+keyname/2} %{+keyname/1}` | Appends two or more fields together in the order specified   | <<esql-append-order-modifier,link>>\n| `?`           | Named skip key     | left           | `%{?ignoreme}`  | Skips the matched value in the output. Same behavior as `%{}`| <<esql-named-skip-key,link>>\n|======\n\n[[esql-dissect-modifier-skip-right-padding]]\n====== Right padding modifier (`->`)\ninclude::../ingest/processors/dissect.asciidoc[tag=dissect-modifier-skip-right-padding]\n\nFor example:\n[source.merge.styled,esql]\n----\ninclude::{esql-specs}/docs.csv-spec[tag=dissectRightPaddingModifier]\n----\n[%header.monospaced.styled,format=dsv,separator=|]\n|===\ninclude::{esql-specs}/docs.csv-spec[tag=dissectRightPaddingModifier-result]\n|===\n\ninclude::../ingest/processors/dissect.asciidoc[tag=dissect-modifier-empty-right-padding]\n\nFor example:\n[source.merge.styled,esql]\n----\ninclude::{esql-specs}/docs.csv-spec[tag=dissectEmptyRightPaddingModifier]\n----\n[%header.monospaced.styled,format=dsv,separator=|]\n|===\ninclude::{esql-specs}/docs.csv-spec[tag=dissectEmptyRightPaddingModifier-result]\n|===\n\n[[esql-append-modifier]]\n====== Append modifier (`+`)\ninclude::../ingest/processors/dissect.asciidoc[tag=append-modifier]\n\n[source.merge.styled,esql]\n----\ninclude::{esql-specs}/docs.csv-spec[tag=dissectAppendModifier]\n----\n[%header.monospaced.styled,format=dsv,separator=|]\n|===\ninclude::{esql-specs}/docs.csv-spec[tag=dissectAppendModifier-result]\n|===\n\n[[esql-append-order-modifier]]\n====== Append with order modifier (`+` and `/n`)\ninclude::../ingest/processors/dissect.asciidoc[tag=append-order-modifier]\n\n[source.merge.styled,esql]\n----\ninclude::{esql-specs}/docs.csv-spec[tag=dissectAppendWithOrderModifier]\n----\n[%header.monospaced.styled,format=dsv,separator=|]\n|===\ninclude::{esql-specs}/docs.csv-spec[tag=dissectAppendWithOrderModifier-result]\n|===\n\n[[esql-named-skip-key]]\n====== Named skip key (`?`)\ninclude::../ingest/processors/dissect.asciidoc[tag=named-skip-key]\nThis can be done with a named skip key using the `{?name}` syntax. In the\nfollowing query, `ident` and `auth` are not added to the output table:\n\n[source.merge.styled,esql]\n----\ninclude::{esql-specs}/docs.csv-spec[tag=dissectNamedSkipKey]\n----\n[%header.monospaced.styled,format=dsv,separator=|]\n|===\ninclude::{esql-specs}/docs.csv-spec[tag=dissectNamedSkipKey-result]\n|===\n\n[[esql-dissect-limitations]]\n===== Limitations\n\n// tag::dissect-limitations[]\nThe `DISSECT` command does not support reference keys.\n// end::dissect-limitations[]\n\n[[esql-process-data-with-grok]]\n==== Process data with `GROK`\n\nThe <<esql-grok>> processing command matches a string against a pattern based on\nregular expressions, and extracts the specified keys as columns.\n\nFor example, the following pattern:\n[source,txt]\n----\n%{IP:ip} \\[%{TIMESTAMP_ISO8601:@timestamp}\\] %{GREEDYDATA:status}\n----\n\nmatches a log line of this format:\n[source,txt]\n----\n1.2.3.4 [2023-01-23T12:15:00.000Z] Connected\n----\n\nPutting it together as an {esql} query:\n\n[source.merge.styled,esql]\n----\ninclude::{esql-specs}/docs.csv-spec[tag=grokWithEscapeTripleQuotes]\n----\n\n`GROK` adds the following columns to the input table:\n\n[%header.monospaced.styled,format=dsv,separator=|]\n|===\n@timestamp:keyword | ip:keyword | status:keyword\n2023-01-23T12:15:00.000Z | 1.2.3.4 | Connected\n|===\n\n[NOTE]\n====\n\nSpecial regex characters in grok patterns, like `[` and `]` need to be escaped\nwith a `\\`. For example, in the earlier pattern:\n[source,txt]\n----\n%{IP:ip} \\[%{TIMESTAMP_ISO8601:@timestamp}\\] %{GREEDYDATA:status}\n----\n\nIn {esql} queries, when using single quotes for strings, the backslash character itself is a special character that\nneeds to be escaped with another `\\`. For this example, the corresponding {esql}\nquery becomes:\n[source.merge.styled,esql]\n----\ninclude::{esql-specs}/docs.csv-spec[tag=grokWithEscape]\n----\n\nFor this reason, in general it is more convenient to use triple quotes `\"\"\"` for GROK patterns,\nthat do not require escaping for backslash.\n\n[source.merge.styled,esql]\n----\ninclude::{esql-specs}/docs.csv-spec[tag=grokWithEscapeTripleQuotes]\n----\n====\n\n\n[[esql-grok-patterns]]\n===== Grok patterns\n\nThe syntax for a grok pattern is `%{SYNTAX:SEMANTIC}`\n\nThe `SYNTAX` is the name of the pattern that matches your text. For example,\n`3.44` is matched by the `NUMBER` pattern and `55.3.244.1` is matched by the\n`IP` pattern. The syntax is how you match.\n\nThe `SEMANTIC` is the identifier you give to the piece of text being matched.\nFor example, `3.44` could be the duration of an event, so you could call it\nsimply `duration`. Further, a string `55.3.244.1` might identify the `client`\nmaking a request.\n\nBy default, matched values are output as keyword string data types. To convert a\nsemantic's data type, suffix it with the target data type. For example\n`%{NUMBER:num:int}`, which converts the `num` semantic from a string to an\ninteger. Currently the only supported conversions are `int` and `float`. For\nother types, use the <<esql-type-conversion-functions>>.\n\nFor an overview of the available patterns, refer to\n{es-repo}/blob/{branch}/libs/grok/src/main/resources/patterns[GitHub]. You can\nalso retrieve a list of all patterns using a <<grok-processor-rest-get,REST\nAPI>>.\n\n[[esql-grok-regex]]\n===== Regular expressions\n\nGrok is based on regular expressions. Any regular expressions are valid in grok\nas well. Grok uses the Oniguruma regular expression library. Refer to\nhttps://github.com/kkos/oniguruma/blob/master/doc/RE[the Oniguruma GitHub\nrepository] for the full supported regexp syntax.\n\n[[esql-custom-patterns]]\n===== Custom patterns\n\nIf grok doesn't have a pattern you need, you can use the Oniguruma syntax for\nnamed capture which lets you match a piece of text and save it as a column:\n[source,txt]\n----\n(?<field_name>the pattern here)\n----\n\nFor example, postfix logs have a `queue id` that is a 10 or 11-character\nhexadecimal value. This can be captured to a column named `queue_id` with:\n[source,txt]\n----\n(?<queue_id>[0-9A-F]{10,11})\n----\n\n[[esql-grok-examples]]\n===== Examples\n\ninclude::processing-commands/grok.asciidoc[tag=examples]\n\n[[esql-grok-debugger]]\n===== Grok debugger\n\nTo write and debug grok patterns, you can use the\n{kibana-ref}/xpack-grokdebugger.html[Grok Debugger]. It provides a UI for\ntesting patterns against sample data. Under the covers, it uses the same engine\nas the `GROK` command.\n\n[[esql-grok-limitations]]\n===== Limitations\n\n// tag::grok-limitations[]\nThe `GROK` command does not support configuring <<custom-patterns,custom\npatterns>>, or <<trace-match,multiple patterns>>. The `GROK` command is not\nsubject to <<grok-watchdog,Grok watchdog settings>>.\n// end::grok-limitations[]\n"
}