{
    "meta": {
        "timestamp": "2024-11-01T03:02:52.147580",
        "size": 6583,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/transform-checkpoints.html",
        "type": "documentation",
        "role": [
            "xpack"
        ],
        "has_code": false,
        "title": "transform-checkpoints",
        "version": "8.15"
    },
    "doc": "[role=\"xpack\"]\n[[transform-checkpoints]]\n= How {transform} checkpoints work\n++++\n<titleabbrev>How checkpoints work</titleabbrev>\n++++\n\nEach time a {transform} examines the source indices and creates or updates the \ndestination index, it generates a _checkpoint_.\n\nIf your {transform} runs only once, there is logically only one checkpoint. If \nyour {transform} runs continuously, however, it creates checkpoints as it \ningests and transforms new source data. The `sync` property of the {transform} \nconfigures checkpointing by specifying a time field.\n\nTo create a checkpoint, the {ctransform}:\n\n. Checks for changes to source indices.\n+\nUsing a simple periodic timer, the {transform} checks for changes to the source \nindices. This check is done based on the interval defined in the transform's \n`frequency` property.\n+\nIf the source indices remain unchanged or if a checkpoint is already in progress\nthen it waits for the next timer.\n+\nIf changes are found a checkpoint is created.\n\n. Identifies which entities and/or time buckets have changed.\n+\nThe {transform} searches to see which entities or time buckets have changed \nbetween the last and the new checkpoint. The {transform} uses the values to\nsynchronize the source and destination indices with fewer operations than a\nfull re-run.\n \n. Updates the destination index (the {dataframe}) with the changes.\n+\n--\nThe {transform} applies changes related to either new or changed entities or\ntime buckets to the destination index. The set of changes can be paginated. The\n{transform} performs a composite aggregation similarly to the batch {transform} \noperation, however it also injects query filters based on the previous step to \nreduce the amount of work. After all changes have been applied, the checkpoint \nis complete.\n--\n\nThis checkpoint process involves both search and indexing activity on the\ncluster. We have attempted to favor control over performance while developing\n{transforms}. We decided it was preferable for the {transform} to take longer to \ncomplete, rather than to finish quickly and take precedence in resource \nconsumption. That being said, the cluster still requires enough resources to \nsupport both the composite aggregation search and the indexing of its results. \n\nTIP: If the cluster experiences unsuitable performance degradation due to the\n{transform}, stop the {transform} and refer to <<transform-performance>>.\n\n\n[discrete]\n[[sync-field-ingest-timestamp]]\n== Using the ingest timestamp for syncing the {transform}\n\n\n\nIn most cases, it is strongly recommended to use the ingest timestamp of the \nsource indices for syncing the {transform}. This is the most optimal way for \n{transforms} to be able to identify new changes. If your data source follows the \n{ecs-ref}/ecs-reference.html[ECS standard], you might already have an \n{ecs-ref}/ecs-event.html#field-event-ingested[`event.ingested`] field. In this \ncase, use `event.ingested` as the `sync`.`time`.`field` property of your \n{transform}.\n\nIf you don't have a `event.ingested` field or it isn't populated, you can set it \nby using an ingest pipeline. Create an ingest pipeline either using the \n<<put-pipeline-api, ingest pipeline API>> (like the example below) or via {kib} \nunder **Stack Management > Ingest Pipelines**. Use a \n<<set-processor,`set` processor>> to set the field and associate it with the \nvalue of the ingest timestamp.\n\n[source,console]\n----------------------------------\nPUT _ingest/pipeline/set_ingest_time\n{\n  \"description\": \"Set ingest timestamp.\",\n  \"processors\": [\n    {\n      \"set\": {\n        \"field\": \"event.ingested\",\n        \"value\": \"{{{_ingest.timestamp}}}\"\n      }\n    }\n  ]\n}\n----------------------------------\n\nAfter you created the ingest pipeline, apply it to the source indices of your \n{transform}. The pipeline adds the field `event.ingested` to every document with \nthe value of the ingest timestamp. Configure the `sync`.`time`.`field` property \nof your {transform} to use the field by using the \n<<put-transform,create {transform} API>> for new {transforms} or the \n<<update-transform, update {transform} API>> for existing {transforms}. The \n`event.ingested` field is used for syncing the {transform}. \n\nRefer to <<add-pipeline-to-indexing-request>> and <<ingest>> to learn more about \nhow to use an ingest pipeline.\n\n\n[discrete]\n[[ml-transform-checkpoint-heuristics]]\n== Change detection heuristics\n\nWhen the {transform} runs in continuous mode, it updates the documents in the\ndestination index as new data comes in. The {transform} uses a set of heuristics\ncalled change detection to update the destination index with fewer operations.\n\nIn this example, the data is grouped by host names. Change detection detects \nwhich host names have changed,  for example, host `A`, `C` and `G` and only \nupdates documents with those hosts but does not update documents that store \ninformation about host `B`, `D`, or any other host that are not changed.\n\nAnother heuristic can be applied for time buckets when a `date_histogram` is \nused to group by time buckets. Change detection detects which time buckets have \nchanged and only update those.\n\n\n[discrete]\n[[ml-transform-checkpoint-errors]]\n== Error handling\n\nFailures in {transforms} tend to be related to searching or indexing.\nTo increase the resiliency of {transforms}, the cursor positions of\nthe aggregated search and the changed entities search are tracked in memory and\npersisted periodically.\n\nCheckpoint failures can be categorized as follows:\n\n* Temporary failures: The checkpoint is retried. If 10 consecutive failures\noccur, the {transform} has a failed status. For example, this situation might \noccur when there are shard failures and queries return only partial results.\n* Irrecoverable failures: The {transform} immediately fails. For example, this \nsituation occurs when the source index is not found.\n* Adjustment failures: The {transform} retries with adjusted settings. For \nexample, if a parent circuit breaker memory errors occur during the composite \naggregation, the {transform} receives partial results. The aggregated search is \nretried with a smaller number of buckets. This retry is performed at the \ninterval defined in the `frequency` property for the {transform}. If the search \nis retried to the point where it reaches a minimal number of buckets, an \nirrecoverable failure occurs.\n\nIf the node running the {transforms} fails, the {transform} restarts from the \nmost recent persisted cursor position. This recovery process might repeat some \nof the work the {transform} had already done, but it ensures data consistency.\n"
}