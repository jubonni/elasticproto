{
    "meta": {
        "timestamp": "2024-11-01T03:02:53.742579",
        "size": 5619,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenizers.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "analysis-tokenizers",
        "version": "8.15"
    },
    "doc": "[[analysis-tokenizers]]\n== Tokenizer reference\n\nA _tokenizer_ receives a stream of characters, breaks it up into individual\n_tokens_ (usually individual words), and outputs a stream of _tokens_. For\ninstance, a <<analysis-whitespace-tokenizer,`whitespace`>> tokenizer breaks\ntext into tokens whenever it sees any whitespace. It would convert the text\n`\"Quick brown fox!\"` into the terms `[Quick, brown, fox!]`.\n\nThe tokenizer is also responsible for recording the following:\n\n* Order or _position_ of each term (used for phrase and word proximity queries)\n* Start and end _character offsets_ of the original word which the term\nrepresents (used for highlighting search snippets).\n* _Token type_, a classification of each term produced, such as `<ALPHANUM>`,\n`<HANGUL>`, or `<NUM>`. Simpler analyzers only produce the `word` token type.\n\nElasticsearch has a number of built in tokenizers which can be used to build\n<<analysis-custom-analyzer,custom analyzers>>.\n\n[discrete]\n=== Word Oriented Tokenizers\n\nThe following tokenizers are usually used for tokenizing full text into\nindividual words:\n\n<<analysis-standard-tokenizer,Standard Tokenizer>>::\n\nThe `standard` tokenizer divides text into terms on word boundaries, as\ndefined by the Unicode Text Segmentation algorithm. It removes most\npunctuation symbols. It is the best choice for most languages.\n\n<<analysis-letter-tokenizer,Letter Tokenizer>>::\n\nThe `letter` tokenizer divides text into terms whenever it encounters a\ncharacter which is not a letter.\n\n<<analysis-lowercase-tokenizer,Lowercase Tokenizer>>::\n\nThe `lowercase` tokenizer, like the `letter` tokenizer,  divides text into\nterms whenever it encounters a character which is not a letter, but it also\nlowercases all terms.\n\n<<analysis-whitespace-tokenizer,Whitespace Tokenizer>>::\n\nThe `whitespace` tokenizer divides text into terms whenever it encounters any\nwhitespace character.\n\n<<analysis-uaxurlemail-tokenizer,UAX URL Email Tokenizer>>::\n\nThe `uax_url_email` tokenizer is like the `standard` tokenizer except that it\nrecognises URLs and email addresses as single tokens.\n\n<<analysis-classic-tokenizer,Classic Tokenizer>>::\n\nThe `classic` tokenizer is a grammar based tokenizer for the English Language.\n\n<<analysis-thai-tokenizer,Thai Tokenizer>>::\n\nThe `thai` tokenizer segments Thai text into words.\n\n[discrete]\n=== Partial Word Tokenizers\n\nThese tokenizers break up text or words into small fragments, for partial word\nmatching:\n\n<<analysis-ngram-tokenizer,N-Gram Tokenizer>>::\n\nThe `ngram` tokenizer can break up text into words when it encounters any of\na list of specified characters (e.g. whitespace or punctuation), then it returns\nn-grams of each word: a sliding window of continuous letters, e.g. `quick` ->\n`[qu, ui, ic, ck]`.\n\n<<analysis-edgengram-tokenizer,Edge N-Gram Tokenizer>>::\n\nThe `edge_ngram` tokenizer can break up text into words when it encounters any of\na list of specified characters (e.g. whitespace or punctuation), then it returns\nn-grams of each word which are anchored to the start of the word, e.g. `quick` ->\n`[q, qu, qui, quic, quick]`.\n\n\n[discrete]\n=== Structured Text Tokenizers\n\nThe following tokenizers are usually used with structured text like\nidentifiers, email addresses, zip codes, and paths, rather than with full\ntext:\n\n<<analysis-keyword-tokenizer,Keyword Tokenizer>>::\n\nThe `keyword` tokenizer is a ``noop'' tokenizer that accepts whatever text it\nis given and outputs the exact same text as a single term. It can be combined\nwith token filters like <<analysis-lowercase-tokenfilter,`lowercase`>> to\nnormalise the analysed terms.\n\n<<analysis-pattern-tokenizer,Pattern Tokenizer>>::\n\nThe `pattern` tokenizer uses a regular expression to either split text into\nterms whenever it matches a word separator, or to capture matching text as\nterms.\n\n<<analysis-simplepattern-tokenizer,Simple Pattern Tokenizer>>::\n\nThe `simple_pattern` tokenizer uses a regular expression to capture matching\ntext as terms. It uses a restricted subset of regular expression features\nand is generally faster than the `pattern` tokenizer.\n\n<<analysis-chargroup-tokenizer,Char Group Tokenizer>>::\n\nThe `char_group` tokenizer is configurable through sets of characters to split\non, which is usually less expensive than running regular expressions.\n\n<<analysis-simplepatternsplit-tokenizer,Simple Pattern Split Tokenizer>>::\n\nThe `simple_pattern_split` tokenizer uses the same restricted regular expression\nsubset as the `simple_pattern` tokenizer, but splits the input at matches rather\nthan returning the matches as terms.\n\n<<analysis-pathhierarchy-tokenizer,Path Tokenizer>>::\n\nThe `path_hierarchy` tokenizer takes a hierarchical value like a filesystem\npath, splits on the path separator, and emits a term for each component in the\ntree, e.g. `/foo/bar/baz` -> `[/foo, /foo/bar, /foo/bar/baz ]`.\n\n\ninclude::tokenizers/chargroup-tokenizer.asciidoc[]\n\ninclude::tokenizers/classic-tokenizer.asciidoc[]\n\ninclude::tokenizers/edgengram-tokenizer.asciidoc[]\n\ninclude::tokenizers/keyword-tokenizer.asciidoc[]\n\ninclude::tokenizers/letter-tokenizer.asciidoc[]\n\ninclude::tokenizers/lowercase-tokenizer.asciidoc[]\n\ninclude::tokenizers/ngram-tokenizer.asciidoc[]\n\ninclude::tokenizers/pathhierarchy-tokenizer.asciidoc[]\n\ninclude::tokenizers/pattern-tokenizer.asciidoc[]\n\ninclude::tokenizers/simplepattern-tokenizer.asciidoc[]\n\ninclude::tokenizers/simplepatternsplit-tokenizer.asciidoc[]\n\ninclude::tokenizers/standard-tokenizer.asciidoc[]\n\ninclude::tokenizers/thai-tokenizer.asciidoc[]\n\ninclude::tokenizers/uaxurlemail-tokenizer.asciidoc[]\n\ninclude::tokenizers/whitespace-tokenizer.asciidoc[]\n"
}