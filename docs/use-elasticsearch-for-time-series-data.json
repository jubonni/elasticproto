{
    "meta": {
        "timestamp": "2024-11-01T03:07:10.525271",
        "size": 5786,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/use-elasticsearch-for-time-series-data.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "use-elasticsearch-for-time-series-data",
        "version": "8.15"
    },
    "doc": "[[use-elasticsearch-for-time-series-data]]\n== Use {es} for time series data\n\n{es} offers features to help you store, manage, and search time series data,\nsuch as logs and metrics. Once in {es}, you can analyze and visualize your data\nusing {kib} and other {stack} features.\n\n[discrete]\n[[set-up-data-tiers]]\n=== Set up data tiers\n\n{es}'s <<index-lifecycle-management,{ilm-init}>> feature uses <<data-tiers,data\ntiers>> to automatically move older data to nodes with less expensive hardware\nas it ages. This helps improve performance and reduce storage costs.\n\nThe hot and content tiers are required. The warm, cold, and frozen tiers are\noptional.\n\nUse high-performance nodes in the hot and warm tiers for faster\nindexing and faster searches on your most recent data. Use slower, less\nexpensive nodes in the cold and frozen tiers to reduce costs.\n\nThe content tier is not typically used for time series data. However, it's\nrequired to create system indices and other indices that aren't part of a data\nstream.\n\nThe steps for setting up data tiers vary based on your deployment type:\n\ninclude::{es-ref-dir}/tab-widgets/data-tiers-widget.asciidoc[]\n\n[discrete]\n[[register-snapshot-repository]]\n=== Register a snapshot repository\n\nThe cold and frozen tiers can use <<searchable-snapshots,{search-snaps}>> to\nreduce local storage costs.\n\nTo use {search-snaps}, you must register a supported snapshot repository. The\nsteps for registering this repository vary based on your deployment type and\nstorage provider:\n\ninclude::{es-ref-dir}/tab-widgets/snapshot-repo-widget.asciidoc[]\n\n[discrete]\n[[create-edit-index-lifecycle-policy]]\n=== Create or edit an index lifecycle policy\n\nA <<data-streams,data stream>> stores your data across multiple backing\nindices. {ilm-init} uses an <<ilm-index-lifecycle,index lifecycle policy>> to\nautomatically move these indices through your data tiers.\n\nIf you use {fleet} or {agent}, edit one of {es}'s built-in lifecycle policies.\nIf you use a custom application, create your own policy. In either case,\nensure your policy:\n\n* Includes a phase for each data tier you've configured.\n* Calculates the threshold, or `min_age`, for phase transition from rollover.\n* Uses {search-snaps} in the cold and frozen phases, if wanted.\n* Includes a delete phase, if needed.\n\ninclude::{es-ref-dir}/tab-widgets/ilm-widget.asciidoc[]\n\n[discrete]\n[[create-ts-component-templates]]\n=== Create component templates\n\nTIP: If you use {fleet} or {agent}, skip to <<search-visualize-your-data>>.\n{fleet} and {agent} use built-in templates to create data streams for you.\n\nIf you use a custom application, you need to set up your own data stream.\ninclude::{es-ref-dir}/data-streams/set-up-a-data-stream.asciidoc[tag=ds-create-component-templates]\n\n[discrete]\n[[create-ts-index-template]]\n=== Create an index template\n\ninclude::{es-ref-dir}/data-streams/set-up-a-data-stream.asciidoc[tag=ds-create-index-template]\n\n[discrete]\n[[add-data-to-data-stream]]\n=== Add data to a data stream\n\ninclude::{es-ref-dir}/data-streams/set-up-a-data-stream.asciidoc[tag=ds-create-data-stream]\n\n[discrete]\n[[search-visualize-your-data]]\n=== Search and visualize your data\n\nTo explore and search your data in {kib}, open the main menu and select\n**Discover**. See {kib}'s {kibana-ref}/discover.html[Discover documentation].\n\nUse {kib}'s **Dashboard** feature to visualize your data in a chart, table, map,\nand more. See {kib}'s {kibana-ref}/dashboard.html[Dashboard documentation].\n\nYou can also search and aggregate your data using the <<search-search,search\nAPI>>. Use <<runtime-search-request,runtime fields>> and <<grok,grok\npatterns>> to dynamically extract data from log messages and other unstructured\ncontent at search time.\n\n[source,console]\n----\nGET my-data-stream/_search\n{\n  \"runtime_mappings\": {\n    \"source.ip\": {\n      \"type\": \"ip\",\n      \"script\": \"\"\"\n        String sourceip=grok('%{IPORHOST:sourceip} .*').extract(doc[ \"message\" ].value)?.sourceip;\n        if (sourceip != null) emit(sourceip);\n      \"\"\"\n    }\n  },\n  \"query\": {\n    \"bool\": {\n      \"filter\": [\n        {\n          \"range\": {\n            \"@timestamp\": {\n              \"gte\": \"now-1d/d\",\n              \"lt\": \"now/d\"\n            }\n          }\n        },\n        {\n          \"range\": {\n            \"source.ip\": {\n              \"gte\": \"192.0.2.0\",\n              \"lte\": \"192.0.2.255\"\n            }\n          }\n        }\n      ]\n    }\n  },\n  \"fields\": [\n    \"*\"\n  ],\n  \"_source\": false,\n  \"sort\": [\n    {\n      \"@timestamp\": \"desc\"\n    },\n    {\n      \"source.ip\": \"desc\"\n    }\n  ]\n}\n----\n// TEST[setup:my_data_stream]\n// TEST[teardown:data_stream_cleanup]\n\n{es} searches are synchronous by default. Searches across frozen data, long time\nranges, or large datasets may take longer. Use the <<submit-async-search,async\nsearch API>> to run searches in the background. For more search options, see\n<<search-your-data>>.\n\n[source,console]\n----\nPOST my-data-stream/_async_search\n{\n  \"runtime_mappings\": {\n    \"source.ip\": {\n      \"type\": \"ip\",\n      \"script\": \"\"\"\n        String sourceip=grok('%{IPORHOST:sourceip} .*').extract(doc[ \"message\" ].value)?.sourceip;\n        if (sourceip != null) emit(sourceip);\n      \"\"\"\n    }\n  },\n  \"query\": {\n    \"bool\": {\n      \"filter\": [\n        {\n          \"range\": {\n            \"@timestamp\": {\n              \"gte\": \"now-2y/d\",\n              \"lt\": \"now/d\"\n            }\n          }\n        },\n        {\n          \"range\": {\n            \"source.ip\": {\n              \"gte\": \"192.0.2.0\",\n              \"lte\": \"192.0.2.255\"\n            }\n          }\n        }\n      ]\n    }\n  },\n  \"fields\": [\n    \"*\"\n  ],\n  \"_source\": false,\n  \"sort\": [\n    {\n      \"@timestamp\": \"desc\"\n    },\n    {\n      \"source.ip\": \"desc\"\n    }\n  ]\n}\n----\n// TEST[setup:my_data_stream]\n// TEST[teardown:data_stream_cleanup]\n"
}