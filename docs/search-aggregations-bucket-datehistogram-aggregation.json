{
    "meta": {
        "timestamp": "2024-11-01T03:07:09.042269",
        "size": 28184,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-datehistogram-aggregation.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "search-aggregations-bucket-datehistogram-aggregation",
        "version": "8.15"
    },
    "doc": "[[search-aggregations-bucket-datehistogram-aggregation]]\n=== Date histogram aggregation\n++++\n<titleabbrev>Date histogram</titleabbrev>\n++++\n\nThis multi-bucket aggregation is similar to the normal\n<<search-aggregations-bucket-histogram-aggregation,histogram>>, but it can\nonly be used with date or date range values. Because dates are represented internally in\nElasticsearch as long values, it is possible, but not as accurate, to use the\nnormal `histogram` on dates as well. The main difference in the two APIs is\nthat here the interval can be specified using date/time expressions. Time-based\ndata requires special support because time-based intervals are not always a\nfixed length.\n\nLike the histogram, values are rounded *down* into the closest bucket. For\nexample, if the interval is a calendar day, `2020-01-03T07:00:01Z` is rounded to\n`2020-01-03T00:00:00Z`. Values are rounded as follows:\n\n[source,java]\n----\nbucket_key = Math.floor(value / interval) * interval\n----\n\n[[calendar_and_fixed_intervals]]\n==== Calendar and fixed intervals\n\nWhen configuring a date histogram aggregation, the interval can be specified\nin two manners: calendar-aware time intervals, and fixed time intervals.\n\nCalendar-aware intervals understand that daylight savings changes the length\nof specific days, months have different amounts of days, and leap seconds can\nbe tacked onto a particular year.\n\nFixed intervals are, by contrast, always multiples of SI units and do not change\nbased on calendaring context.\n\n[[calendar_intervals]]\n==== Calendar intervals\n\nCalendar-aware intervals are configured with the `calendar_interval` parameter.\nYou can specify calendar intervals using the unit name, such as `month`, or as a\nsingle unit quantity, such as `1M`. For example, `day` and `1d` are equivalent.\nMultiple quantities, such as `2d`, are not supported.\n\nThe accepted calendar intervals are:\n\n`minute`, `1m` ::\n\nAll minutes begin at 00 seconds.\nOne minute is the interval between 00 seconds of the first minute and 00\nseconds of the following minute in the specified time zone, compensating for any\nintervening leap seconds, so that the number of minutes and seconds past the\nhour is the same at the start and end.\n\n`hour`, `1h` ::\n\nAll hours begin at 00 minutes and 00 seconds.\nOne hour (1h) is the interval between 00:00 minutes of the first hour and 00:00\nminutes of the following hour in the specified time zone, compensating for any\nintervening leap seconds, so that the number of minutes and seconds past the hour\nis the same at the start and end.\n\n`day`, `1d` ::\n\nAll days begin at the earliest possible time, which is usually 00:00:00\n(midnight).\nOne day (1d) is the interval between the start of the day and the start of\nthe following day in the specified time zone, compensating for any intervening\ntime changes.\n\n`week`, `1w` ::\n\nOne week is the interval between the start day_of_week:hour:minute:second\nand the same day of the week and time of the following week in the specified\ntime zone.\n\n`month`, `1M` ::\n\nOne month is the interval between the start day of the month and time of\nday and the same day of the month and time of the following month in the specified\ntime zone, so that the day of the month and time of day are the same at the start\nand end. Note that the day may differ if an\n<<search-aggregations-bucket-datehistogram-offset-months,`offset` is used that is longer than a month>>.\n\n`quarter`, `1q` ::\n\nOne quarter is the interval between the start day of the month and\ntime of day and the same day of the month and time of day three months later,\nso that the day of the month and time of day are the same at the start and end. +\n\n`year`, `1y` ::\n\nOne year is the interval between the start day of the month and time of\nday and the same day of the month and time of day the following year in the\nspecified time zone, so that the date and time are the same at the start and end. +\n\n[[calendar_interval_examples]]\n===== Calendar interval examples\nAs an example, here is an aggregation requesting bucket intervals of a month in calendar time:\n\n[source,console,id=datehistogram-aggregation-calendar-interval-example]\n--------------------------------------------------\nPOST /sales/_search?size=0\n{\n  \"aggs\": {\n    \"sales_over_time\": {\n      \"date_histogram\": {\n        \"field\": \"date\",\n        \"calendar_interval\": \"month\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:sales]\n\nIf you attempt to use multiples of calendar units, the aggregation will fail because only\nsingular calendar units are supported:\n\n[source,console,id=datehistogram-aggregation-calendar-interval-multiples-example]\n--------------------------------------------------\nPOST /sales/_search?size=0\n{\n  \"aggs\": {\n    \"sales_over_time\": {\n      \"date_histogram\": {\n        \"field\": \"date\",\n        \"calendar_interval\": \"2d\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:sales]\n// TEST[catch:bad_request]\n\n[source,js]\n--------------------------------------------------\n{\n  \"error\" : {\n    \"root_cause\" : [...],\n    \"type\" : \"x_content_parse_exception\",\n    \"reason\" : \"[1:82] [date_histogram] failed to parse field [calendar_interval]\",\n    \"caused_by\" : {\n      \"type\" : \"illegal_argument_exception\",\n      \"reason\" : \"The supplied interval [2d] could not be parsed as a calendar interval.\",\n      \"stack_trace\" : \"java.lang.IllegalArgumentException: The supplied interval [2d] could not be parsed as a calendar interval.\"\n    }\n  }\n}\n\n--------------------------------------------------\n// NOTCONSOLE\n\n[[fixed_intervals]]\n==== Fixed intervals\n\nFixed intervals are configured with the `fixed_interval` parameter.\n\nIn contrast to calendar-aware intervals, fixed intervals are a fixed number of SI\nunits and never deviate, regardless of where they fall on the calendar. One second\nis always composed of `1000ms`. This allows fixed intervals to be specified in\nany multiple of the supported units.\n\nHowever, it means fixed intervals cannot express other units such as months,\nsince the duration of a month is not a fixed quantity. Attempting to specify\na calendar interval like month or quarter will throw an exception.\n\nThe accepted units for fixed intervals are:\n\nmilliseconds (`ms`) ::\nA single millisecond. This is a very, very small interval.\n\nseconds (`s`) ::\nDefined as 1000 milliseconds each.\n\nminutes (`m`) ::\nDefined as 60 seconds each (60,000 milliseconds).\nAll minutes begin at 00 seconds.\n\nhours (`h`) ::\nDefined as 60 minutes each (3,600,000 milliseconds).\nAll hours begin at 00 minutes and 00 seconds.\n\ndays (`d`) ::\nDefined as 24 hours (86,400,000 milliseconds).\nAll days begin at the earliest possible time, which is usually 00:00:00\n(midnight).\n\n[[fixed_interval_examples]]\n===== Fixed interval examples\n\nIf we try to recreate the \"month\" `calendar_interval` from earlier, we can approximate that with\n30 fixed days:\n\n[source,console,id=datehistogram-aggregation-fixed-interval-example]\n--------------------------------------------------\nPOST /sales/_search?size=0\n{\n  \"aggs\": {\n    \"sales_over_time\": {\n      \"date_histogram\": {\n        \"field\": \"date\",\n        \"fixed_interval\": \"30d\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:sales]\n\nBut if we try to use a calendar unit that is not supported, such as weeks, we'll get an exception:\n\n[source,console,id=datehistogram-aggregation-fixed-interval-unsupported-example]\n--------------------------------------------------\nPOST /sales/_search?size=0\n{\n  \"aggs\": {\n    \"sales_over_time\": {\n      \"date_histogram\": {\n        \"field\": \"date\",\n        \"fixed_interval\": \"2w\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:sales]\n// TEST[catch:bad_request]\n\n[source,js]\n--------------------------------------------------\n{\n  \"error\" : {\n    \"root_cause\" : [...],\n    \"type\" : \"x_content_parse_exception\",\n    \"reason\" : \"[1:82] [date_histogram] failed to parse field [fixed_interval]\",\n    \"caused_by\" : {\n      \"type\" : \"illegal_argument_exception\",\n      \"reason\" : \"failed to parse setting [date_histogram.fixedInterval] with value [2w] as a time value: unit is missing or unrecognized\",\n      \"stack_trace\" : \"java.lang.IllegalArgumentException: failed to parse setting [date_histogram.fixedInterval] with value [2w] as a time value: unit is missing or unrecognized\"\n    }\n  }\n}\n\n--------------------------------------------------\n// NOTCONSOLE\n\n[[datehistogram-aggregation-notes]]\n==== Date histogram usage notes\n\nIn all cases, when the specified end time does not exist, the actual end time is\nthe closest available time after the specified end.\n\nWidely distributed applications must also consider vagaries such as countries that\nstart and stop daylight savings time at 12:01 A.M., so end up with one minute of\nSunday followed by an additional 59 minutes of Saturday once a year, and countries\nthat decide to move across the international date line. Situations like\nthat can make irregular time zone offsets seem easy.\n\nAs always, rigorous testing, especially around time-change events, will ensure\nthat your time interval specification is\nwhat you intend it to be.\n\nWARNING: To avoid unexpected results, all connected servers and clients must\nsync to a reliable network time service.\n\nNOTE: Fractional time values are not supported, but you can address this by\nshifting to another time unit (e.g., `1.5h` could instead be specified as `90m`).\n\nNOTE: You can also specify time values using abbreviations supported by\n<<time-units,time units>> parsing.\n\n[[datehistogram-aggregation-keys]]\n==== Keys\n\nInternally, a date is represented as a 64 bit number representing a timestamp\nin milliseconds-since-the-epoch (01/01/1970 midnight UTC). These timestamps are\nreturned as the ++key++ name of the bucket. The `key_as_string` is the same\ntimestamp converted to a formatted\ndate string using the `format` parameter specification:\n\nTIP: If you don't specify `format`, the first date\n<<mapping-date-format,format>> specified in the field mapping is used.\n\n[source,console,id=datehistogram-aggregation-format-example]\n--------------------------------------------------\nPOST /sales/_search?size=0\n{\n  \"aggs\": {\n    \"sales_over_time\": {\n      \"date_histogram\": {\n        \"field\": \"date\",\n        \"calendar_interval\": \"1M\",\n        \"format\": \"yyyy-MM-dd\" <1>\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:sales]\n\n<1> Supports expressive date <<date-format-pattern,format pattern>>\n\nResponse:\n\n[source,console-result]\n--------------------------------------------------\n{\n  ...\n  \"aggregations\": {\n    \"sales_over_time\": {\n      \"buckets\": [\n        {\n          \"key_as_string\": \"2015-01-01\",\n          \"key\": 1420070400000,\n          \"doc_count\": 3\n        },\n        {\n          \"key_as_string\": \"2015-02-01\",\n          \"key\": 1422748800000,\n          \"doc_count\": 2\n        },\n        {\n          \"key_as_string\": \"2015-03-01\",\n          \"key\": 1425168000000,\n          \"doc_count\": 2\n        }\n      ]\n    }\n  }\n}\n--------------------------------------------------\n// TESTRESPONSE[s/\\.\\.\\./\"took\": $body.took,\"timed_out\": false,\"_shards\": $body._shards,\"hits\": $body.hits,/]\n\n[[datehistogram-aggregation-time-zone]]\n==== Time zone\n\n{es} stores date-times in Coordinated Universal Time (UTC). By default, all bucketing and\nrounding is also done in UTC. Use the `time_zone` parameter to indicate\nthat bucketing should use a different time zone.\n\nWhen you specify a time zone, the following logic is used to determine the bucket the document belongs in:\n\n[source,java]\n----\nbucket_key = localToUtc(Math.floor(utcToLocal(value) / interval) * interval))\n----\n\nFor example, if the interval is a calendar day and the time zone is\n`America/New_York`, then the date value `2020-01-03T01:00:01Z` is processed as follows:\n\n. Converted to EST: `2020-01-02T20:00:01`\n. Rounded down to the nearest interval: `2020-01-02T00:00:00`\n. Converted back to UTC: `2020-01-02T05:00:00:00Z`\n\nWhen a `key_as_string` is generated for the bucket, the key value is stored in `America/New_York` time, so it'll display as `\"2020-01-02T00:00:00\"`.\n\nYou can specify time zones as an ISO 8601 UTC offset, such as `+01:00` or\n`-08:00`, or as an IANA time zone ID,\nsuch as `America/Los_Angeles`.\n\nConsider the following example:\n\n[source,console,id=datehistogram-aggregation-timezone-example]\n---------------------------------\nPUT my-index-000001/_doc/1?refresh\n{\n  \"date\": \"2015-10-01T00:30:00Z\"\n}\n\nPUT my-index-000001/_doc/2?refresh\n{\n  \"date\": \"2015-10-01T01:30:00Z\"\n}\n\nGET my-index-000001/_search?size=0\n{\n  \"aggs\": {\n    \"by_day\": {\n      \"date_histogram\": {\n        \"field\":     \"date\",\n        \"calendar_interval\":  \"day\"\n      }\n    }\n  }\n}\n---------------------------------\n\nIf you don't specify a time zone, UTC is used. This would result in both of these\ndocuments being placed into the same day bucket, which starts at midnight UTC\non 1 October 2015:\n\n[source,console-result]\n---------------------------------\n{\n  ...\n  \"aggregations\": {\n    \"by_day\": {\n      \"buckets\": [\n        {\n          \"key_as_string\": \"2015-10-01T00:00:00.000Z\",\n          \"key\":           1443657600000,\n          \"doc_count\":     2\n        }\n      ]\n    }\n  }\n}\n---------------------------------\n// TESTRESPONSE[s/\\.\\.\\./\"took\": $body.took,\"timed_out\": false,\"_shards\": $body._shards,\"hits\": $body.hits,/]\n\nIf you specify a `time_zone` of `-01:00`, midnight in that time zone is one hour\nbefore midnight UTC:\n\n[source,console]\n---------------------------------\nGET my-index-000001/_search?size=0\n{\n  \"aggs\": {\n    \"by_day\": {\n      \"date_histogram\": {\n        \"field\":     \"date\",\n        \"calendar_interval\":  \"day\",\n        \"time_zone\": \"-01:00\"\n      }\n    }\n  }\n}\n---------------------------------\n// TEST[continued]\n\nNow the first document falls into the bucket for 30 September 2015, while the\nsecond document falls into the bucket for 1 October 2015:\n\n[source,console-result]\n---------------------------------\n{\n  ...\n  \"aggregations\": {\n    \"by_day\": {\n      \"buckets\": [\n        {\n          \"key_as_string\": \"2015-09-30T00:00:00.000-01:00\", <1>\n          \"key\": 1443574800000,\n          \"doc_count\": 1\n        },\n        {\n          \"key_as_string\": \"2015-10-01T00:00:00.000-01:00\", <1>\n          \"key\": 1443661200000,\n          \"doc_count\": 1\n        }\n      ]\n    }\n  }\n}\n---------------------------------\n// TESTRESPONSE[s/\\.\\.\\./\"took\": $body.took,\"timed_out\": false,\"_shards\": $body._shards,\"hits\": $body.hits,/]\n\n<1> The `key_as_string` value represents midnight on each day\n    in the specified time zone.\n\nWARNING: Many time zones shift their clocks for daylight savings time. Buckets\nclose to the moment when those changes happen can have slightly different sizes\nthan you would expect from the `calendar_interval` or `fixed_interval`.\nFor example, consider a DST start in the `CET` time zone: on 27 March 2016 at 2am,\nclocks were turned forward 1 hour to 3am local time. If you use `day` as the\n`calendar_interval`, the bucket covering that day will only hold data for 23\nhours instead of the usual 24 hours for other buckets. The same is true for\nshorter intervals, like a `fixed_interval` of `12h`, where you'll have only a 11h\nbucket on the morning of 27 March when the DST shift happens.\n\n[[search-aggregations-bucket-datehistogram-offset]]\n==== Offset\n\n// tag::offset-explanation[]\nUse the `offset` parameter to change the start value of each bucket by the\nspecified positive (`+`) or negative offset (`-`) duration, such as `1h` for\nan hour, or `1d` for a day. See <<time-units>> for more possible time\nduration options.\n\nFor example, when using an interval of `day`, each bucket runs from midnight\nto midnight. Setting the `offset` parameter to `+6h` changes each bucket\nto run from 6am to 6am:\n// end::offset-explanation[]\n\n[source,console,id=datehistogram-aggregation-offset-example]\n-----------------------------\nPUT my-index-000001/_doc/1?refresh\n{\n  \"date\": \"2015-10-01T05:30:00Z\"\n}\n\nPUT my-index-000001/_doc/2?refresh\n{\n  \"date\": \"2015-10-01T06:30:00Z\"\n}\n\nGET my-index-000001/_search?size=0\n{\n  \"aggs\": {\n    \"by_day\": {\n      \"date_histogram\": {\n        \"field\":     \"date\",\n        \"calendar_interval\":  \"day\",\n        \"offset\":    \"+6h\"\n      }\n    }\n  }\n}\n-----------------------------\n\n// tag::offset-result-intro[]\nInstead of a single bucket starting at midnight, the above request groups the\ndocuments into buckets starting at 6am:\n// end::offset-result-intro[]\n\n[source,console-result]\n-----------------------------\n{\n  ...\n  \"aggregations\": {\n    \"by_day\": {\n      \"buckets\": [\n        {\n          \"key_as_string\": \"2015-09-30T06:00:00.000Z\",\n          \"key\": 1443592800000,\n          \"doc_count\": 1\n        },\n        {\n          \"key_as_string\": \"2015-10-01T06:00:00.000Z\",\n          \"key\": 1443679200000,\n          \"doc_count\": 1\n        }\n      ]\n    }\n  }\n}\n-----------------------------\n// TESTRESPONSE[s/\\.\\.\\./\"took\": $body.took,\"timed_out\": false,\"_shards\": $body._shards,\"hits\": $body.hits,/]\n\n// tag::offset-note[]\nNOTE: The start `offset` of each bucket is calculated after `time_zone`\nadjustments have been made.\n// end::offset-note[]\n\n[[search-aggregations-bucket-datehistogram-offset-months]]\n===== Long offsets over calendar intervals\n\nIt is typical to use offsets in units smaller than the `calendar_interval`. For example,\nusing offsets in hours when the interval is days, or an offset of days when the interval is months.\nIf the calendar interval is always of a standard length, or the `offset` is less than one unit of the calendar\ninterval (for example less than `+24h` for `days` or less than `+28d` for months),\nthen each bucket will have a repeating start. For example `+6h` for `days` will result in all buckets\nstarting at 6am each day. However, `+30h` will also result in buckets starting at 6am, except when crossing\ndays that change from standard to summer-savings time or vice-versa.\n\nThis situation is much more pronounced for months, where each month has a different length\nto at least one of its adjacent months.\nTo demonstrate this, consider eight documents each with a date field on the 20th day of each of the\neight months from January to August of 2022.\n\nWhen querying for a date histogram over the calendar interval of months, the response will return one bucket per month, each with a single document.\nEach bucket will have a key named after the first day of the month, plus any offset.\nFor example, the offset of `+19d` will result in buckets with names like `2022-01-20`.\n\n[source,console,id=datehistogram-aggregation-offset-example-19d]\n--------------------------------------------------\n\"buckets\": [\n  { \"key_as_string\": \"2022-01-20\", \"key\": 1642636800000, \"doc_count\": 1 },\n  { \"key_as_string\": \"2022-02-20\", \"key\": 1645315200000, \"doc_count\": 1 },\n  { \"key_as_string\": \"2022-03-20\", \"key\": 1647734400000, \"doc_count\": 1 },\n  { \"key_as_string\": \"2022-04-20\", \"key\": 1650412800000, \"doc_count\": 1 },\n  { \"key_as_string\": \"2022-05-20\", \"key\": 1653004800000, \"doc_count\": 1 },\n  { \"key_as_string\": \"2022-06-20\", \"key\": 1655683200000, \"doc_count\": 1 },\n  { \"key_as_string\": \"2022-07-20\", \"key\": 1658275200000, \"doc_count\": 1 },\n  { \"key_as_string\": \"2022-08-20\", \"key\": 1660953600000, \"doc_count\": 1 }\n]\n--------------------------------------------------\n// TESTRESPONSE[skip:no setup made for this example yet]\n\nIncreasing the offset to `+20d`, each document will appear in a bucket for the previous month,\nwith all bucket keys ending with the same day of the month, as normal.\nHowever, further increasing to `+28d`,\nwhat used to be a February bucket has now become `\"2022-03-01\"`.\n\n[source,console,id=datehistogram-aggregation-offset-example-28d]\n--------------------------------------------------\n\"buckets\": [\n  { \"key_as_string\": \"2021-12-29\", \"key\": 1640736000000, \"doc_count\": 1 },\n  { \"key_as_string\": \"2022-01-29\", \"key\": 1643414400000, \"doc_count\": 1 },\n  { \"key_as_string\": \"2022-03-01\", \"key\": 1646092800000, \"doc_count\": 1 },\n  { \"key_as_string\": \"2022-03-29\", \"key\": 1648512000000, \"doc_count\": 1 },\n  { \"key_as_string\": \"2022-04-29\", \"key\": 1651190400000, \"doc_count\": 1 },\n  { \"key_as_string\": \"2022-05-29\", \"key\": 1653782400000, \"doc_count\": 1 },\n  { \"key_as_string\": \"2022-06-29\", \"key\": 1656460800000, \"doc_count\": 1 },\n  { \"key_as_string\": \"2022-07-29\", \"key\": 1659052800000, \"doc_count\": 1 }\n]\n--------------------------------------------------\n// TESTRESPONSE[skip:no setup made for this example yet]\n\nIf we continue to increase the offset, the 30-day months will also shift into the next month,\nso that 3 of the 8 buckets have different days than the other five.\nIn fact if we keep going, we will find cases where two documents appear in the same month.\nDocuments that were originally 30 days apart can be shifted into the same 31-day month bucket.\n\nFor example, for `+50d` we see:\n\n[source,console,id=datehistogram-aggregation-offset-example-50d]\n--------------------------------------------------\n\"buckets\": [\n  { \"key_as_string\": \"2022-01-20\", \"key\": 1642636800000, \"doc_count\": 1 },\n  { \"key_as_string\": \"2022-02-20\", \"key\": 1645315200000, \"doc_count\": 2 },\n  { \"key_as_string\": \"2022-04-20\", \"key\": 1650412800000, \"doc_count\": 2 },\n  { \"key_as_string\": \"2022-06-20\", \"key\": 1655683200000, \"doc_count\": 2 },\n  { \"key_as_string\": \"2022-08-20\", \"key\": 1660953600000, \"doc_count\": 1 }\n]\n--------------------------------------------------\n// TESTRESPONSE[skip:no setup made for this example yet]\n\nIt is therefore always important when using `offset` with `calendar_interval` bucket sizes\nto understand the consequences of using offsets larger than the interval size.\n\nMore examples:\n\n* If the goal is to, for example, have an annual histogram where each year starts on the 5th February,\nyou could use `calendar_interval` of `year` and `offset` of `+33d`, and each year will be shifted identically,\nbecause the offset includes only January, which is the same length every year.\nHowever, if the goal is to have the year start on the 5th March instead, this technique will not work because\nthe offset includes February, which changes length every four years.\n* If you want a quarterly histogram starting on a date within the first month of the year, it will work,\nbut as soon as you push the start date into the second month by having an offset longer than a month, the\nquarters will all start on different dates.\n\n[[date-histogram-keyed-response]]\n==== Keyed response\n\nSetting the `keyed` flag to `true` associates a unique string key with each\nbucket and returns the ranges as a hash rather than an array:\n\n[source,console,id=datehistogram-aggregation-keyed-example]\n--------------------------------------------------\nPOST /sales/_search?size=0\n{\n  \"aggs\": {\n    \"sales_over_time\": {\n      \"date_histogram\": {\n        \"field\": \"date\",\n        \"calendar_interval\": \"1M\",\n        \"format\": \"yyyy-MM-dd\",\n        \"keyed\": true\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:sales]\n\nResponse:\n\n[source,console-result]\n--------------------------------------------------\n{\n  ...\n  \"aggregations\": {\n    \"sales_over_time\": {\n      \"buckets\": {\n        \"2015-01-01\": {\n          \"key_as_string\": \"2015-01-01\",\n          \"key\": 1420070400000,\n          \"doc_count\": 3\n        },\n        \"2015-02-01\": {\n          \"key_as_string\": \"2015-02-01\",\n          \"key\": 1422748800000,\n          \"doc_count\": 2\n        },\n        \"2015-03-01\": {\n          \"key_as_string\": \"2015-03-01\",\n          \"key\": 1425168000000,\n          \"doc_count\": 2\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TESTRESPONSE[s/\\.\\.\\./\"took\": $body.took,\"timed_out\": false,\"_shards\": $body._shards,\"hits\": $body.hits,/]\n\n[[date-histogram-scripts]]\n==== Scripts\n\nIf the data in your documents doesn't exactly match what you'd like to aggregate,\nuse a <<runtime,runtime field>> . For example, if the revenue\nfor promoted sales should be recognized a day after the sale date:\n\n[source,console,id=datehistogram-aggregation-runtime-field]\n----\nPOST /sales/_search?size=0\n{\n  \"runtime_mappings\": {\n    \"date.promoted_is_tomorrow\": {\n      \"type\": \"date\",\n      \"script\": \"\"\"\n        long date = doc['date'].value.toInstant().toEpochMilli();\n        if (doc['promoted'].value) {\n          date += 86400;\n        }\n        emit(date);\n      \"\"\"\n    }\n  },\n  \"aggs\": {\n    \"sales_over_time\": {\n      \"date_histogram\": {\n        \"field\": \"date.promoted_is_tomorrow\",\n        \"calendar_interval\": \"1M\"\n      }\n    }\n  }\n}\n----\n// TEST[setup:sales]\n\n////\n\n[source,console-result]\n----\n{\n  ...\n  \"aggregations\": {\n    \"sales_over_time\": {\n      \"buckets\": [\n        {\n          \"key_as_string\": \"2015-01-01T00:00:00.000Z\",\n          \"key\": 1420070400000,\n          \"doc_count\": 3\n        },\n        {\n          \"key_as_string\": \"2015-02-01T00:00:00.000Z\",\n          \"key\": 1422748800000,\n          \"doc_count\": 2\n        },\n        {\n          \"key_as_string\": \"2015-03-01T00:00:00.000Z\",\n          \"key\": 1425168000000,\n          \"doc_count\": 2\n        }\n      ]\n    }\n  }\n}\n----\n// TESTRESPONSE[s/\\.\\.\\./\"took\": $body.took,\"timed_out\": false,\"_shards\": $body._shards,\"hits\": $body.hits,/]\n\n////\n\n[[date-histogram-params]]\n==== Parameters\n\nYou can control the order of the returned\nbuckets using the `order`\nsettings and filter the returned buckets based on a `min_doc_count` setting\n(by default all buckets between the first\nbucket that matches documents and the last one are returned). This histogram\nalso supports the `extended_bounds`\nsetting, which enables extending the bounds of the histogram beyond the data\nitself, and `hard_bounds` that limits the histogram to specified bounds.\nFor more information, see\n<<search-aggregations-bucket-histogram-aggregation-extended-bounds,`Extended Bounds`>> and\n<<search-aggregations-bucket-histogram-aggregation-hard-bounds,`Hard Bounds`>>.\n\n[[date-histogram-missing-value]]\n===== Missing value\n\nThe `missing` parameter defines how to treat documents that are missing a value.\nBy default, they are ignored, but it is also possible to treat them as if they\nhave a value.\n\n[source,console,id=datehistogram-aggregation-missing-example]\n--------------------------------------------------\nPOST /sales/_search?size=0\n{\n  \"aggs\": {\n    \"sale_date\": {\n      \"date_histogram\": {\n        \"field\": \"date\",\n        \"calendar_interval\": \"year\",\n        \"missing\": \"2000/01/01\" <1>\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:sales]\n\n<1> Documents without a value in the `date` field will fall into the\nsame bucket as documents that have the value `2000-01-01`.\n\n[[date-histogram-order]]\n===== Order\n\nBy default the returned buckets are sorted by their `key` ascending, but you can\ncontrol the order using\nthe `order` setting. This setting supports the same `order` functionality as\n<<search-aggregations-bucket-terms-aggregation-order,`Terms Aggregation`>>.\n\n[[date-histogram-aggregate-scripts]]\n===== Using a script to aggregate by day of the week\n\nWhen you need to aggregate the results by day of the week, run a `terms`\naggregation on a <<runtime,runtime field>> that returns the day of the week:\n\n[source,console,id=datehistogram-aggregation-day-of-week-runtime-field]\n----\nPOST /sales/_search?size=0\n{\n  \"runtime_mappings\": {\n    \"date.day_of_week\": {\n      \"type\": \"keyword\",\n      \"script\": \"emit(doc['date'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ENGLISH))\"\n    }\n  },\n  \"aggs\": {\n    \"day_of_week\": {\n      \"terms\": { \"field\": \"date.day_of_week\" }\n    }\n  }\n}\n----\n// TEST[setup:sales]\n\nResponse:\n\n[source,console-result]\n----\n{\n  ...\n  \"aggregations\": {\n    \"day_of_week\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"Sunday\",\n          \"doc_count\": 4\n        },\n        {\n          \"key\": \"Thursday\",\n          \"doc_count\": 3\n        }\n      ]\n    }\n  }\n}\n----\n// TESTRESPONSE[s/\\.\\.\\./\"took\": $body.took,\"timed_out\": false,\"_shards\": $body._shards,\"hits\": $body.hits,/]\n\nThe response will contain all the buckets having the relative day of\nthe week as key : 1 for Monday, 2 for Tuesday... 7 for Sunday.\n"
}