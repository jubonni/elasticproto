{
    "meta": {
        "timestamp": "2024-11-01T02:49:24.632072",
        "size": 5465,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-classic-tokenizer.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "analysis-classic-tokenizer",
        "version": "8.15"
    },
    "doc": "[[analysis-classic-tokenizer]]\n=== Classic tokenizer\n++++\n<titleabbrev>Classic</titleabbrev>\n++++\n\nThe `classic` tokenizer is a grammar based tokenizer that is good for English\nlanguage documents. This tokenizer has heuristics for special treatment of\nacronyms, company names, email addresses, and internet host names. However,\nthese rules don't always work, and the tokenizer doesn't work well for most\nlanguages other than English:\n\n* It splits words at most punctuation characters, removing punctuation. However, a\n  dot that's not followed by whitespace is considered part of a token.\n\n* It splits words at hyphens, unless there's a number in the token, in which case\n  the whole token is interpreted as a product number and is not split.\n\n* It recognizes email addresses and internet hostnames as one token.\n\n[discrete]\n=== Example output\n\n[source,console]\n---------------------------\nPOST _analyze\n{\n  \"tokenizer\": \"classic\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n---------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"The\",\n      \"start_offset\": 0,\n      \"end_offset\": 3,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"2\",\n      \"start_offset\": 4,\n      \"end_offset\": 5,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"QUICK\",\n      \"start_offset\": 6,\n      \"end_offset\": 11,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"Brown\",\n      \"start_offset\": 12,\n      \"end_offset\": 17,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 3\n    },\n    {\n      \"token\": \"Foxes\",\n      \"start_offset\": 18,\n      \"end_offset\": 23,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 4\n    },\n    {\n      \"token\": \"jumped\",\n      \"start_offset\": 24,\n      \"end_offset\": 30,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 5\n    },\n    {\n      \"token\": \"over\",\n      \"start_offset\": 31,\n      \"end_offset\": 35,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 6\n    },\n    {\n      \"token\": \"the\",\n      \"start_offset\": 36,\n      \"end_offset\": 39,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 7\n    },\n    {\n      \"token\": \"lazy\",\n      \"start_offset\": 40,\n      \"end_offset\": 44,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 8\n    },\n    {\n      \"token\": \"dog's\",\n      \"start_offset\": 45,\n      \"end_offset\": 50,\n      \"type\": \"<APOSTROPHE>\",\n      \"position\": 9\n    },\n    {\n      \"token\": \"bone\",\n      \"start_offset\": 51,\n      \"end_offset\": 55,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 10\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe above sentence would produce the following terms:\n\n[source,text]\n---------------------------\n[ The, 2, QUICK, Brown, Foxes, jumped, over, the, lazy, dog's, bone ]\n---------------------------\n\n[discrete]\n=== Configuration\n\nThe `classic` tokenizer accepts the following parameters:\n\n[horizontal]\n`max_token_length`::\n\n    The maximum token length. If a token is seen that exceeds this length then\n    it is split at `max_token_length` intervals. Defaults to `255`.\n\n[discrete]\n=== Example configuration\n\nIn this example, we configure the `classic` tokenizer to have a\n`max_token_length` of 5 (for demonstration purposes):\n\n[source,console]\n----------------------------\nPUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"my_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"classic\",\n          \"max_token_length\": 5\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n----------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"The\",\n      \"start_offset\": 0,\n      \"end_offset\": 3,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"2\",\n      \"start_offset\": 4,\n      \"end_offset\": 5,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"QUICK\",\n      \"start_offset\": 6,\n      \"end_offset\": 11,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"Brown\",\n      \"start_offset\": 12,\n      \"end_offset\": 17,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 3\n    },\n    {\n      \"token\": \"Foxes\",\n      \"start_offset\": 18,\n      \"end_offset\": 23,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 4\n    },\n    {\n      \"token\": \"over\",\n      \"start_offset\": 31,\n      \"end_offset\": 35,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 6\n    },\n    {\n      \"token\": \"the\",\n      \"start_offset\": 36,\n      \"end_offset\": 39,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 7\n    },\n    {\n      \"token\": \"lazy\",\n      \"start_offset\": 40,\n      \"end_offset\": 44,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 8\n    },\n    {\n      \"token\": \"dog's\",\n      \"start_offset\": 45,\n      \"end_offset\": 50,\n      \"type\": \"<APOSTROPHE>\",\n      \"position\": 9\n    },\n    {\n      \"token\": \"bone\",\n      \"start_offset\": 51,\n      \"end_offset\": 55,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 10\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe above example produces the following terms:\n\n[source,text]\n---------------------------\n[ The, 2, QUICK, Brown, Foxes, jumpe, d, over, the, lazy, dog's, bone ]\n---------------------------\n"
}