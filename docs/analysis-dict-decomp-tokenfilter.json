{
    "meta": {
        "timestamp": "2024-11-01T03:02:52.409579",
        "size": 4875,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-dict-decomp-tokenfilter.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "analysis-dict-decomp-tokenfilter",
        "version": "8.15"
    },
    "doc": "[[analysis-dict-decomp-tokenfilter]]\n=== Dictionary decompounder token filter\n++++\n<titleabbrev>Dictionary decompounder</titleabbrev>\n++++\n\n[NOTE]\n====\nIn most cases, we recommend using the faster\n<<analysis-hyp-decomp-tokenfilter,`hyphenation_decompounder`>> token filter\nin place of this filter. However, you can use the\n`dictionary_decompounder` filter to check the quality of a word list before\nimplementing it in the `hyphenation_decompounder` filter.\n====\n\nUses a specified list of words and a brute force approach to find subwords in\ncompound words. If found, these subwords are included in the token output.\n\nThis filter uses Lucene's\n{lucene-analysis-docs}/compound/DictionaryCompoundWordTokenFilter.html[DictionaryCompoundWordTokenFilter],\nwhich was built for Germanic languages.\n\n[[analysis-dict-decomp-tokenfilter-analyze-ex]]\n==== Example\n\nThe following <<indices-analyze,analyze API>> request uses the\n`dictionary_decompounder` filter to find subwords in `Donaudampfschiff`. The\nfilter then checks these subwords against the specified list of words: `Donau`,\n`dampf`, `meer`, and `schiff`.\n\n[source,console]\n--------------------------------------------------\nGET _analyze\n{\n  \"tokenizer\": \"standard\",\n  \"filter\": [\n    {\n      \"type\": \"dictionary_decompounder\",\n      \"word_list\": [\"Donau\", \"dampf\", \"meer\", \"schiff\"]\n    }\n  ],\n  \"text\": \"Donaudampfschiff\"\n}\n--------------------------------------------------\n\nThe filter produces the following tokens:\n\n[source,text]\n--------------------------------------------------\n[ Donaudampfschiff, Donau, dampf, schiff ]\n--------------------------------------------------\n\n/////////////////////\n[source,console-result]\n--------------------------------------------------\n{\n  \"tokens\" : [\n    {\n      \"token\" : \"Donaudampfschiff\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 16,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 0\n    },\n    {\n      \"token\" : \"Donau\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 16,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 0\n    },\n    {\n      \"token\" : \"dampf\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 16,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 0\n    },\n    {\n      \"token\" : \"schiff\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 16,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 0\n    }\n  ]\n}\n--------------------------------------------------\n/////////////////////\n\n[[analysis-dict-decomp-tokenfilter-configure-parms]]\n==== Configurable parameters\n\n`word_list`::\n+\n--\n(Required+++*+++, array of strings)\nA list of subwords to look for in the token stream. If found, the subword is\nincluded in the token output.\n\nEither this parameter or `word_list_path` must be specified.\n--\n\n`word_list_path`::\n+\n--\n(Required+++*+++, string)\nPath to a file that contains a list of subwords to find in the token stream. If\nfound, the subword is included in the token output.\n\nThis path must be absolute or relative to the `config` location, and the file\nmust be UTF-8 encoded. Each token in the file must be separated by a line break.\n\nEither this parameter or `word_list` must be specified.\n--\n\n`max_subword_size`::\n(Optional, integer)\nMaximum subword character length. Longer subword tokens are excluded from the\noutput. Defaults to `15`.\n\n`min_subword_size`::\n(Optional, integer)\nMinimum subword character length. Shorter subword tokens are excluded from the\noutput. Defaults to `2`.\n\n`min_word_size`::\n(Optional, integer)\nMinimum word character length. Shorter word tokens are excluded from the\noutput. Defaults to `5`.\n\n`only_longest_match`::\n(Optional, Boolean)\nIf `true`, only include the longest matching subword. Defaults to `false`.\n\n[[analysis-dict-decomp-tokenfilter-customize]]\n==== Customize and add to an analyzer\n\nTo customize the `dictionary_decompounder` filter, duplicate it to create the\nbasis for a new custom token filter. You can modify the filter using its\nconfigurable parameters.\n\nFor example, the following <<indices-create-index,create index API>> request\nuses a custom `dictionary_decompounder` filter to configure a new\n<<analysis-custom-analyzer,custom analyzer>>.\n\nThe custom `dictionary_decompounder` filter find subwords in the\n`analysis/example_word_list.txt` file. Subwords longer than 22 characters are\nexcluded from the token output.\n\n[source,console]\n--------------------------------------------------\nPUT dictionary_decompound_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"standard_dictionary_decompound\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [ \"22_char_dictionary_decompound\" ]\n        }\n      },\n      \"filter\": {\n        \"22_char_dictionary_decompound\": {\n          \"type\": \"dictionary_decompounder\",\n          \"word_list_path\": \"analysis/example_word_list.txt\",\n          \"max_subword_size\": 22\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n"
}