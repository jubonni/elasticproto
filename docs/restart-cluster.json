{
    "meta": {
        "timestamp": "2024-11-01T03:02:53.356581",
        "size": 7645,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/restart-cluster.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "restart-cluster",
        "version": "8.15"
    },
    "doc": "[[restart-cluster]]\n== Full-cluster restart and rolling restart\n\nThere may be <<security-basic-setup,situations where you want\nto perform a full-cluster restart>> or a rolling restart. In the case of\n<<restart-cluster-full,full-cluster restart>>, you shut down and restart all the\nnodes in the cluster while in the case of\n<<restart-cluster-rolling,rolling restart>>, you shut down only one node at a\ntime, so the service remains uninterrupted.\n\n[WARNING]\n====\nNodes exceeding the low watermark threshold will be slow to restart. Reduce the disk\nusage below the <<cluster-routing-watermark-low,low watermark>> before restarting nodes.\n====\n\n[discrete]\n[[restart-cluster-full]]\n=== Full-cluster restart\n\n// tag::disable_shard_alloc[]\n. *Disable shard allocation.*\n+\n--\ninclude::{es-ref-dir}/upgrade/disable-shard-alloc.asciidoc[]\n--\n// end::disable_shard_alloc[]\n\n. *Stop indexing and perform a flush.*\n+\n--\nPerforming a <<indices-flush, flush>> speeds up shard recovery.\n\n[source,console]\n--------------------------------------------------\nPOST /_flush\n--------------------------------------------------\n--\n\n//tag::stop_ml[]\n. *Temporarily stop the tasks associated with active {ml} jobs and {dfeeds}.* (Optional)\n+\n--\n{ml-cap} features require specific {subscriptions}[subscriptions].\n\nYou have two options to handle {ml} jobs and {dfeeds} when you shut down a\ncluster:\n\n* Temporarily halt the tasks associated with your {ml} jobs and {dfeeds} and\nprevent new jobs from opening by using the\n<<ml-set-upgrade-mode,set upgrade mode API>>:\n+\n[source,console]\n--------------------------------------------------\nPOST _ml/set_upgrade_mode?enabled=true\n--------------------------------------------------\n// TEST\n+\nWhen you disable upgrade mode, the jobs resume using the last model state that\nwas automatically saved. This option avoids the overhead of managing active jobs\nduring the shutdown and is faster than explicitly stopping {dfeeds} and closing\njobs.\n\n* {ml-docs}/stopping-ml.html[Stop all {dfeeds} and close all jobs]. This option\nsaves the model state at the time of closure. When you reopen the jobs after the\ncluster restart, they use the exact same model. However, saving the latest model\nstate takes longer than using upgrade mode, especially if you have a lot of jobs\nor jobs with large model states.\n--\n// end::stop_ml[]\n\n. *Shut down all nodes.*\n+\n--\ninclude::{es-ref-dir}/upgrade/shut-down-node.asciidoc[]\n--\n\n. *Perform any needed changes.*\n\n. *Restart nodes.*\n+\n--\nIf you have dedicated master nodes, start them first and wait for them to\nform a cluster and elect a master before proceeding with your data nodes.\nYou can check progress by looking at the logs.\n\nAs soon as enough master-eligible nodes have discovered each other, they form a\ncluster and elect a master. At that point, you can use\nthe <<cat-health, cat health>> and <<cat-nodes,cat nodes>> APIs to monitor nodes\njoining the cluster:\n\n[source,console]\n--------------------------------------------------\nGET _cat/health\n\nGET _cat/nodes\n--------------------------------------------------\n// TEST[continued]\n\nThe `status` column returned by `_cat/health` shows the health of each node\nin the cluster: `red`, `yellow`, or `green`.\n--\n\n. *Wait for all nodes to join the cluster and report a status of yellow.*\n+\n--\nWhen a node joins the cluster, it begins to recover any primary shards that\nare stored locally. The <<cat-health,`_cat/health`>> API initially reports\na `status` of `red`, indicating that not all primary shards have been allocated.\n\nOnce a node recovers its local shards, the cluster `status` switches to\n`yellow`, indicating that all primary shards have been recovered, but not all\nreplica shards are allocated. This is to be expected because you have not yet\nre-enabled allocation. Delaying the allocation of replicas until all nodes\nare `yellow` allows the master to allocate replicas to nodes that\nalready have local shard copies.\n--\n\n. *Re-enable allocation.*\n+\n--\nWhen all nodes have joined the cluster and recovered their primary shards,\nre-enable allocation by restoring `cluster.routing.allocation.enable` to its\ndefault:\n\n[source,console]\n------------------------------------------------------\nPUT _cluster/settings\n{\n  \"persistent\": {\n    \"cluster.routing.allocation.enable\": null\n  }\n}\n------------------------------------------------------\n// TEST[continued]\n\nOnce allocation is re-enabled, the cluster starts allocating replica shards to\nthe data nodes. At this point it is safe to resume indexing and searching,\nbut your cluster will recover more quickly if you can wait until all primary\nand replica shards have been successfully allocated and the status of all nodes\nis `green`.\n\nYou can monitor progress with the <<cat-health,`_cat/health`>> and\n<<cat-recovery,`_cat/recovery`>> APIs:\n\n[source,console]\n--------------------------------------------------\nGET _cat/health\n\nGET _cat/recovery\n--------------------------------------------------\n// TEST[continued]\n--\n// tag::restart_ml[]\n. *Restart machine learning jobs.* (Optional)\n+\n--\nIf you temporarily halted the tasks associated with your {ml} jobs, use the\n<<ml-set-upgrade-mode,set upgrade mode API>> to return them to active states:\n\n[source,console]\n--------------------------------------------------\nPOST _ml/set_upgrade_mode?enabled=false\n--------------------------------------------------\n// TEST[continued]\n\nIf you closed all {ml} jobs before stopping the nodes, open the jobs and start\nthe datafeeds from {kib} or with the <<ml-open-job,open jobs>> and\n<<ml-start-datafeed,start datafeed>> APIs.\n--\n// end::restart_ml[]\n\n\n[discrete]\n[[restart-cluster-rolling]]\n=== Rolling restart\n\n\ninclude::{es-ref-dir}/setup/restart-cluster.asciidoc[tag=disable_shard_alloc]\n\n. *Stop non-essential indexing and perform a flush.* (Optional)\n+\n--\nWhile you can continue indexing during the rolling restart, shard recovery\ncan be faster if you temporarily stop non-essential indexing and perform a\n<<indices-flush, flush>>.\n\n[source,console]\n--------------------------------------------------\nPOST /_flush\n--------------------------------------------------\n--\n\ninclude::{es-ref-dir}/setup/restart-cluster.asciidoc[tag=stop_ml]\n+\n--\n* If you perform a rolling restart, you can also leave your machine learning\njobs running. When you shut down a machine learning node, its jobs automatically\nmove to another node and restore the model states. This option enables your jobs\nto continue running during the shutdown but it puts increased load on the\ncluster.\n--\n\n. *Shut down a single node in case of rolling restart.*\n+\n--\ninclude::{es-ref-dir}/upgrade/shut-down-node.asciidoc[]\n--\n\n. *Perform any needed changes.*\n\n. *Restart the node you changed.*\n+\n--\nStart the node and confirm that it joins the cluster by checking the log file or\nby submitting a `_cat/nodes` request:\n\n[source,console]\n--------------------------------------------------\nGET _cat/nodes\n--------------------------------------------------\n// TEST[continued]\n--\n\n. *Reenable shard allocation.*\n+\n--\nFor data nodes, once the node has joined the cluster, remove the\n`cluster.routing.allocation.enable` setting to enable shard allocation and start\nusing the node:\n\n[source,console]\n--------------------------------------------------\nPUT _cluster/settings\n{\n  \"persistent\": {\n    \"cluster.routing.allocation.enable\": null\n  }\n}\n--------------------------------------------------\n// TEST[continued]\n--\n\n. *Repeat in case of rolling restart.*\n+\n--\nWhen the node has recovered and the cluster is stable, repeat these steps\nfor each node that needs to be changed.\n--\n\ninclude::{es-ref-dir}/setup/restart-cluster.asciidoc[tag=restart_ml]\n"
}