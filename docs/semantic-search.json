{
    "meta": {
        "size": 7661,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/semantic-search.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "semantic-search",
        "version": "8.15"
    },
    "doc": "[[semantic-search]]\n== Semantic search\n\nSemantic search is a search method that helps you find data based on the intent and contextual meaning of a search query, instead of a match on query terms (lexical search).\n\n{es} provides various semantic search capabilities using {ml-docs}/ml-nlp.html[natural language processing (NLP)] and vector search.\nUsing an NLP model enables you to extract text embeddings out of text.\nEmbeddings are vectors that provide a numeric representation of a text.\nPieces of content with similar meaning have similar representations.\n\nimage::images/semantic-options.svg[Overview of semantic search workflows in {es}]\n\nYou have several options for using NLP models in the {stack}:\n\n* use the `semantic_text` workflow (recommended)\n* use the {infer} API workflow\n* deploy models directly in {es}\n\nRefer to <<using-nlp-models,this section>> to choose your workflow.\n\nYou can also store your own embeddings in {es} as vectors.\nRefer to <<using-query,this section>> for guidance on which query type to use for semantic search.\n\nAt query time, {es} can use the same NLP model to convert a query into embeddings, enabling you to find documents with similar text embeddings.\n\n\n[discrete]\n[[using-nlp-models]]\n=== Choose a semantic search workflow\n\n[discrete]\n==== `semantic_text` workflow\n\nThe simplest way to use NLP models in the {stack} is through the <<semantic-search-semantic-text, `semantic_text` workflow>>.\nWe recommend using this approach because it abstracts away a lot of manual work.\nAll you need to do is create an {infer} endpoint and an index mapping to start ingesting, embedding, and querying data.\nThere is no need to define model-related settings and parameters, or to create {infer} ingest pipelines.\nRefer to the <<put-inference-api, Create an {infer} endpoint API>> documentation for a list of supported services.\n\nThe <<semantic-search-semantic-text, Semantic search with `semantic_text`>> tutorial shows you the process end-to-end.\n\n[discrete]\n==== {infer} API workflow\n\nThe <<semantic-search-inference, {infer} API workflow>> is more complex but offers greater control over the {infer} endpoint configuration.\nYou need to create an {infer} endpoint, provide various model-related settings and parameters, define an index mapping, and set up an {infer} ingest pipeline with the appropriate settings.\n\nThe <<semantic-search-inference, Semantic search with the {infer} API>> tutorial shows you the process end-to-end.\n\n[discrete]\n==== Model deployment workflow\n\nYou can also deploy NLP in {es} manually, without using an {infer} endpoint.\nThis is the most complex and labor intensive workflow for performing semantic search in the {stack}.\nYou need to select an NLP model from the {ml-docs}/ml-nlp-model-ref.html#ml-nlp-model-ref-text-embedding[list of supported dense and sparse vector models], deploy it using the Eland client, create an index mapping, and set up a suitable ingest pipeline to start ingesting and querying data.\n\nThe <<semantic-search-deployed-nlp-model, Semantic search with a model deployed in {es}>> tutorial shows you the process end-to-end.\n\n\n[discrete]\n[[using-query]]\n=== Using the right query\n\nCrafting the right query is crucial for semantic search.\nWhich query you use and which field you target in your queries depends on your chosen workflow.\nIf you're using the `semantic_text` workflow it's quite simple.\nIf not, it depends on which type of embeddings you're working with.\n\n[cols=\"30%, 30%, 40%\", options=\"header\"]\n|=======================================================================================================================================================================================================\n| Field type to query                    | Query to use                                      | Notes                                                                                                                                                             \n| <<semantic-text,`semantic_text`>>      | <<query-dsl-semantic-query,`semantic`>>           | The `semantic_text` field handles generating embeddings for you at index time and query time.                                                               \n| <<sparse-vector,`sparse_vector`>>      | <<query-dsl-sparse-vector-query,`sparse_vector`>> | The `sparse_vector` query can generate query embeddings for you, but you can also provide your own. You must provide embeddings at index time.\n| <<dense-vector,`dense_vector`>>        | <<query-dsl-knn-query,`knn`>>                     | The `knn` query can generate query embeddings for you, but you can also provide your own. You must provide embeddings at index time.\n|=======================================================================================================================================================================================================\n\nIf you want {es} to generate embeddings at both index and query time, use the `semantic_text` field and the `semantic` query.\nIf you want to bring your own embeddings, use the `sparse_vector` or `dense_vector` field type and the associated query depending on the NLP model you used to generate the embeddings.\n\nIMPORTANT: For the easiest way to perform semantic search in the {stack}, refer to the <<semantic-search-semantic-text, `semantic_text`>> end-to-end tutorial.\n\n\n[discrete]\n[[semantic-search-read-more]]\n=== Read more\n\n* Tutorials:\n** <<semantic-search-semantic-text, Semantic search with `semantic_text`>>\n** <<semantic-search-inference, Semantic search with the {infer} API>>\n** <<semantic-search-elser,Semantic search with ELSER>> using the model deployment workflow\n** <<semantic-search-deployed-nlp-model, Semantic search with a model deployed in {es}>>\n** {ml-docs}/ml-nlp-text-emb-vector-search-example.html[Semantic search with the msmarco-MiniLM-L-12-v3 sentence-transformer model]\n* Interactive examples:\n** The https://github.com/elastic/elasticsearch-labs[`elasticsearch-labs`] repo contains a number of interactive semantic search examples in the form of executable Python notebooks, using the {es} Python client\n** https://github.com/elastic/elasticsearch-labs/blob/main/notebooks/search/03-ELSER.ipynb[Semantic search with ELSER using the model deployment workflow]\n** https://github.com/elastic/elasticsearch-labs/blob/main/notebooks/search/09-semantic-text.ipynb[Semantic search with `semantic_text`]\n* Blogs:\n** https://www.elastic.co/search-labs/blog/semantic-search-simplified-semantic-text[{es} new semantic_text mapping: Simplifying semantic search]\n** {blog-ref}may-2023-launch-sparse-encoder-ai-model[Introducing Elastic Learned Sparse Encoder: Elastic's AI model for semantic search]\n** {blog-ref}lexical-ai-powered-search-elastic-vector-database[How to get the best of lexical and AI-powered search with Elastic's vector database]\n** Information retrieval blog series:\n*** {blog-ref}improving-information-retrieval-elastic-stack-search-relevance[Part 1: Steps to improve search relevance]\n*** {blog-ref}improving-information-retrieval-elastic-stack-benchmarking-passage-retrieval[Part 2: Benchmarking passage retrieval]\n*** {blog-ref}may-2023-launch-information-retrieval-elasticsearch-ai-model[Part 3: Introducing Elastic Learned Sparse Encoder, our new retrieval model]\n*** {blog-ref}improving-information-retrieval-elastic-stack-hybrid[Part 4: Hybrid retrieval]\n\n\ninclude::semantic-search-semantic-text.asciidoc[]\ninclude::semantic-text-hybrid-search[]\ninclude::semantic-search-inference.asciidoc[]\ninclude::semantic-search-elser.asciidoc[]\ninclude::cohere-es.asciidoc[]\ninclude::semantic-search-deploy-model.asciidoc[]\ninclude::ingest-vectors.asciidoc[]\n"
}