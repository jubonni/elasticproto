{
    "meta": {
        "timestamp": "2024-11-01T03:02:52.018603",
        "size": 13480,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-nori.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "analysis-nori",
        "version": "8.15"
    },
    "doc": "[[analysis-nori]]\n=== Korean (nori) analysis plugin\n\nThe Korean (nori) Analysis plugin integrates Lucene nori analysis\nmodule into elasticsearch. It uses the https://bitbucket.org/eunjeon/mecab-ko-dic[mecab-ko-dic dictionary]\nto perform morphological analysis of Korean texts.\n\n:plugin_name: analysis-nori\ninclude::install_remove.asciidoc[]\n\n[[analysis-nori-analyzer]]\n==== `nori` analyzer\n\nThe `nori` analyzer consists of the following tokenizer and token filters:\n\n* <<analysis-nori-tokenizer,`nori_tokenizer`>>\n* <<analysis-nori-speech,`nori_part_of_speech`>> token filter\n* <<analysis-nori-readingform,`nori_readingform`>> token filter\n* {ref}/analysis-lowercase-tokenfilter.html[`lowercase`] token filter\n\nIt supports the `decompound_mode` and `user_dictionary` settings from\n<<analysis-nori-tokenizer,`nori_tokenizer`>> and the `stoptags` setting from\n<<analysis-nori-speech,`nori_part_of_speech`>>.\n\n[[analysis-nori-tokenizer]]\n==== `nori_tokenizer`\n\nThe `nori_tokenizer` accepts the following settings:\n\n`decompound_mode`::\n+\n--\n\nThe decompound mode determines how the tokenizer handles compound tokens.\nIt can be set to:\n\n`none`::\n\n    No decomposition for compounds. Example output:\n\n    \uac00\uac70\ub3c4\ud56d\n    \uac00\uace1\uc5ed\n\n`discard`::\n\n    Decomposes compounds and discards the original form (*default*). Example output:\n\n    \uac00\uace1\uc5ed => \uac00\uace1, \uc5ed\n\n`mixed`::\n\n    Decomposes compounds and keeps the original form. Example output:\n\n    \uac00\uace1\uc5ed => \uac00\uace1\uc5ed, \uac00\uace1, \uc5ed\n--\n\n`discard_punctuation`::\n\n    Whether punctuation should be discarded from the output. Defaults to `true`.\n\n`lenient`::\n\n    Whether the `user_dictionary` should be deduplicated on the provided `text`.\n    False by default causing duplicates to generate an error.\n\n`user_dictionary`::\n+\n--\nThe Nori tokenizer uses the https://bitbucket.org/eunjeon/mecab-ko-dic[mecab-ko-dic dictionary] by default.\nA `user_dictionary` with custom nouns (`NNG`) may be appended to the default dictionary.\nThe dictionary should have the following format:\n\n[source,txt]\n-----------------------\n<token> [<token 1> ... <token n>]\n-----------------------\n\nThe first token is mandatory and represents the custom noun that should be added in\nthe dictionary. For compound nouns the custom segmentation can be provided\nafter the first token (`[<token 1> ... <token n>]`). The segmentation of the\ncustom compound nouns is controlled by the `decompound_mode` setting.\n\n\nAs a demonstration of how the user dictionary can be used, save the following\ndictionary to `$ES_HOME/config/userdict_ko.txt`:\n\n[source,txt]\n-----------------------\nc++                 <1>\nC\uc060\uc060\n\uc138\uc885\n\uc138\uc885\uc2dc \uc138\uc885 \uc2dc        <2>\n-----------------------\n\n<1> A simple noun\n<2> A compound noun (`\uc138\uc885\uc2dc`) followed by its decomposition: `\uc138\uc885` and `\uc2dc`.\n\nThen create an analyzer as follows:\n\n[source,console]\n--------------------------------------------------\nPUT nori_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"tokenizer\": {\n          \"nori_user_dict\": {\n            \"type\": \"nori_tokenizer\",\n            \"decompound_mode\": \"mixed\",\n            \"discard_punctuation\": \"false\",\n            \"user_dictionary\": \"userdict_ko.txt\",\n            \"lenient\": \"true\"\n          }\n        },\n        \"analyzer\": {\n          \"my_analyzer\": {\n            \"type\": \"custom\",\n            \"tokenizer\": \"nori_user_dict\"\n          }\n        }\n      }\n    }\n  }\n}\n\nGET nori_sample/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"\uc138\uc885\uc2dc\"  <1>\n}\n--------------------------------------------------\n\n<1> Sejong city\n\nThe above `analyze` request returns the following:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"tokens\" : [ {\n    \"token\" : \"\uc138\uc885\uc2dc\",\n    \"start_offset\" : 0,\n    \"end_offset\" : 3,\n    \"type\" : \"word\",\n    \"position\" : 0,\n    \"positionLength\" : 2    <1>\n  }, {\n    \"token\" : \"\uc138\uc885\",\n    \"start_offset\" : 0,\n    \"end_offset\" : 2,\n    \"type\" : \"word\",\n    \"position\" : 0\n  }, {\n    \"token\" : \"\uc2dc\",\n    \"start_offset\" : 2,\n    \"end_offset\" : 3,\n    \"type\" : \"word\",\n    \"position\" : 1\n   }]\n}\n--------------------------------------------------\n\n<1> This is a compound token that spans two positions (`mixed` mode).\n--\n\n`user_dictionary_rules`::\n+\n--\n\nYou can also inline the rules directly in the tokenizer definition using\nthe `user_dictionary_rules` option:\n\n[source,console]\n--------------------------------------------------\nPUT nori_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"tokenizer\": {\n          \"nori_user_dict\": {\n            \"type\": \"nori_tokenizer\",\n            \"decompound_mode\": \"mixed\",\n            \"user_dictionary_rules\": [\"c++\", \"C\uc060\uc060\", \"\uc138\uc885\", \"\uc138\uc885\uc2dc \uc138\uc885 \uc2dc\"]\n          }\n        },\n        \"analyzer\": {\n          \"my_analyzer\": {\n            \"type\": \"custom\",\n            \"tokenizer\": \"nori_user_dict\"\n          }\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n--\n\nThe `nori_tokenizer` sets a number of additional attributes per token that are used by token filters\nto modify the stream.\nYou can view all these additional attributes with the following request:\n\n[source,console]\n--------------------------------------------------\nGET _analyze\n{\n  \"tokenizer\": \"nori_tokenizer\",\n  \"text\": \"\ubfcc\ub9ac\uac00 \uae4a\uc740 \ub098\ubb34\ub294\",   <1>\n  \"attributes\" : [\"posType\", \"leftPOS\", \"rightPOS\", \"morphemes\", \"reading\"],\n  \"explain\": true\n}\n--------------------------------------------------\n\n<1> A tree with deep roots\n\nWhich responds with:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"detail\": {\n    \"custom_analyzer\": true,\n    \"charfilters\": [],\n    \"tokenizer\": {\n      \"name\": \"nori_tokenizer\",\n      \"tokens\": [\n        {\n          \"token\": \"\ubfcc\ub9ac\",\n          \"start_offset\": 0,\n          \"end_offset\": 2,\n          \"type\": \"word\",\n          \"position\": 0,\n          \"leftPOS\": \"NNG(General Noun)\",\n          \"morphemes\": null,\n          \"posType\": \"MORPHEME\",\n          \"reading\": null,\n          \"rightPOS\": \"NNG(General Noun)\"\n        },\n        {\n          \"token\": \"\uac00\",\n          \"start_offset\": 2,\n          \"end_offset\": 3,\n          \"type\": \"word\",\n          \"position\": 1,\n          \"leftPOS\": \"JKS(Subject case marker)\",\n          \"morphemes\": null,\n          \"posType\": \"MORPHEME\",\n          \"reading\": null,\n          \"rightPOS\": \"JKS(Subject case marker)\"\n        },\n        {\n          \"token\": \"\uae4a\",\n          \"start_offset\": 4,\n          \"end_offset\": 5,\n          \"type\": \"word\",\n          \"position\": 2,\n          \"leftPOS\": \"VA(Adjective)\",\n          \"morphemes\": null,\n          \"posType\": \"MORPHEME\",\n          \"reading\": null,\n          \"rightPOS\": \"VA(Adjective)\"\n        },\n        {\n          \"token\": \"\uc740\",\n          \"start_offset\": 5,\n          \"end_offset\": 6,\n          \"type\": \"word\",\n          \"position\": 3,\n          \"leftPOS\": \"ETM(Adnominal form transformative ending)\",\n          \"morphemes\": null,\n          \"posType\": \"MORPHEME\",\n          \"reading\": null,\n          \"rightPOS\": \"ETM(Adnominal form transformative ending)\"\n        },\n        {\n          \"token\": \"\ub098\ubb34\",\n          \"start_offset\": 7,\n          \"end_offset\": 9,\n          \"type\": \"word\",\n          \"position\": 4,\n          \"leftPOS\": \"NNG(General Noun)\",\n          \"morphemes\": null,\n          \"posType\": \"MORPHEME\",\n          \"reading\": null,\n          \"rightPOS\": \"NNG(General Noun)\"\n        },\n        {\n          \"token\": \"\ub294\",\n          \"start_offset\": 9,\n          \"end_offset\": 10,\n          \"type\": \"word\",\n          \"position\": 5,\n          \"leftPOS\": \"JX(Auxiliary postpositional particle)\",\n          \"morphemes\": null,\n          \"posType\": \"MORPHEME\",\n          \"reading\": null,\n          \"rightPOS\": \"JX(Auxiliary postpositional particle)\"\n        }\n      ]\n    },\n    \"tokenfilters\": []\n  }\n}\n--------------------------------------------------\n\n[[analysis-nori-speech]]\n==== `nori_part_of_speech` token filter\n\nThe `nori_part_of_speech` token filter removes tokens that match a set of\npart-of-speech tags. The list of supported tags and their meanings can be found here:\n{lucene-core-javadoc}/../analysis/nori/org/apache/lucene/analysis/ko/POS.Tag.html[Part of speech tags]\n\nIt accepts the following setting:\n\n`stoptags`::\n\n    An array of part-of-speech tags that should be removed.\n\nand defaults to:\n\n[source,js]\n--------------------------------------------------\n\"stoptags\": [\n    \"E\",\n    \"IC\",\n    \"J\",\n    \"MAG\", \"MAJ\", \"MM\",\n    \"SP\", \"SSC\", \"SSO\", \"SC\", \"SE\",\n    \"XPN\", \"XSA\", \"XSN\", \"XSV\",\n    \"UNA\", \"NA\", \"VSV\"\n]\n--------------------------------------------------\n// NOTCONSOLE\n\nFor example:\n\n[source,console]\n--------------------------------------------------\nPUT nori_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"my_analyzer\": {\n            \"tokenizer\": \"nori_tokenizer\",\n            \"filter\": [\n              \"my_posfilter\"\n            ]\n          }\n        },\n        \"filter\": {\n          \"my_posfilter\": {\n            \"type\": \"nori_part_of_speech\",\n            \"stoptags\": [\n              \"NR\"   <1>\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n\nGET nori_sample/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"\uc5ec\uc12f \uc6a9\uc774\"  <2>\n}\n--------------------------------------------------\n\n<1> Korean numerals should be removed (`NR`)\n<2> Six dragons\n\nWhich responds with:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"tokens\" : [ {\n    \"token\" : \"\uc6a9\",\n    \"start_offset\" : 3,\n    \"end_offset\" : 4,\n    \"type\" : \"word\",\n    \"position\" : 1\n  }, {\n    \"token\" : \"\uc774\",\n    \"start_offset\" : 4,\n    \"end_offset\" : 5,\n    \"type\" : \"word\",\n    \"position\" : 2\n  } ]\n}\n--------------------------------------------------\n\n\n[[analysis-nori-readingform]]\n==== `nori_readingform` token filter\n\nThe `nori_readingform` token filter rewrites tokens written in Hanja to their Hangul form.\n\n[source,console]\n--------------------------------------------------\nPUT nori_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"my_analyzer\": {\n            \"tokenizer\": \"nori_tokenizer\",\n            \"filter\": [ \"nori_readingform\" ]\n          }\n        }\n      }\n    }\n  }\n}\n\nGET nori_sample/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"\u9115\u6b4c\"      <1>\n}\n--------------------------------------------------\n\n<1> A token written in Hanja: Hyangga\n\nWhich responds with:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"tokens\" : [ {\n    \"token\" : \"\ud5a5\uac00\",     <1>\n    \"start_offset\" : 0,\n    \"end_offset\" : 2,\n    \"type\" : \"word\",\n    \"position\" : 0\n  }]\n}\n--------------------------------------------------\n\n<1> The Hanja form is replaced by the Hangul translation.\n\n\n[[analysis-nori-number]]\n==== `nori_number` token filter\n\nThe `nori_number` token filter normalizes Korean numbers\nto regular Arabic decimal numbers in half-width characters.\n\nKorean numbers are often written using a combination of Hangul and Arabic numbers with various kinds of punctuation.\nFor example, \uff13\uff0e\uff12\ucc9c means 3200.\nThis filter does this kind of normalization and allows a search for 3200 to match \uff13\uff0e\uff12\ucc9c in text,\nbut can also be used to make range facets based on the normalized numbers and so on.\n\n[NOTE]\n====\nNotice that this analyzer uses a token composition scheme and relies on punctuation tokens\nbeing found in the token stream.\nPlease make sure your `nori_tokenizer` has `discard_punctuation` set to false.\nIn case punctuation characters, such as U+FF0E(\uff0e), is removed from the token stream,\nthis filter would find input tokens \uff13 and \uff12\ucc9c and give outputs 3 and 2000 instead of 3200,\nwhich is likely not the intended result.\n\nIf you want to remove punctuation characters from your index that are not part of normalized numbers,\nadd a `stop` token filter with the punctuation you wish to remove after `nori_number` in your analyzer chain.\n====\nBelow are some examples of normalizations this filter supports.\nThe input is untokenized text and the result is the single term attribute emitted for the input.\n\n- \uc601\uc601\uce60 -> 7\n- \uc77c\uc601\uc601\uc601 -> 1000\n- \uc0bc\ucc9c2\ubc312\uc2ed\uc0bc -> 3223\n- \uc870\uc721\ubc31\ub9cc\uc624\ucc9c\uc77c -> 1000006005001\n- \uff13.\uff12\ucc9c ->  3200\n- \uff11.\uff12\ub9cc\uff13\uff14\uff15.\uff16\uff17 -> 12345.67\n- 4,647.100 -> 4647.1\n- 15,7 -> 157 (be aware of this weakness)\n\nFor example:\n\n[source,console]\n--------------------------------------------------\nPUT nori_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"my_analyzer\": {\n            \"tokenizer\": \"tokenizer_discard_puncuation_false\",\n            \"filter\": [\n              \"part_of_speech_stop_sp\", \"nori_number\"\n            ]\n          }\n        },\n        \"tokenizer\": {\n          \"tokenizer_discard_puncuation_false\": {\n            \"type\": \"nori_tokenizer\",\n            \"discard_punctuation\": \"false\"\n          }\n        },\n        \"filter\": {\n            \"part_of_speech_stop_sp\": {\n                \"type\": \"nori_part_of_speech\",\n                \"stoptags\": [\"SP\"]\n            }\n        }\n      }\n    }\n  }\n}\n\nGET nori_sample/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"\uc2ed\ub9cc\uc774\ucc9c\uc624\ubc31\uacfc \uff13.\uff12\ucc9c\"\n}\n--------------------------------------------------\n\nWhich results in:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"tokens\" : [{\n    \"token\" : \"102500\",\n    \"start_offset\" : 0,\n    \"end_offset\" : 6,\n    \"type\" : \"word\",\n    \"position\" : 0\n  }, {\n    \"token\" : \"\uacfc\",\n    \"start_offset\" : 6,\n    \"end_offset\" : 7,\n    \"type\" : \"word\",\n    \"position\" : 1\n  }, {\n    \"token\" : \"3200\",\n    \"start_offset\" : 8,\n    \"end_offset\" : 12,\n    \"type\" : \"word\",\n    \"position\" : 2\n  }]\n}\n--------------------------------------------------\n"
}