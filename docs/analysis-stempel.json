{
    "meta": {
        "size": 2747,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stempel.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "analysis-stempel",
        "version": "8.15"
    },
    "doc": "[[analysis-stempel]]\n=== Stempel Polish analysis plugin\n\nThe Stempel analysis plugin integrates Lucene's Stempel analysis\nmodule for Polish into elasticsearch.\n\n:plugin_name: analysis-stempel\ninclude::install_remove.asciidoc[]\n\n[[analysis-stempel-tokenizer]]\n[discrete]\n==== `stempel` tokenizer and token filters\n\nThe plugin provides the `polish` analyzer and the `polish_stem` and `polish_stop` token filters,\nwhich are not configurable.\n\n==== Reimplementing and extending the analyzers\n\nThe `polish` analyzer could be reimplemented as a `custom` analyzer that can\nthen be extended and configured differently as follows:\n\n[source,console]\n----------------------------------------------------\nPUT /stempel_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"rebuilt_stempel\": {\n          \"tokenizer\":  \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"polish_stop\",\n            \"polish_stem\"\n          ]\n        }\n      }\n    }\n  }\n}\n----------------------------------------------------\n// TEST[s/\\n$/\\nstartyaml\\n  - compare_analyzers: {index: stempel_example, first: polish, second: rebuilt_stempel}\\nendyaml\\n/]\n\n[[analysis-polish-stop]]\n==== `polish_stop` token filter\n\nThe `polish_stop` token filter filters out Polish stopwords (`_polish_`), and\nany other custom stopwords specified by the user. This filter only supports\nthe predefined `_polish_` stopwords list. If you want to use a different\npredefined list, then use the\n{ref}/analysis-stop-tokenfilter.html[`stop` token filter] instead.\n\n[source,console]\n--------------------------------------------------\nPUT /polish_stop_example\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"analyzer_with_stop\": {\n            \"tokenizer\": \"standard\",\n            \"filter\": [\n              \"lowercase\",\n              \"polish_stop\"\n            ]\n          }\n        },\n        \"filter\": {\n          \"polish_stop\": {\n            \"type\": \"polish_stop\",\n            \"stopwords\": [\n              \"_polish_\",\n              \"je\u015b\u0107\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n\nGET polish_stop_example/_analyze\n{\n  \"analyzer\": \"analyzer_with_stop\",\n  \"text\": \"Gdzie kucharek sze\u015b\u0107, tam nie ma co je\u015b\u0107.\"\n}\n--------------------------------------------------\n\nThe above request returns:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"tokens\" : [\n    {\n      \"token\" : \"kucharek\",\n      \"start_offset\" : 6,\n      \"end_offset\" : 14,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 1\n    },\n    {\n      \"token\" : \"sze\u015b\u0107\",\n      \"start_offset\" : 15,\n      \"end_offset\" : 20,\n      \"type\" : \"<ALPHANUM>\",\n      \"position\" : 2\n    }\n  ]\n}\n--------------------------------------------------\n"
}