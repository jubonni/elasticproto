{
    "meta": {
        "timestamp": "2024-11-01T03:07:10.204272",
        "size": 6403,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/infer-service-azure-ai-studio.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "infer-service-azure-ai-studio",
        "version": "8.15"
    },
    "doc": "[[infer-service-azure-ai-studio]]\n=== Azure AI studio {infer} service\n\nCreates an {infer} endpoint to perform an {infer} task with the `azureaistudio` service.\n\n\n[discrete]\n[[infer-service-azure-ai-studio-api-request]]\n==== {api-request-title}\n\n`PUT /_inference/<task_type>/<inference_id>`\n\n[discrete]\n[[infer-service-azure-ai-studio-api-path-params]]\n==== {api-path-parms-title}\n\n`<inference_id>`::\n(Required, string)\ninclude::inference-shared.asciidoc[tag=inference-id]\n\n`<task_type>`::\n(Required, string)\ninclude::inference-shared.asciidoc[tag=task-type]\n+\n--\nAvailable task types:\n\n* `completion`,\n* `text_embedding`.\n--\n\n[discrete]\n[[infer-service-azure-ai-studio-api-request-body]]\n==== {api-request-body-title}\n\n`chunking_settings`::\n(Optional, object)\ninclude::inference-shared.asciidoc[tag=chunking-settings]\n\n`max_chunking_size`:::\n(Optional, integer)\ninclude::inference-shared.asciidoc[tag=chunking-settings-max-chunking-size]\n\n`overlap`:::\n(Optional, integer)\ninclude::inference-shared.asciidoc[tag=chunking-settings-overlap]\n\n`sentence_overlap`:::\n(Optional, integer)\ninclude::inference-shared.asciidoc[tag=chunking-settings-sentence-overlap]\n\n`strategy`:::\n(Optional, string)\ninclude::inference-shared.asciidoc[tag=chunking-settings-strategy]\n\n`service`::\n(Required, string)\nThe type of service supported for the specified task type. In this case,\n`azureaistudio`.\n\n`service_settings`::\n(Required, object)\ninclude::inference-shared.asciidoc[tag=service-settings]\n+\n--\nThese settings are specific to the `azureaistudio` service.\n--\n\n`api_key`:::\n(Required, string)\nA valid API key of your Azure AI Studio model deployment.\nThis key can be found on the overview page for your deployment in the management section of your https://ai.azure.com/[Azure AI Studio] account.\n+\n--\ninclude::inference-shared.asciidoc[tag=api-key-admonition]\n--\n\n`target`:::\n(Required, string)\nThe target URL of your Azure AI Studio model deployment.\nThis can be found on the overview page for your deployment in the management section of your https://ai.azure.com/[Azure AI Studio] account.\n\n`provider`:::\n(Required, string)\nThe model provider for your deployment.\nNote that some providers may support only certain task types.\nSupported providers include:\n\n* `cohere` - available for `text_embedding` and `completion` task types\n* `databricks` - available for `completion` task type only\n* `meta` - available for `completion` task type only\n* `microsoft_phi` - available for `completion` task type only\n* `mistral` - available for `completion` task type only\n* `openai` - available for `text_embedding` and `completion` task types\n\n`endpoint_type`:::\n(Required, string)\nOne of `token` or `realtime`.\nSpecifies the type of endpoint that is used in your model deployment.\nThere are https://learn.microsoft.com/en-us/azure/ai-studio/concepts/deployments-overview#billing-for-deploying-and-inferencing-llms-in-azure-ai-studio[two endpoint types available] for deployment through Azure AI Studio.\n\"Pay as you go\" endpoints are billed per token.\nFor these, you must specify `token` for your `endpoint_type`.\nFor \"real-time\" endpoints which are billed per hour of usage, specify `realtime`.\n\n`rate_limit`:::\n(Optional, object)\nBy default, the `azureaistudio` service sets the number of requests allowed per minute to `240`.\nThis helps to minimize the number of rate limit errors returned from Azure AI Studio.\nTo modify this, set the `requests_per_minute` setting of this object in your service settings:\n+\n--\ninclude::inference-shared.asciidoc[tag=request-per-minute-example]\n--\n\n`task_settings`::\n(Optional, object)\ninclude::inference-shared.asciidoc[tag=task-settings]\n+\n.`task_settings` for the `completion` task type\n[%collapsible%closed]\n=====\n`do_sample`:::\n(Optional, float)\nInstructs the inference process to perform sampling or not.\nHas no effect unless `temperature` or `top_p` is specified.\n\n`max_new_tokens`:::\n(Optional, integer)\nProvides a hint for the maximum number of output tokens to be generated.\nDefaults to 64.\n\n`temperature`:::\n(Optional, float)\nA number in the range of 0.0 to 2.0 that specifies the sampling temperature to use that controls the apparent creativity of generated completions.\nShould not be used if `top_p` is specified.\n\n`top_p`:::\n(Optional, float)\nA number in the range of 0.0 to 2.0 that is an alternative value to temperature that causes the model to consider the results of the tokens with nucleus sampling probability.\nShould not be used if `temperature` is specified.\n=====\n+\n.`task_settings` for the `text_embedding` task type\n[%collapsible%closed]\n=====\n`user`:::\n(optional, string)\nSpecifies the user issuing the request, which can be used for abuse detection.\n=====\n\n\n[discrete]\n[[inference-example-azureaistudio]]\n==== Azure AI Studio service example\n\nThe following example shows how to create an {infer} endpoint called `azure_ai_studio_embeddings` to perform a `text_embedding` task type.\nNote that we do not specify a model here, as it is defined already via our Azure AI Studio deployment.\n\nThe list of embeddings models that you can choose from in your deployment can be found in the https://ai.azure.com/explore/models?selectedTask=embeddings[Azure AI Studio model explorer].\n\n[source,console]\n------------------------------------------------------------\nPUT _inference/text_embedding/azure_ai_studio_embeddings\n{\n    \"service\": \"azureaistudio\",\n    \"service_settings\": {\n        \"api_key\": \"<api_key>\",\n        \"target\": \"<target_uri>\",\n        \"provider\": \"<model_provider>\",\n        \"endpoint_type\": \"<endpoint_type>\"\n    }\n}\n------------------------------------------------------------\n// TEST[skip:TBD]\n\nThe next example shows how to create an {infer} endpoint called `azure_ai_studio_completion` to perform a `completion` task type.\n\n[source,console]\n------------------------------------------------------------\nPUT _inference/completion/azure_ai_studio_completion\n{\n    \"service\": \"azureaistudio\",\n    \"service_settings\": {\n        \"api_key\": \"<api_key>\",\n        \"target\": \"<target_uri>\",\n        \"provider\": \"<model_provider>\",\n        \"endpoint_type\": \"<endpoint_type>\"\n    }\n}\n------------------------------------------------------------\n// TEST[skip:TBD]\n\nThe list of chat completion models that you can choose from in your deployment can be found in the https://ai.azure.com/explore/models?selectedTask=chat-completion[Azure AI Studio model explorer].\n"
}