{
    "meta": {
        "size": 21512,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/repo-analysis-api.html",
        "type": "documentation",
        "role": [
            "xpack",
            "child_attributes"
        ],
        "has_code": false,
        "title": "repo-analysis-api",
        "version": "8.15"
    },
    "doc": "[role=\"xpack\"]\n[[repo-analysis-api]]\n=== Repository analysis API\n++++\n<titleabbrev>Repository analysis</titleabbrev>\n++++\n\nAnalyzes a repository, reporting its performance characteristics and any\nincorrect behaviour found.\n\n////\n[source,console]\n----\nPUT /_snapshot/my_repository\n{\n  \"type\": \"fs\",\n  \"settings\": {\n    \"location\": \"my_backup_location\"\n  }\n}\n----\n// TESTSETUP\n////\n\n[source,console]\n----\nPOST /_snapshot/my_repository/_analyze?blob_count=10&max_blob_size=1mb&timeout=120s\n----\n\n[[repo-analysis-api-request]]\n==== {api-request-title}\n\n`POST /_snapshot/<repository>/_analyze`\n\n[[repo-analysis-api-prereqs]]\n==== {api-prereq-title}\n\n* If the {es} {security-features} are enabled, you must have the `manage`\n<<privileges-list-cluster,cluster privilege>> to use this API. For more\ninformation, see <<security-privileges>>.\n\n[[repo-analysis-api-desc]]\n==== {api-description-title}\n\nThere are a large number of third-party storage systems available, not all of\nwhich are suitable for use as a snapshot repository by {es}. Some storage\nsystems behave incorrectly, or perform poorly, especially when accessed\nconcurrently by multiple clients as the nodes of an {es} cluster do.\n\nThe Repository analysis API performs a collection of read and write operations\non your repository which are designed to detect incorrect behaviour and to\nmeasure the performance characteristics of your storage system.\n\nThe default values for the parameters to this API are deliberately low to reduce\nthe impact of running an analysis inadvertently and to provide a sensible\nstarting point for your investigations. Run your first analysis with the default\nparameter values to check for simple problems. If successful, run a sequence of\nincreasingly large analyses until you encounter a failure or you reach a\n`blob_count` of at least `2000`, a `max_blob_size` of at least `2gb`, a\n`max_total_data_size` of at least `1tb`, and a `register_operation_count` of at\nleast `100`. Always specify a generous timeout, possibly `1h` or longer, to\nallow time for each analysis to run to completion. Perform the analyses using a\nmulti-node cluster of a similar size to your production cluster so that it can\ndetect any problems that only arise when the repository is accessed by many\nnodes at once.\n\nIf the analysis fails then {es} detected that your repository behaved\nunexpectedly. This usually means you are using a third-party storage system\nwith an incorrect or incompatible implementation of the API it claims to\nsupport. If so, this storage system is not suitable for use as a snapshot\nrepository. You will need to work with the supplier of your storage system to\naddress the incompatibilities that {es} detects. See\n<<self-managed-repo-types>> for more information.\n\nIf the analysis is successful this API returns details of the testing process,\noptionally including how long each operation took. You can use this information\nto determine the performance of your storage system. If any operation fails or\nreturns an incorrect result, this API returns an error. If the API returns an\nerror then it may not have removed all the data it wrote to the repository. The\nerror will indicate the location of any leftover data, and this path is also\nrecorded in the {es} logs. You should verify yourself that this location has\nbeen cleaned up correctly. If there is still leftover data at the specified\nlocation then you should manually remove it.\n\nIf the connection from your client to {es} is closed while the client is\nwaiting for the result of the analysis then the test is cancelled. Some clients\nare configured to close their connection if no response is received within a\ncertain timeout. An analysis takes a long time to complete so you may need to\nrelax any such client-side timeouts. On cancellation the analysis attempts to\nclean up the data it was writing, but it may not be able to remove it all. The\npath to the leftover data is recorded in the {es} logs. You should verify\nyourself that this location has been cleaned up correctly. If there is still\nleftover data at the specified location then you should manually remove it.\n\nIf the analysis is successful then it detected no incorrect behaviour, but this\ndoes not mean that correct behaviour is guaranteed. The analysis attempts to\ndetect common bugs but it certainly does not offer 100% coverage. Additionally,\nit does not test the following:\n\n- Your repository must perform durable writes. Once a blob has been written it\n  must remain in place until it is deleted, even after a power loss or similar\n  disaster.\n\n- Your repository must not suffer from silent data corruption. Once a blob has\n  been written its contents must remain unchanged until it is deliberately\n  modified or deleted.\n\n- Your repository must behave correctly even if connectivity from the cluster\n  is disrupted. Reads and writes may fail in this case, but they must not return\n  incorrect results.\n\nIMPORTANT: An analysis writes a substantial amount of data to your repository\nand then reads it back again. This consumes bandwidth on the network between\nthe cluster and the repository, and storage space and IO bandwidth on the\nrepository itself. You must ensure this load does not affect other users of\nthese systems. Analyses respect the repository settings\n`max_snapshot_bytes_per_sec` and `max_restore_bytes_per_sec` if available, and\nthe cluster setting `indices.recovery.max_bytes_per_sec` which you can use to\nlimit the bandwidth they consume.\n\nNOTE: This API is intended for exploratory use by humans. You should expect the\nrequest parameters and the response format to vary in future versions.\n\nNOTE: Different versions of {es} may perform different checks for repository\ncompatibility, with newer versions typically being stricter than older ones. A\nstorage system that passes repository analysis with one version of {es} may\nfail with a different version. This indicates it behaves incorrectly in ways\nthat the former version did not detect. You must work with the supplier of your\nstorage system to address the incompatibilities detected by the repository\nanalysis API in any version of {es}.\n\nNOTE: This API may not work correctly in a mixed-version cluster.\n\n==== Implementation details\n\nNOTE: This section of documentation describes how the Repository analysis API\nworks in this version of {es}, but you should expect the implementation to vary\nbetween versions. The request parameters and response format depend on details\nof the implementation so may also be different in newer versions.\n\nThe analysis comprises a number of blob-level tasks, as set by the `blob_count`\nparameter, and a number of compare-and-exchange operations on linearizable\nregisters, as set by the `register_operation_count` parameter. These tasks are\ndistributed over the data and master-eligible nodes in the cluster for\nexecution.\n\nFor most blob-level tasks, the executing node first writes a blob to the\nrepository, and then instructs some of the other nodes in the cluster to\nattempt to read the data it just wrote. The size of the blob is chosen\nrandomly, according to the `max_blob_size` and `max_total_data_size`\nparameters. If any of these reads fails then the repository does not implement\nthe necessary read-after-write semantics that {es} requires.\n\nFor some blob-level tasks, the executing node will instruct some of its peers\nto attempt to read the data before the writing process completes. These reads\nare permitted to fail, but must not return partial data. If any read returns\npartial data then the repository does not implement the necessary atomicity\nsemantics that {es} requires.\n\nFor some blob-level tasks, the executing node will overwrite the blob while its\npeers are reading it. In this case the data read may come from either the\noriginal or the overwritten blob, but the read operation must not return\npartial data or a mix of data from the two blobs. If any of these reads returns\npartial data or a mix of the two blobs then the repository does not implement\nthe necessary atomicity semantics that {es} requires for overwrites.\n\nThe executing node will use a variety of different methods to write the blob.\nFor instance, where applicable, it will use both single-part and multi-part\nuploads. Similarly, the reading nodes will use a variety of different methods\nto read the data back again. For instance they may read the entire blob from\nstart to end, or may read only a subset of the data.\n\nFor some blob-level tasks, the executing node will abort the write before it is\ncomplete. In this case it still instructs some of the other nodes in the\ncluster to attempt to read the blob, but all of these reads must fail to find\nthe blob.\n\nLinearizable registers are special blobs that {es} manipulates using an atomic\ncompare-and-exchange operation. This operation ensures correct and\nstrongly-consistent behavior even when the blob is accessed by multiple nodes\nat the same time. The detailed implementation of the compare-and-exchange\noperation on linearizable registers varies by repository type. Repository\nanalysis verifies that that uncontended compare-and-exchange operations on a\nlinearizable register blob always succeed. Repository analysis also verifies\nthat contended operations either succeed or report the contention but do not\nreturn incorrect results. If an operation fails due to contention, {es} retries\nthe operation until it succeeds. Most of the compare-and-exchange operations\nperformed by repository analysis atomically increment a counter which is\nrepresented as an 8-byte blob. Some operations also verify the behavior on\nsmall blobs with sizes other than 8 bytes.\n\n[[repo-analysis-api-path-params]]\n==== {api-path-parms-title}\n\n`<repository>`::\n(Required, string)\nName of the snapshot repository to test.\n\n[[repo-analysis-api-query-params]]\n==== {api-query-parms-title}\n\n`blob_count`::\n(Optional, integer) The total number of blobs to write to the repository during\nthe test. Defaults to `100`. For realistic experiments you should set this to\nat least `2000`.\n\n`max_blob_size`::\n(Optional, <<size-units, size units>>) The maximum size of a blob to be written\nduring the test. Defaults to `10mb`. For realistic experiments you should set\nthis to at least `2gb`.\n\n`max_total_data_size`::\n(Optional, <<size-units, size units>>) An upper limit on the total size of all\nthe blobs written during the test. Defaults to `1gb`. For realistic experiments\nyou should set this to at least `1tb`.\n\n`register_operation_count`::\n(Optional, integer) The minimum number of linearizable register operations to\nperform in total. Defaults to `10`. For realistic experiments you should set\nthis to at least `100`.\n\n`timeout`::\n(Optional, <<time-units, time units>>) Specifies the period of time to wait for\nthe test to complete. If no response is received before the timeout expires,\nthe test is cancelled and returns an error. Defaults to `30s`.\n\n===== Advanced query parameters\n\nThe following parameters allow additional control over the analysis, but you\nwill usually not need to adjust them.\n\n`concurrency`::\n(Optional, integer) The number of write operations to perform concurrently.\nDefaults to `10`.\n\n`read_node_count`::\n(Optional, integer) The number of nodes on which to perform a read operation\nafter writing each blob. Defaults to `10`.\n\n`early_read_node_count`::\n(Optional, integer) The number of nodes on which to perform an early read\noperation while writing each blob. Defaults to `2`. Early read operations are\nonly rarely performed.\n\n`rare_action_probability`::\n(Optional, double) The probability of performing a rare action (an early read,\nan overwrite, or an aborted write) on each blob. Defaults to `0.02`.\n\n`seed`::\n(Optional, integer) The seed for the pseudo-random number generator used to\ngenerate the list of operations performed during the test. To repeat the same\nset of operations in multiple experiments, use the same seed in each\nexperiment. Note that the operations are performed concurrently so may not\nalways happen in the same order on each run.\n\n`detailed`::\n(Optional, boolean) Whether to return detailed results, including timing\ninformation for every operation performed during the analysis. Defaults to\n`false`, meaning to return only a summary of the analysis.\n\n`rarely_abort_writes`::\n(Optional, boolean) Whether to rarely abort some write requests. Defaults to\n`true`.\n\n[role=\"child_attributes\"]\n[[repo-analysis-api-response-body]]\n==== {api-response-body-title}\n\nThe response exposes implementation details of the analysis which may change\nfrom version to version. The response body format is therefore not considered\nstable and may be different in newer versions.\n\n`coordinating_node`::\n(object)\nIdentifies the node which coordinated the analysis and performed the final cleanup.\n+\n.Properties of `coordinating_node`\n[%collapsible%open]\n====\n`id`::\n(string)\nThe id of the coordinating node.\n\n`name`::\n(string)\nThe name of the coordinating node\n====\n\n`repository`::\n(string)\nThe name of the repository that was the subject of the analysis.\n\n`blob_count`::\n(integer)\nThe number of blobs written to the repository during the test, equal to the\n`?blob_count` request parameter.\n\n`concurrency`::\n(integer)\nThe number of write operations performed concurrently during the test, equal to\nthe `?concurrency` request parameter.\n\n`read_node_count`::\n(integer)\nThe limit on the number of nodes on which read operations were performed after\nwriting each blob, equal to the `?read_node_count` request parameter.\n\n`early_read_node_count`::\n(integer)\nThe limit on the number of nodes on which early read operations were performed\nafter writing each blob, equal to the `?early_read_node_count` request\nparameter.\n\n`max_blob_size`::\n(string)\nThe limit on the size of a blob written during the test, equal to the\n`?max_blob_size` parameter.\n\n`max_blob_size_bytes`::\n(long)\nThe limit, in bytes, on the size of a blob written during the test, equal to\nthe `?max_blob_size` parameter.\n\n`max_total_data_size`::\n(string)\nThe limit on the total size of all blob written during the test, equal to the\n`?max_total_data_size` parameter.\n\n`max_total_data_size_bytes`::\n(long)\nThe limit, in bytes, on the total size of all blob written during the test,\nequal to the `?max_total_data_size` parameter.\n\n`seed`::\n(long)\nThe seed for the pseudo-random number generator used to generate the operations\nused during the test. Equal to the `?seed` request parameter if set.\n\n`rare_action_probability`::\n(double)\nThe probability of performing rare actions during the test. Equal to the\n`?rare_action_probability` request parameter.\n\n`blob_path`::\n(string)\nThe path in the repository under which all the blobs were written during the\ntest.\n\n`issues_detected`::\n(list)\nA list of correctness issues detected, which will be empty if the API\nsucceeded. Included to emphasize that a successful response does not guarantee\ncorrect behaviour in future.\n\n`summary`::\n(object)\nA collection of statistics that summarise the results of the test.\n+\n.Properties of `summary`\n[%collapsible%open]\n====\n`write`::\n(object)\nA collection of statistics that summarise the results of the write operations\nin the test.\n+\n.Properties of `write`\n[%collapsible%open]\n=====\n`count`::\n(integer)\nThe number of write operations performed in the test.\n\n`total_size`::\n(string)\nThe total size of all the blobs written in the test.\n\n`total_size_bytes`::\n(long)\nThe total size of all the blobs written in the test, in bytes.\n\n`total_throttled`::\n(string)\nThe total time spent waiting due to the `max_snapshot_bytes_per_sec` throttle.\n\n`total_throttled_nanos`::\n(long)\nThe total time spent waiting due to the `max_snapshot_bytes_per_sec` throttle,\nin nanoseconds.\n\n`total_elapsed`::\n(string)\nThe total elapsed time spent on writing blobs in the test.\n\n`total_elapsed_nanos`::\n(long)\nThe total elapsed time spent on writing blobs in the test, in nanoseconds.\n=====\n\n`read`::\n(object)\nA collection of statistics that summarise the results of the read operations in\nthe test.\n+\n.Properties of `read`\n[%collapsible%open]\n=====\n`count`::\n(integer)\nThe number of read operations performed in the test.\n\n`total_size`::\n(string)\nThe total size of all the blobs or partial blobs read in the test.\n\n`total_size_bytes`::\n(long)\nThe total size of all the blobs or partial blobs read in the test, in bytes.\n\n`total_wait`::\n(string)\nThe total time spent waiting for the first byte of each read request to be\nreceived.\n\n`total_wait_nanos`::\n(long)\nThe total time spent waiting for the first byte of each read request to be\nreceived, in nanoseconds.\n\n`max_wait`::\n(string)\nThe maximum time spent waiting for the first byte of any read request to be\nreceived.\n\n`max_wait_nanos`::\n(long)\nThe maximum time spent waiting for the first byte of any read request to be\nreceived, in nanoseconds.\n\n`total_throttled`::\n(string)\nThe total time spent waiting due to the `max_restore_bytes_per_sec` or\n`indices.recovery.max_bytes_per_sec` throttles.\n\n`total_throttled_nanos`::\n(long)\nThe total time spent waiting due to the `max_restore_bytes_per_sec` or\n`indices.recovery.max_bytes_per_sec` throttles, in nanoseconds.\n\n`total_elapsed`::\n(string)\nThe total elapsed time spent on reading blobs in the test.\n\n`total_elapsed_nanos`::\n(long)\nThe total elapsed time spent on reading blobs in the test, in nanoseconds.\n=====\n====\n\n`details`::\n(array)\nA description of every read and write operation performed during the test. This\nis only returned if the `?detailed` request parameter is set to `true`.\n+\n.Properties of items within `details`\n[%collapsible]\n====\n`blob`::\n(object)\nA description of the blob that was written and read.\n+\n.Properties of `blob`\n[%collapsible%open]\n=====\n`name`::\n(string)\nThe name of the blob.\n\n`size`::\n(string)\nThe size of the blob.\n\n`size_bytes`::\n(long)\nThe size of the blob in bytes.\n\n`read_start`::\n(long)\nThe position, in bytes, at which read operations started.\n\n`read_end`::\n(long)\nThe position, in bytes, at which read operations completed.\n\n`read_early`::\n(boolean)\nWhether any read operations were started before the write operation completed.\n\n`overwritten`::\n(boolean)\nWhether the blob was overwritten while the read operations were ongoing.\n=====\n\n`writer_node`::\n(object)\nIdentifies the node which wrote this blob and coordinated the read operations.\n+\n.Properties of `writer_node`\n[%collapsible%open]\n=====\n`id`::\n(string)\nThe id of the writer node.\n\n`name`::\n(string)\nThe name of the writer node\n=====\n\n`write_elapsed`::\n(string)\nThe elapsed time spent writing this blob.\n\n`write_elapsed_nanos`::\n(long)\nThe elapsed time spent writing this blob, in nanoseconds.\n\n`overwrite_elapsed`::\n(string)\nThe elapsed time spent overwriting this blob. Omitted if the blob was not\noverwritten.\n\n`overwrite_elapsed_nanos`::\n(long)\nThe elapsed time spent overwriting this blob, in nanoseconds. Omitted if the\nblob was not overwritten.\n\n`write_throttled`::\n(string)\nThe length of time spent waiting for the `max_snapshot_bytes_per_sec` (or\n`indices.recovery.max_bytes_per_sec` if the\n<<recovery-settings-for-managed-services,recovery settings for managed services>>\nare set) throttle while writing this blob.\n\n`write_throttled_nanos`::\n(long)\nThe length of time spent waiting for the `max_snapshot_bytes_per_sec` (or\n`indices.recovery.max_bytes_per_sec` if the\n<<recovery-settings-for-managed-services,recovery settings for managed services>>\nare set) throttle while writing this blob, in nanoseconds.\n\n`reads`::\n(array)\nA description of every read operation performed on this blob.\n+\n.Properties of items within `reads`\n[%collapsible%open]\n=====\n`node`::\n(object)\nIdentifies the node which performed the read operation.\n+\n.Properties of `node`\n[%collapsible%open]\n======\n`id`::\n(string)\nThe id of the reader node.\n\n`name`::\n(string)\nThe name of the reader node\n======\n\n`before_write_complete`::\n(boolean)\nWhether the read operation may have started before the write operation was\ncomplete. Omitted if `false`.\n\n`found`::\n(boolean)\nWhether the blob was found by this read operation or not. May be `false` if the\nread was started before the write completed, or the write was aborted before\ncompletion.\n\n`first_byte_time`::\n(string)\nThe length of time waiting for the first byte of the read operation to be\nreceived. Omitted if the blob was not found.\n\n`first_byte_time_nanos`::\n(long)\nThe length of time waiting for the first byte of the read operation to be\nreceived, in nanoseconds. Omitted if the blob was not found.\n\n`elapsed`::\n(string)\nThe length of time spent reading this blob. Omitted if the blob was not found.\n\n`elapsed_nanos`::\n(long)\nThe length of time spent reading this blob, in nanoseconds. Omitted if the blob\nwas not found.\n\n`throttled`::\n(string)\nThe length of time spent waiting due to the `max_restore_bytes_per_sec` or\n`indices.recovery.max_bytes_per_sec` throttles during the read of this blob.\nOmitted if the blob was not found.\n\n`throttled_nanos`::\n(long)\nThe length of time spent waiting due to the `max_restore_bytes_per_sec` or\n`indices.recovery.max_bytes_per_sec` throttles during the read of this blob, in\nnanoseconds. Omitted if the blob was not found.\n\n=====\n\n====\n\n`listing_elapsed`::\n(string)\nThe time it took to retrieve a list of all the blobs in the container.\n\n`listing_elapsed_nanos`::\n(long)\nThe time it took to retrieve a list of all the blobs in the container, in\nnanoseconds.\n\n`delete_elapsed`::\n(string)\nThe time it took to delete all the blobs in the container.\n\n`delete_elapsed_nanos`::\n(long)\nThe time it took to delete all the blobs in the container, in nanoseconds.\n"
}