{
    "meta": {
        "timestamp": "2024-11-01T03:02:52.780583",
        "size": 7110,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-indexing-speed.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "tune-for-indexing-speed",
        "version": "8.15"
    },
    "doc": "[[tune-for-indexing-speed]]\n== Tune for indexing speed\n\n[discrete]\n=== Use bulk requests\n\nBulk requests will yield much better performance than single-document index\nrequests. In order to know the optimal size of a bulk request, you should run\na benchmark on a single node with a single shard. First try to index 100\ndocuments at once, then 200, then 400, etc. doubling the number of documents\nin a bulk request in every benchmark run. When the indexing speed starts to\nplateau then you know you reached the optimal size of a bulk request for your\ndata. In case of tie, it is better to err in the direction of too few rather\nthan too many documents. Beware that too large bulk requests might put the\ncluster under memory pressure when many of them are sent concurrently, so\nit is advisable to avoid going beyond a couple tens of megabytes per request\neven if larger requests seem to perform better.\n\n[discrete]\n[[multiple-workers-threads]]\n=== Use multiple workers/threads to send data to Elasticsearch\n\nA single thread sending bulk requests is unlikely to be able to max out the\nindexing capacity of an Elasticsearch cluster. In order to use all resources\nof the cluster, you should send data from multiple threads or processes. In\naddition to making better use of the resources of the cluster, this should\nhelp reduce the cost of each fsync.\n\nMake sure to watch for `TOO_MANY_REQUESTS (429)` response codes\n(`EsRejectedExecutionException` with the Java client), which is the way that\nElasticsearch tells you that it cannot keep up with the current indexing rate.\nWhen it happens, you should pause indexing a bit before trying again, ideally\nwith randomized exponential backoff.\n\nSimilarly to sizing bulk requests, only testing can tell what the optimal\nnumber of workers is. This can be tested by progressively increasing the\nnumber of workers until either I/O or CPU is saturated on the cluster.\n\n[discrete]\n=== Unset or increase the refresh interval\n\nThe operation that consists of making changes visible to search - called a\n<<indices-refresh,refresh>> - is costly, and calling it often while there is\nongoing indexing activity can hurt indexing speed.\n\ninclude::{es-ref-dir}/indices/refresh.asciidoc[tag=refresh-interval-default]\nThis is the optimal configuration if you have no or very little search traffic\n(e.g. less than one search request every 5 minutes) and want to optimize for\nindexing speed. This behavior aims to automatically optimize bulk indexing in\nthe default case when no searches are performed. In order to opt out of this\nbehavior set the refresh interval explicitly.\n\nOn the other hand, if your index experiences regular search requests, this\ndefault behavior means that Elasticsearch will refresh your index every 1\nsecond. If you can afford to increase the amount of time between when a document\ngets indexed and when it becomes visible, increasing the\n<<index-refresh-interval-setting,`index.refresh_interval`>> to a larger value, e.g.\n`30s`, might help improve indexing speed.\n\n[discrete]\n=== Disable replicas for initial loads\n\nIf you have a large amount of data that you want to load all at once into\nElasticsearch, it may be beneficial to set `index.number_of_replicas` to `0` in\norder to speed up indexing. Having no replicas means that losing a single node\nmay incur data loss, so it is important that the data lives elsewhere so that\nthis initial load can be retried in case of an issue. Once the initial load is\nfinished, you can set `index.number_of_replicas` back to its original value.\n\nIf `index.refresh_interval` is configured in the index settings, it may further\nhelp to unset it during this initial load and setting it back to its original\nvalue once the initial load is finished.\n\n[discrete]\n=== Disable swapping\n\nYou should make sure that the operating system is not swapping out the java\nprocess by <<setup-configuration-memory,disabling swapping>>.\n\n[discrete]\n=== Give memory to the filesystem cache\n\nThe filesystem cache will be used in order to buffer I/O operations. You should\nmake sure to give at least half the memory of the machine running Elasticsearch\nto the filesystem cache.\n\n[discrete]\n=== Use auto-generated ids\n\nWhen indexing a document that has an explicit id, Elasticsearch needs to check\nwhether a document with the same id already exists within the same shard, which\nis a costly operation and gets even more costly as the index grows. By using\nauto-generated ids, Elasticsearch can skip this check, which makes indexing\nfaster.\n\n[discrete]\n[[indexing-use-faster-hardware]]\n=== Use faster hardware\n\nIf indexing is I/O-bound, consider increasing the size of the filesystem cache\n(see above) or using faster storage. Elasticsearch generally creates individual\nfiles with sequential writes. However, indexing involves writing multiple files\nconcurrently, and a mix of random and sequential reads too, so SSD drives tend\nto perform better than spinning disks.\n\nStripe your index across multiple SSDs by configuring a RAID 0 array. Remember\nthat it will increase the risk of failure since the failure of any one SSD\ndestroys the index. However this is typically the right tradeoff to make:\noptimize single shards for maximum performance, and then add replicas across\ndifferent nodes so there's redundancy for any node failures. You can also use\n<<snapshot-restore,snapshot and restore>> to backup the index for further\ninsurance.\n\n[discrete]\n==== Local vs.remote storage\n\ninclude::./remote-storage.asciidoc[]\n\n[discrete]\n=== Indexing buffer size\n\nIf your node is doing only heavy indexing, be sure\n<<indexing-buffer,`indices.memory.index_buffer_size`>> is large enough to give\nat most 512 MB indexing buffer per shard doing heavy indexing (beyond that\nindexing performance does not typically improve). Elasticsearch takes that\nsetting (a percentage of the java heap or an absolute byte-size), and\nuses it as a shared buffer across all active shards. Very active shards will\nnaturally use this buffer more than shards that are performing lightweight\nindexing.\n\nThe default is `10%` which is often plenty: for example, if you give the JVM\n10GB of memory, it will give 1GB to the index buffer, which is enough to host\ntwo shards that are heavily indexing.\n\n[discrete]\n=== Use {ccr} to prevent searching from stealing resources from indexing\n\nWithin a single cluster, indexing and searching can compete for resources. By\nsetting up two clusters, configuring <<xpack-ccr,{ccr}>> to replicate data from\none cluster to the other one, and routing all searches to the cluster that has\nthe follower indices, search activity will no longer steal resources from\nindexing on the cluster that hosts the leader indices.\n\n[discrete]\n=== Avoid hot spotting\n\n<<hotspotting,Hot Spotting>> can occur when node resources, shards, or requests \nare not evenly distributed. {es} maintains cluster state by syncing it across \nnodes, so continually hot spotted nodes can cause overall cluster performance \ndegredation.\n\n[discrete]\n=== Additional optimizations\n\nMany of the strategies outlined in <<tune-for-disk-usage>> also\nprovide an improvement in the speed of indexing.\n"
}