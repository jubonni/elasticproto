{
    "meta": {
        "timestamp": "2024-11-01T02:49:25.672069",
        "size": 6908,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-minhash-tokenfilter.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "analysis-minhash-tokenfilter",
        "version": "8.15"
    },
    "doc": "[[analysis-minhash-tokenfilter]]\n=== MinHash token filter\n++++\n<titleabbrev>MinHash</titleabbrev>\n++++\n\nUses the {wikipedia}/MinHash[MinHash] technique to produce a\nsignature for a token stream. You can use MinHash signatures to estimate the\nsimilarity of documents. See <<analysis-minhash-tokenfilter-similarity-search>>.\n\nThe `min_hash` filter performs the following operations on a token stream in\norder:\n\n. Hashes each token in the stream.\n. Assigns the hashes to buckets, keeping only the smallest hashes of each\n  bucket.\n. Outputs the smallest hash from each bucket as a token stream.\n\nThis filter uses Lucene's\n{lucene-analysis-docs}/minhash/MinHashFilter.html[MinHashFilter].\n\n[[analysis-minhash-tokenfilter-configure-parms]]\n==== Configurable parameters\n\n`bucket_count`::\n(Optional, integer)\nNumber of buckets to which hashes are assigned. Defaults to `512`.\n\n`hash_count`::\n(Optional, integer)\nNumber of ways to hash each token in the stream. Defaults to `1`.\n\n`hash_set_size`::\n(Optional, integer)\nNumber of hashes to keep from each bucket. Defaults to `1`.\n+\nHashes are retained by ascending size, starting with the bucket's smallest hash\nfirst.\n\n`with_rotation`::\n(Optional, Boolean)\nIf `true`, the filter fills empty buckets with the value of the first non-empty\nbucket to its circular right if the `hash_set_size` is `1`. If the\n`bucket_count` argument is greater than `1`, this parameter defaults to `true`.\nOtherwise, this parameter defaults to `false`.\n\n[[analysis-minhash-tokenfilter-configuration-tips]]\n==== Tips for configuring the `min_hash` filter\n\n* `min_hash` filter input tokens should typically be k-words shingles produced\nfrom <<analysis-shingle-tokenfilter,shingle token filter>>. You should\nchoose `k` large enough so that the probability of any given shingle\noccurring in a document is low. At the same time, as\ninternally each shingle is hashed into to 128-bit hash, you should choose\n`k` small enough so that all possible\ndifferent k-words shingles can be hashed to 128-bit hash with\nminimal collision.\n\n* We recommend you test different arguments for the `hash_count`, `bucket_count` and\n  `hash_set_size` parameters:\n\n** To improve precision, increase the `bucket_count` or\n   `hash_set_size` arguments. Higher `bucket_count` and `hash_set_size` values\n   increase the likelihood that different tokens are indexed to different\n   buckets.\n\n** To improve the recall, increase the value of the `hash_count` argument. For\n   example, setting `hash_count` to `2` hashes each token in two different ways,\n   increasing the number of potential candidates for search.\n\n* By default, the `min_hash` filter produces 512 tokens for each document. Each\ntoken is 16 bytes in size. This means each document's size will be increased by\naround 8Kb.\n\n* The `min_hash` filter is used for Jaccard similarity. This means\nthat it doesn't matter how many times a document contains a certain token,\nonly that if it contains it or not.\n\n[[analysis-minhash-tokenfilter-similarity-search]]\n==== Using the `min_hash` token filter for similarity search\n\nThe `min_hash` token filter allows you to hash documents for similarity search.\nSimilarity search, or nearest neighbor search is a complex problem.\nA naive solution requires an exhaustive pairwise comparison between a query\ndocument and every document in an index. This is a prohibitive operation\nif the index is large. A number of approximate nearest neighbor search\nsolutions have been developed to make similarity search more practical and\ncomputationally feasible. One of these solutions involves hashing of documents.\n\nDocuments are hashed in a way that similar documents are more likely\nto produce the same hash code and are put into the same hash bucket,\nwhile dissimilar documents are more likely to be hashed into\ndifferent hash buckets. This type of hashing is known as\nlocality sensitive hashing (LSH).\n\nDepending on what constitutes the similarity between documents,\nvarious LSH functions https://arxiv.org/abs/1408.2927[have been proposed].\nFor {wikipedia}/Jaccard_index[Jaccard similarity], a popular\nLSH function is {wikipedia}/MinHash[MinHash].\nA general idea of the way MinHash produces a signature for a document\nis by applying a random permutation over the whole index vocabulary (random\nnumbering for the vocabulary), and recording the minimum value for this permutation\nfor the document (the minimum number for a vocabulary word that is present\nin the document). The permutations are run several times;\ncombining the minimum values for all of them will constitute a\nsignature for the document.\n\nIn practice, instead of random permutations, a number of hash functions\nare chosen. A hash function calculates a hash code for each of a\ndocument's tokens and chooses the minimum hash code among them.\nThe minimum hash codes from all hash functions are combined\nto form a signature for the document.\n\n[[analysis-minhash-tokenfilter-customize]]\n==== Customize and add to an analyzer\n\nTo customize the `min_hash` filter, duplicate it to create the basis for a new\ncustom token filter. You can modify the filter using its configurable\nparameters.\n\nFor example, the following <<indices-create-index,create index API>> request\nuses the following custom token filters to configure a new\n<<analysis-custom-analyzer,custom analyzer>>:\n\n* `my_shingle_filter`, a custom <<analysis-shingle-tokenfilter,`shingle`\n  filter>>. `my_shingle_filter` only outputs five-word shingles.\n* `my_minhash_filter`, a custom `min_hash` filter. `my_minhash_filter` hashes\n  each five-word shingle once. It then assigns the hashes into 512 buckets,\n  keeping only the smallest hash from each bucket.\n\nThe request also assigns the custom analyzer to the `fingerprint` field mapping.\n\n[source,console]\n----\nPUT /my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"filter\": {\n        \"my_shingle_filter\": {      <1>\n          \"type\": \"shingle\",\n          \"min_shingle_size\": 5,\n          \"max_shingle_size\": 5,\n          \"output_unigrams\": false\n        },\n        \"my_minhash_filter\": {\n          \"type\": \"min_hash\",\n          \"hash_count\": 1,          <2>\n          \"bucket_count\": 512,      <3>\n          \"hash_set_size\": 1,       <4>\n          \"with_rotation\": true     <5>\n        }\n      },\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"my_shingle_filter\",\n            \"my_minhash_filter\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"fingerprint\": {\n        \"type\": \"text\",\n        \"analyzer\": \"my_analyzer\"\n      }\n    }\n  }\n}\n----\n\n<1> Configures a custom shingle filter to output only five-word shingles.\n<2> Each five-word shingle in the stream is hashed once.\n<3> The hashes are assigned to 512 buckets.\n<4> Only the smallest hash in each bucket is retained.\n<5> The filter fills empty buckets with the values of neighboring buckets.\n"
}