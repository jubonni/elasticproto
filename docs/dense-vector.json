{
    "meta": {
        "size": 20142,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html",
        "type": "documentation",
        "role": [
            "xpack",
            "child_attributes"
        ],
        "has_code": true,
        "title": "dense-vector",
        "version": "8.15"
    },
    "doc": "[role=\"xpack\"]\n[[dense-vector]]\n=== Dense vector field type\n++++\n<titleabbrev>Dense vector</titleabbrev>\n++++\n\nThe `dense_vector` field type stores dense vectors of numeric values. Dense\nvector fields are primarily used for <<knn-search,k-nearest neighbor (kNN) search>>.\n\nThe `dense_vector` type does not support aggregations or sorting.\n\nYou add a `dense_vector` field as an array of numeric values\nbased on <<dense-vector-params, `element_type`>> with\n`float` by default:\n\n[source,console]\n--------------------------------------------------\nPUT my-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 3\n      },\n      \"my_text\" : {\n        \"type\" : \"keyword\"\n      }\n    }\n  }\n}\n\nPUT my-index/_doc/1\n{\n  \"my_text\" : \"text1\",\n  \"my_vector\" : [0.5, 10, 6]\n}\n\nPUT my-index/_doc/2\n{\n  \"my_text\" : \"text2\",\n  \"my_vector\" : [-0.5, 10, 10]\n}\n\n--------------------------------------------------\n\nNOTE: Unlike most other data types, dense vectors are always single-valued.\nIt is not possible to store multiple values in one `dense_vector` field.\n\n[[index-vectors-knn-search]]\n==== Index vectors for kNN search\n\ninclude::{es-ref-dir}/search/search-your-data/knn-search.asciidoc[tag=knn-def]\n\nDense vector fields can be used to rank documents in\n<<query-dsl-script-score-query,`script_score` queries>>. This lets you perform\na brute-force kNN search by scanning all documents and ranking them by\nsimilarity.\n\nIn many cases, a brute-force kNN search is not efficient enough. For this\nreason, the `dense_vector` type supports indexing vectors into a specialized\ndata structure to support fast kNN retrieval through the <<search-api-knn, `knn` option>> in the search API\n\nUnmapped array fields of float elements with size between 128 and 4096 are dynamically mapped as `dense_vector` with a default similariy of `cosine`.\nYou can override the default similarity by explicitly mapping the field as `dense_vector` with the desired similarity.\n\nIndexing is enabled by default for dense vector fields and indexed as `int8_hnsw`.\nWhen indexing is enabled, you can define the vector similarity to use in kNN search:\n\n[source,console]\n--------------------------------------------------\nPUT my-index-2\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 3,\n        \"similarity\": \"dot_product\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n\nNOTE: Indexing vectors for approximate kNN search is an expensive process. It\ncan take substantial time to ingest documents that contain vector fields with\n`index` enabled. See <<tune-knn-search,k-nearest neighbor (kNN) search>> to\nlearn more about the memory requirements.\n\nYou can disable indexing by setting the `index` parameter to `false`:\n\n[source,console]\n--------------------------------------------------\nPUT my-index-2\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 3,\n        \"index\": false\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n{es} uses the https://arxiv.org/abs/1603.09320[HNSW algorithm] to support\nefficient kNN search. Like most kNN algorithms, HNSW is an approximate method\nthat sacrifices result accuracy for improved speed.\n\n[[dense-vector-quantization]]\n==== Automatically quantize vectors for kNN search\n\nThe `dense_vector` type supports quantization to reduce the memory footprint required when <<approximate-knn, searching>> `float` vectors.\nThe three following quantization strategies are supported:\n\n+\n--\n`int8` - Quantizes each dimension of the vector to 1-byte integers. This reduces the memory footprint by 75% (or 4x) at the cost of some accuracy.\n`int4` - Quantizes each dimension of the vector to half-byte integers. This reduces the memory footprint by 87% (or 8x) at the cost of accuracy.\n`bbq` - experimental:[] Better binary quantization which reduces each dimension to a single bit precision. This reduces the memory footprint by 96% (or 32x) at a larger cost of accuracy. Generally, oversampling during query time and reranking can help mitigate the accuracy loss.\n--\n\nWhen using a quantized format, you may want to oversample and rescore the results to improve accuracy. See <<dense-vector-knn-search-reranking, oversampling and rescoring>> for more information.\n\nTo use a quantized index, you can set your index type to `int8_hnsw`, `int4_hnsw`, or `bbq_hnsw`. When indexing `float` vectors, the current default\nindex type is `int8_hnsw`.\n\nNOTE: Quantization will continue to keep the raw float vector values on disk for reranking, reindexing, and quantization improvements over the lifetime of the data.\nThis means disk usage will increase by ~25% for `int8`, ~12.5% for `int4`, and ~3.1% for `bbq` due to the overhead of storing the quantized and raw vectors.\n\nNOTE: `int4` quantization requires an even number of vector dimensions.\n\nNOTE: experimental:[] `bbq` quantization only supports vector dimensions that are greater than 64.\n\nHere is an example of how to create a byte-quantized index:\n\n[source,console]\n--------------------------------------------------\nPUT my-byte-quantized-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 3,\n        \"index\": true,\n        \"index_options\": {\n          \"type\": \"int8_hnsw\"\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\nHere is an example of how to create a half-byte-quantized index:\n\n[source,console]\n--------------------------------------------------\nPUT my-byte-quantized-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 4,\n        \"index\": true,\n        \"index_options\": {\n          \"type\": \"int4_hnsw\"\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\nexperimental:[] Here is an example of how to create a binary quantized index:\n\n[source,console]\n--------------------------------------------------\nPUT my-byte-quantized-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 64,\n        \"index\": true,\n        \"index_options\": {\n          \"type\": \"bbq_hnsw\"\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n[role=\"child_attributes\"]\n[[dense-vector-params]]\n==== Parameters for dense vector fields\n\nThe following mapping parameters are accepted:\n\n[[dense-vector-element-type]]\n`element_type`::\n(Optional, string)\nThe data type used to encode vectors. The supported data types are\n`float` (default), `byte`, and bit.\n\n.Valid values for `element_type`\n[%collapsible%open]\n====\n`float`:::\nindexes a 4-byte floating-point\nvalue per dimension. This is the default value.\n\n`byte`:::\nindexes a 1-byte integer value per dimension.\n\n`bit`:::\nindexes a single bit per dimension. Useful for very high-dimensional vectors or models that specifically support bit vectors.\nNOTE: when using `bit`, the number of dimensions must be a multiple of 8 and must represent the number of bits.\n\n====\n\n`dims`::\n(Optional, integer)\nNumber of vector dimensions. Can't exceed `4096`. If `dims` is not specified,\nit will be set to the length of the first vector added to the field.\n\n`index`::\n(Optional, Boolean)\nIf `true`, you can search this field using the <<knn-search-api, kNN search\nAPI>>. Defaults to `true`.\n\n[[dense-vector-similarity]]\n`similarity`::\n(Optional^*^, string)\nThe vector similarity metric to use in kNN search. Documents are ranked by\ntheir vector field's similarity to the query vector. The `_score` of each\ndocument will be derived from the similarity, in a way that ensures scores are\npositive and that a larger score corresponds to a higher ranking.\nDefaults to `l2_norm` when `element_type: bit` otherwise defaults to `cosine`.\n\nNOTE: `bit` vectors only support `l2_norm` as their similarity metric.\n\n+\n^*^ This parameter can only be specified when `index` is `true`.\n+\n.Valid values for `similarity`\n[%collapsible%open]\n====\n`l2_norm`:::\nComputes similarity based on the L^2^ distance (also known as Euclidean\ndistance) between the vectors. The document `_score` is computed as\n`1 / (1 + l2_norm(query, vector)^2)`.\n\nFor `bit` vectors, instead of using `l2_norm`, the `hamming` distance between the vectors is used. The `_score`\ntransformation is `(numBits - hamming(a, b)) / numBits`\n\n`dot_product`:::\nComputes the dot product of two unit vectors. This option provides an optimized way\nto perform cosine similarity. The constraints and computed score are defined\nby `element_type`.\n+\nWhen `element_type` is `float`, all vectors must be unit length, including both\ndocument and query vectors. The document `_score` is computed as\n`(1 + dot_product(query, vector)) / 2`.\n+\nWhen `element_type` is `byte`, all vectors must have the same\nlength including both document and query vectors or results will be inaccurate.\nThe document `_score` is computed as\n`0.5 + (dot_product(query, vector) / (32768 * dims))`\nwhere `dims` is the number of dimensions per vector.\n\n`cosine`:::\nComputes the cosine similarity. During indexing {es} automatically\nnormalizes vectors with `cosine` similarity to unit length. This allows\nto internally use `dot_product` for computing similarity, which is more efficient.\nOriginal un-normalized vectors can be still accessed\nthrough scripts. The document `_score`\nis computed as `(1 + cosine(query, vector)) / 2`. The `cosine` similarity does\nnot allow vectors with zero magnitude, since cosine is not defined in this\ncase.\n\n`max_inner_product`:::\nComputes the maximum inner product of two vectors. This is similar to `dot_product`,\nbut doesn't require vectors to be normalized. This means that each vector's magnitude\ncan significantly effect the score. The document `_score` is adjusted to prevent negative\nvalues. For `max_inner_product` values `< 0`, the `_score` is\n`1 / (1 + -1 * max_inner_product(query, vector))`. For non-negative `max_inner_product` results\nthe `_score` is calculated `max_inner_product(query, vector) + 1`.\n====\n\nNOTE: Although they are conceptually related, the `similarity` parameter is\ndifferent from <<text,`text`>> field <<similarity,`similarity`>> and accepts\na distinct set of options.\n\n[[dense-vector-index-options]]\n`index_options`::\n(Optional^*^, object)\nAn optional section that configures the kNN indexing algorithm. The HNSW\nalgorithm has two internal parameters that influence how the data structure is\nbuilt. These can be adjusted to improve the accuracy of results, at the\nexpense of slower indexing speed.\n+\n^*^ This parameter can only be specified when `index` is `true`.\n+\n.Properties of `index_options`\n[%collapsible%open]\n====\n`type`:::\n(Required, string)\nThe type of kNN algorithm to use. Can be either any of:\n+\n--\n* `hnsw` - This utilizes the https://arxiv.org/abs/1603.09320[HNSW algorithm] for scalable\n    approximate kNN search. This supports all `element_type` values.\n* `int8_hnsw` - The default index type for float vectors.\nThis utilizes the https://arxiv.org/abs/1603.09320[HNSW algorithm] in addition to automatically scalar\nquantization for scalable approximate kNN search with `element_type` of `float`. This can reduce the memory footprint\nby 4x at the cost of some accuracy. See <<dense-vector-quantization, Automatically quantize vectors for kNN search>>.\n* `int4_hnsw` - This utilizes the https://arxiv.org/abs/1603.09320[HNSW algorithm] in addition to automatically scalar\nquantization for scalable approximate kNN search with `element_type` of `float`. This can reduce the memory footprint\nby 8x at the cost of some accuracy. See <<dense-vector-quantization, Automatically quantize vectors for kNN search>>.\n* experimental:[] `bbq_hnsw` - This utilizes the https://arxiv.org/abs/1603.09320[HNSW algorithm] in addition to automatically binary\nquantization for scalable approximate kNN search with `element_type` of `float`. This can reduce the memory footprint\nby 32x at the cost of accuracy. See <<dense-vector-quantization, Automatically quantize vectors for kNN search>>.\n* `flat` - This utilizes a brute-force search algorithm for exact kNN search. This supports all `element_type` values.\n* `int8_flat` - This utilizes a brute-force search algorithm in addition to automatically scalar quantization. Only supports\n`element_type` of `float`.\n* `int4_flat` - This utilizes a brute-force search algorithm in addition to automatically half-byte scalar quantization. Only supports\n`element_type` of `float`.\n* experimental:[] `bbq_flat` - This utilizes a brute-force search algorithm in addition to automatically binary quantization. Only supports\n`element_type` of `float`.\n--\n`m`:::\n(Optional, integer)\nThe number of neighbors each node will be connected to in the HNSW graph.\nDefaults to `16`. Only applicable to `hnsw`, `int8_hnsw`, and `int4_hnsw` index types.\n\n`ef_construction`:::\n(Optional, integer)\nThe number of candidates to track while assembling the list of nearest\nneighbors for each new node. Defaults to `100`. Only applicable to `hnsw`, `int8_hnsw`, and `int4_hnsw` index types.\n\n`confidence_interval`:::\n(Optional, float)\nOnly applicable to `int8_hnsw`, `int4_hnsw`, `int8_flat`, and `int4_flat` index types. The confidence interval to use when quantizing the vectors.\nCan be any value between and including `0.90` and `1.0` or exactly `0`. When the value is `0`, this indicates that dynamic\nquantiles should be calculated for optimized quantization. When between `0.90` and `1.0`,\nthis value restricts the values used when calculating the quantization thresholds.\nFor example, a value of `0.95` will only use the middle 95% of the values when calculating the quantization thresholds\n(e.g. the highest and lowest 2.5% of values will be ignored).\nDefaults to `1/(dims + 1)` for `int8` quantized vectors and `0` for `int4` for dynamic quantile calculation.\n====\n\n[[dense-vector-synthetic-source]]\n==== Synthetic `_source`\n\nIMPORTANT: Synthetic `_source` is Generally Available only for TSDB indices\n(indices that have `index.mode` set to `time_series`). For other indices\nsynthetic `_source` is in technical preview. Features in technical preview may\nbe changed or removed in a future release. Elastic will work to fix\nany issues, but features in technical preview are not subject to the support SLA\nof official GA features.\n\n`dense_vector` fields support <<synthetic-source,synthetic `_source`>> .\n\n[[dense-vector-index-bit]]\n==== Indexing & Searching bit vectors\n\nWhen using `element_type: bit`, this will treat all vectors as bit vectors. Bit vectors utilize only a single\nbit per dimension and are internally encoded as bytes. This can be useful for very high-dimensional vectors or models.\n\nWhen using `bit`, the number of dimensions must be a multiple of 8 and must represent the number of bits. Additionally,\nwith `bit` vectors, the typical vector similarity values are effectively all scored the same, e.g. with `hamming` distance.\n\nLet's compare two `byte[]` arrays, each representing 40 individual bits.\n\n`[-127, 0, 1, 42, 127]` in bits `1000000100000000000000010010101001111111`\n`[127, -127, 0, 1, 42]` in bits `0111111110000001000000000000000100101010`\n\nWhen comparing these two bit, vectors, we first take the {wikipedia}/Hamming_distance[`hamming` distance].\n\n`xor` result:\n```\n1000000100000000000000010010101001111111\n^\n0111111110000001000000000000000100101010\n=\n1111111010000001000000010010101101010101\n```\n\nThen, we gather the count of `1` bits in the `xor` result: `18`. To scale for scoring, we subtract from the total number\nof bits and divide by the total number of bits: `(40 - 18) / 40 = 0.55`. This would be the `_score` betwee these two\nvectors.\n\nHere is an example of indexing and searching bit vectors:\n\n[source,console]\n--------------------------------------------------\nPUT my-bit-vectors\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 40, <1>\n        \"element_type\": \"bit\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n<1> The number of dimensions that represents the number of bits\n\n[source,console]\n--------------------------------------------------\nPOST /my-bit-vectors/_bulk?refresh\n{\"index\": {\"_id\" : \"1\"}}\n{\"my_vector\": [127, -127, 0, 1, 42]} <1>\n{\"index\": {\"_id\" : \"2\"}}\n{\"my_vector\": \"8100012a7f\"} <2>\n--------------------------------------------------\n// TEST[continued]\n<1> 5 bytes representing the 40 bit dimensioned vector\n<2> A hexidecimal string representing the 40 bit dimensioned vector\n\nThen, when searching, you can use the `knn` query to search for similar bit vectors:\n\n[source,console]\n--------------------------------------------------\nPOST /my-bit-vectors/_search?filter_path=hits.hits\n{\n  \"query\": {\n    \"knn\": {\n      \"query_vector\": [127, -127, 0, 1, 42],\n      \"field\": \"my_vector\"\n    }\n  }\n}\n--------------------------------------------------\n// TEST[continued]\n\n[source,console-result]\n----\n{\n    \"hits\": {\n        \"hits\": [\n            {\n                \"_index\": \"my-bit-vectors\",\n                \"_id\": \"1\",\n                \"_score\": 1.0,\n                \"_source\": {\n                    \"my_vector\": [\n                        127,\n                        -127,\n                        0,\n                        1,\n                        42\n                    ]\n                }\n            },\n            {\n                \"_index\": \"my-bit-vectors\",\n                \"_id\": \"2\",\n                \"_score\": 0.55,\n                \"_source\": {\n                    \"my_vector\": \"8100012a7f\"\n                }\n            }\n        ]\n    }\n}\n----\n\n==== Updatable field type\n\nTo better accommodate scaling and performance needs, updating the `type` setting in `index_options` is possible with the <<indices-put-mapping,Update Mapping API>>, according to the following graph (jumps allowed):\n\n[source,txt]\n----\nflat --> int8_flat --> int4_flat --> hnsw --> int8_hnsw --> int4_hnsw\n----\n\nFor updating all HNSW types (`hnsw`, `int8_hnsw`, `int4_hnsw`) the number of connections `m` must either stay the same or increase. For scalar quantized formats  (`int8_flat`, `int4_flat`, `int8_hnsw`, `int4_hnsw`) the `confidence_interval` must always be consistent (once defined, it cannot change).\n\nUpdating `type` in `index_options` will fail in all other scenarios.\n\nSwitching `types` won't re-index vectors that have already been indexed (they will keep using their original `type`), vectors being indexed after the change will use the new `type` instead.\n\nFor example, it's possible to define a dense vector field that utilizes the `flat` type (raw float32 arrays) for a first batch of data to be indexed.\n\n[source,console]\n--------------------------------------------------\nPUT my-index-000001\n{\n    \"mappings\": {\n        \"properties\": {\n            \"text_embedding\": {\n                \"type\": \"dense_vector\",\n                \"dims\": 384,\n                \"index_options\": {\n                    \"type\": \"flat\"\n                }\n            }\n        }\n    }\n}\n--------------------------------------------------\n\nChanging the `type` to `int4_hnsw` makes sure vectors indexed after the change will use an int4 scalar quantized representation and HNSW (e.g., for KNN queries).\nThat includes new segments created by <<index-modules-merge,merging>> previously created segments.\n\n[source,console]\n--------------------------------------------------\nPUT /my-index-000001/_mapping\n{\n    \"properties\": {\n        \"text_embedding\": {\n            \"type\": \"dense_vector\",\n            \"dims\": 384,\n            \"index_options\": {\n                \"type\": \"int4_hnsw\"\n            }\n        }\n    }\n}\n--------------------------------------------------\n// TEST[setup:my_index]\n\nVectors indexed before this change will keep using the `flat` type (raw float32 representation and brute force search for KNN queries).\n\nIn order to have all the vectors updated to the new type, either reindexing or force merging should be used.\n\nFor debugging purposes, it's possible to inspect how many segments (and docs) exist for each `type` with the <<index-segments-api-request,Index Segments API>>.\n"
}