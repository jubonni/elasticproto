{
    "meta": {
        "timestamp": "2024-11-01T03:07:10.562273",
        "size": 10653,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-word-delimiter-tokenfilter.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "analysis-word-delimiter-tokenfilter",
        "version": "8.15"
    },
    "doc": "[[analysis-word-delimiter-tokenfilter]]\n=== Word delimiter token filter\n++++\n<titleabbrev>Word delimiter</titleabbrev>\n++++\n\n[WARNING]\n====\nWe recommend using the\n<<analysis-word-delimiter-graph-tokenfilter,`word_delimiter_graph`>> instead of\nthe `word_delimiter` filter.\n\nThe `word_delimiter` filter can produce invalid token graphs. See\n<<analysis-word-delimiter-graph-differences>>.\n\nThe `word_delimiter` filter also uses Lucene's\n{lucene-analysis-docs}/miscellaneous/WordDelimiterFilter.html[WordDelimiterFilter],\nwhich is marked as deprecated. \n====\n\nSplits tokens at non-alphanumeric characters. The `word_delimiter` filter\nalso performs optional token normalization based on a set of rules. By default,\nthe filter uses the following rules:\n\n* Split tokens at non-alphanumeric characters.\n  The filter uses these characters as delimiters.\n  For example: `Super-Duper` -> `Super`, `Duper`\n* Remove leading or trailing delimiters from each token.\n  For example: `XL---42+'Autocoder'` -> `XL`, `42`, `Autocoder`\n* Split tokens at letter case transitions.\n  For example: `PowerShot` -> `Power`, `Shot`\n* Split tokens at letter-number transitions.\n  For example: `XL500` -> `XL`, `500`\n* Remove the English possessive (`'s`) from the end of each token.\n  For example: `Neil's` -> `Neil`\n\n[TIP]\n====\nThe `word_delimiter` filter was designed to remove punctuation from complex\nidentifiers, such as product IDs or part numbers. For these use cases, we\nrecommend using the `word_delimiter` filter with the\n<<analysis-keyword-tokenizer,`keyword`>> tokenizer.\n\nAvoid using the `word_delimiter` filter to split hyphenated words, such as\n`wi-fi`. Because users often search for these words both with and without\nhyphens, we recommend using the\n<<analysis-synonym-graph-tokenfilter,`synonym_graph`>> filter instead.\n====\n\n[[analysis-word-delimiter-tokenfilter-analyze-ex]]\n==== Example\n\nThe following <<indices-analyze,analyze API>> request uses the\n`word_delimiter` filter to split `Neil's-Super-Duper-XL500--42+AutoCoder`\ninto normalized tokens using the filter's default rules:\n\n[source,console]\n----\nGET /_analyze\n{\n  \"tokenizer\": \"keyword\",\n  \"filter\": [ \"word_delimiter\" ],\n  \"text\": \"Neil's-Super-Duper-XL500--42+AutoCoder\"\n}\n----\n\nThe filter produces the following tokens:\n\n[source,txt]\n----\n[ Neil, Super, Duper, XL, 500, 42, Auto, Coder ]\n----\n\n////\n[source,console-result]\n----\n{\n  \"tokens\": [\n    {\n      \"token\": \"Neil\",\n      \"start_offset\": 0,\n      \"end_offset\": 4,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"Super\",\n      \"start_offset\": 7,\n      \"end_offset\": 12,\n      \"type\": \"word\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"Duper\",\n      \"start_offset\": 13,\n      \"end_offset\": 18,\n      \"type\": \"word\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"XL\",\n      \"start_offset\": 19,\n      \"end_offset\": 21,\n      \"type\": \"word\",\n      \"position\": 3\n    },\n    {\n      \"token\": \"500\",\n      \"start_offset\": 21,\n      \"end_offset\": 24,\n      \"type\": \"word\",\n      \"position\": 4\n    },\n    {\n      \"token\": \"42\",\n      \"start_offset\": 26,\n      \"end_offset\": 28,\n      \"type\": \"word\",\n      \"position\": 5\n    },\n    {\n      \"token\": \"Auto\",\n      \"start_offset\": 29,\n      \"end_offset\": 33,\n      \"type\": \"word\",\n      \"position\": 6\n    },\n    {\n      \"token\": \"Coder\",\n      \"start_offset\": 33,\n      \"end_offset\": 38,\n      \"type\": \"word\",\n      \"position\": 7\n    }\n  ]\n}\n----\n////\n\n[analysis-word-delimiter-tokenfilter-analyzer-ex]]\n==== Add to an analyzer\n\nThe following <<indices-create-index,create index API>> request uses the\n`word_delimiter` filter to configure a new\n<<analysis-custom-analyzer,custom analyzer>>.\n\n[source,console]\n----\nPUT /my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"keyword\",\n          \"filter\": [ \"word_delimiter\" ]\n        }\n      }\n    }\n  }\n}\n----\n\n[WARNING]\n====\nAvoid using the `word_delimiter` filter with tokenizers that remove punctuation,\nsuch as the <<analysis-standard-tokenizer,`standard`>> tokenizer. This could\nprevent the `word_delimiter` filter from splitting tokens correctly. It can also\ninterfere with the filter's configurable parameters, such as `catenate_all` or\n`preserve_original`. We recommend using the\n<<analysis-keyword-tokenizer,`keyword`>> or\n<<analysis-whitespace-tokenizer,`whitespace`>> tokenizer instead.\n====\n\n[[word-delimiter-tokenfilter-configure-parms]]\n==== Configurable parameters\n\n`catenate_all`::\n+\n--\n(Optional, Boolean)\nIf `true`, the filter produces catenated tokens for chains of alphanumeric\ncharacters separated by non-alphabetic delimiters. For example:\n`super-duper-xl-500` -> [ `super`, **`superduperxl500`**, `duper`, `xl`, `500`\n]. Defaults to `false`.\n\n[WARNING]\n====\nWhen used for search analysis, catenated tokens can cause problems for the\n<<query-dsl-match-query-phrase,`match_phrase`>> query and other queries that\nrely on token position for matching. Avoid setting this parameter to `true` if\nyou plan to use these queries.\n====\n--\n\n`catenate_numbers`::\n+\n--\n(Optional, Boolean)\nIf `true`, the filter produces catenated tokens for chains of numeric characters\nseparated by non-alphabetic delimiters. For example: `01-02-03` ->\n[ `01`, **`010203`**, `02`, `03` ]. Defaults to `false`.\n\n[WARNING]\n====\nWhen used for search analysis, catenated tokens can cause problems for the\n<<query-dsl-match-query-phrase,`match_phrase`>> query and other queries that\nrely on token position for matching. Avoid setting this parameter to `true` if\nyou plan to use these queries.\n====\n--\n\n`catenate_words`::\n+\n--\n(Optional, Boolean)\nIf `true`, the filter produces catenated tokens for chains of alphabetical\ncharacters separated by non-alphabetic delimiters. For example: `super-duper-xl`\n-> [ `super`, **`superduperxl`**, `duper`, `xl` ]. Defaults to `false`.\n\n[WARNING]\n====\nWhen used for search analysis, catenated tokens can cause problems for the\n<<query-dsl-match-query-phrase,`match_phrase`>> query and other queries that\nrely on token position for matching. Avoid setting this parameter to `true` if\nyou plan to use these queries.\n====\n--\n\n`generate_number_parts`::\n(Optional, Boolean)\nIf `true`, the filter includes tokens consisting of only numeric characters in\nthe output. If `false`, the filter excludes these tokens from the output.\nDefaults to `true`.\n\n`generate_word_parts`::\n(Optional, Boolean)\nIf `true`, the filter includes tokens consisting of only alphabetical characters\nin the output. If `false`, the filter excludes these tokens from the output.\nDefaults to `true`.\n\n`preserve_original`::\n(Optional, Boolean)\nIf `true`, the filter includes the original version of any split tokens in the\noutput. This original version includes non-alphanumeric delimiters. For example:\n`super-duper-xl-500` -> [ **`super-duper-xl-500`**, `super`, `duper`, `xl`,\n`500` ]. Defaults to `false`.\n\n`protected_words`::\n(Optional, array of strings)\nArray of tokens the filter won't split.\n\n`protected_words_path`::\n+\n--\n(Optional, string)\nPath to a file that contains a list of tokens the filter won't split.\n\nThis path must be absolute or relative to the `config` location, and the file\nmust be UTF-8 encoded. Each token in the file must be separated by a line\nbreak.\n--\n\n`split_on_case_change`::\n(Optional, Boolean)\nIf `true`, the filter splits tokens at letter case transitions. For example:\n`camelCase` -> [ `camel`, `Case` ]. Defaults to `true`.\n\n`split_on_numerics`::\n(Optional, Boolean)\nIf `true`, the filter splits tokens at letter-number transitions. For example:\n`j2se` -> [ `j`, `2`, `se` ]. Defaults to `true`.\n\n`stem_english_possessive`::\n(Optional, Boolean)\nIf `true`, the filter removes the English possessive (`'s`) from the end of each\ntoken. For example: `O'Neil's` -> [ `O`, `Neil` ]. Defaults to `true`.\n\n`type_table`::\n+\n--\n(Optional, array of strings)\nArray of custom type mappings for characters. This allows you to map\nnon-alphanumeric characters as numeric or alphanumeric to avoid splitting on\nthose characters.\n\nFor example, the following array maps the plus (`+`) and hyphen (`-`) characters\nas alphanumeric, which means they won't be treated as delimiters:\n\n`[ \"+ => ALPHA\", \"- => ALPHA\" ]`\n\nSupported types include:\n\n* `ALPHA` (Alphabetical)\n* `ALPHANUM` (Alphanumeric)\n* `DIGIT` (Numeric)\n* `LOWER` (Lowercase alphabetical)\n* `SUBWORD_DELIM` (Non-alphanumeric delimiter)\n* `UPPER` (Uppercase alphabetical)\n--\n\n`type_table_path`::\n+\n--\n(Optional, string)\nPath to a file that contains custom type mappings for characters. This allows\nyou to map non-alphanumeric characters as numeric or alphanumeric to avoid\nsplitting on those characters.\n\nFor example, the contents of this file may contain the following:\n\n[source,txt]\n----\n# Map the $, %, '.', and ',' characters to DIGIT\n# This might be useful for financial data.\n$ => DIGIT\n% => DIGIT\n. => DIGIT\n\\\\u002C => DIGIT\n\n# in some cases you might not want to split on ZWJ\n# this also tests the case where we need a bigger byte[]\n# see https://en.wikipedia.org/wiki/Zero-width_joiner\n\\\\u200D => ALPHANUM\n----\n\nSupported types include:\n\n* `ALPHA` (Alphabetical)\n* `ALPHANUM` (Alphanumeric)\n* `DIGIT` (Numeric)\n* `LOWER` (Lowercase alphabetical)\n* `SUBWORD_DELIM` (Non-alphanumeric delimiter)\n* `UPPER` (Uppercase alphabetical)\n\nThis file path must be absolute or relative to the `config` location, and the\nfile must be UTF-8 encoded. Each mapping in the file must be separated by a line\nbreak.\n--\n\n[[analysis-word-delimiter-tokenfilter-customize]]\n==== Customize\n\nTo customize the `word_delimiter` filter, duplicate it to create the basis\nfor a new custom token filter. You can modify the filter using its configurable\nparameters.\n\nFor example, the following request creates a `word_delimiter`\nfilter that uses the following rules:\n\n* Split tokens at non-alphanumeric characters, _except_ the hyphen (`-`)\n  character.\n* Remove leading or trailing delimiters from each token.\n* Do _not_ split tokens at letter case transitions.\n* Do _not_ split tokens at letter-number transitions.\n* Remove the English possessive (`'s`) from the end of each token.\n\n[source,console]\n----\nPUT /my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"keyword\",\n          \"filter\": [ \"my_custom_word_delimiter_filter\" ]\n        }\n      },\n      \"filter\": {\n        \"my_custom_word_delimiter_filter\": {\n          \"type\": \"word_delimiter\",\n          \"type_table\": [ \"- => ALPHA\" ],\n          \"split_on_case_change\": false,\n          \"split_on_numerics\": false,\n          \"stem_english_possessive\": true\n        }\n      }\n    }\n  }\n}\n----\n"
}