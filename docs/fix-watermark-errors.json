{
    "meta": {
        "timestamp": "2024-11-01T03:07:09.120272",
        "size": 4155,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/fix-watermark-errors.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "fix-watermark-errors",
        "version": "8.15"
    },
    "doc": "[[fix-watermark-errors]]\n=== Fix watermark errors\n\n++++\n<titleabbrev>Watermark errors</titleabbrev>\n++++\n:keywords: {es}, high watermark, low watermark, full disk, flood stage watermark\n\nWhen a data node is critically low on disk space and has reached the\n<<cluster-routing-flood-stage,flood-stage disk usage watermark>>, the following\nerror is logged: `Error: disk usage exceeded flood-stage watermark, index has read-only-allow-delete block`. \n\nTo prevent a full disk, when a node reaches this watermark, {es} <<index-block-settings,blocks writes>>\nto any index with a shard on the node. If the block affects related system\nindices, {kib} and other {stack} features may become unavailable. For example, \nthis could induce {kib}'s `Kibana Server is not Ready yet` \n{kibana-ref}/access.html#not-ready[error message]. \n\n{es} will automatically remove the write block when the affected node's disk\nusage falls below the <<cluster-routing-watermark-high,high disk watermark>>. \nTo achieve this, {es} attempts to rebalance some of the affected node's shards \nto other nodes in the same data tier.\n\n[[fix-watermark-errors-rebalance]]\n==== Monitor rebalancing\n\nTo verify that shards are moving off the affected node until it falls below high \nwatermark., use the <<cat-shards,cat shards API>> and <<cat-recovery,cat recovery API>>: \n\n[source,console]\n----\nGET _cat/shards?v=true\n\nGET _cat/recovery?v=true&active_only=true\n----\n\nIf shards remain on the node keeping it about high watermark, use the \n<<cluster-allocation-explain,cluster allocation explanation API>> to get an \nexplanation for their allocation status.\n\n[source,console]\n----\nGET _cluster/allocation/explain\n{\n  \"index\": \"my-index\",\n  \"shard\": 0,\n  \"primary\": false\n}\n----\n// TEST[s/^/PUT my-index\\n/]\n// TEST[s/\"primary\": false,/\"primary\": false/]\n\n[[fix-watermark-errors-temporary]]\n==== Temporary Relief\n\nTo immediately restore write operations, you can temporarily increase the \n<<disk-based-shard-allocation,disk watermarks>> and remove the \n<<index-block-settings,write block>>.\n\n[source,console]\n----\nPUT _cluster/settings\n{\n  \"persistent\": {\n    \"cluster.routing.allocation.disk.watermark.low\": \"90%\",\n    \"cluster.routing.allocation.disk.watermark.low.max_headroom\": \"100GB\",\n    \"cluster.routing.allocation.disk.watermark.high\": \"95%\",\n    \"cluster.routing.allocation.disk.watermark.high.max_headroom\": \"20GB\",\n    \"cluster.routing.allocation.disk.watermark.flood_stage\": \"97%\",\n    \"cluster.routing.allocation.disk.watermark.flood_stage.max_headroom\": \"5GB\",\n    \"cluster.routing.allocation.disk.watermark.flood_stage.frozen\": \"97%\",\n    \"cluster.routing.allocation.disk.watermark.flood_stage.frozen.max_headroom\": \"5GB\"\n  }\n}\n\nPUT */_settings?expand_wildcards=all\n{\n  \"index.blocks.read_only_allow_delete\": null\n}\n----\n// TEST[s/^/PUT my-index\\n/]\n\nWhen a long-term solution is in place, to reset or reconfigure the disk watermarks:\n\n[source,console]\n----\nPUT _cluster/settings\n{\n  \"persistent\": {\n    \"cluster.routing.allocation.disk.watermark.low\": null,\n    \"cluster.routing.allocation.disk.watermark.low.max_headroom\": null,\n    \"cluster.routing.allocation.disk.watermark.high\": null,\n    \"cluster.routing.allocation.disk.watermark.high.max_headroom\": null,\n    \"cluster.routing.allocation.disk.watermark.flood_stage\": null,\n    \"cluster.routing.allocation.disk.watermark.flood_stage.max_headroom\": null,\n    \"cluster.routing.allocation.disk.watermark.flood_stage.frozen\": null,\n    \"cluster.routing.allocation.disk.watermark.flood_stage.frozen.max_headroom\": null\n  }\n}\n----\n\n[[fix-watermark-errors-resolve]]\n==== Resolve\n\nAs a long-term solution, we recommend you do one of the following best suited \nto your use case: \n\n* add nodes to the affected <<data-tiers,data tiers>>\n\n* upgrade existing nodes to increase disk space\n+\nTIP: On {ess}, https://support.elastic.co[Elastic Support] intervention may \nbecome necessary if <<cluster-health,cluster health>> reaches `status:red`. \n\n* delete unneeded indices using the <<indices-delete-index,delete index API>>\n\n* update related <<index-lifecycle-management,ILM policy>> to push indices \nthrough to later <<data-tiers,data tiers>>\n"
}