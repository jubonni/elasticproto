{
    "meta": {
        "timestamp": "2024-11-01T03:02:52.814578",
        "size": 26188,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/elasticsearch-intro.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "elasticsearch-intro",
        "version": "8.15"
    },
    "doc": "[[elasticsearch-intro]]\n== {es} basics\n\nThis guide covers the core concepts you need to understand to get started with {es}.\nIf you'd prefer to start working with {es} right away, set up a <<run-elasticsearch-locally,local development environment>> and jump to <<quickstart,hands-on code examples>>.\n\nThis guide covers the following topics:\n\n* <<elasticsearch-intro-what-is-es>>: Learn about {es} and some of its main use cases.\n* <<elasticsearch-intro-deploy>>: Understand your options for deploying {es} in different environments, including a fast local development setup.\n* <<documents-indices>>: Understand {es}'s most important primitives and how it stores data.\n* <<es-ingestion-overview>>: Understand your options for ingesting data into {es}.\n* <<search-analyze>>: Understand your options for searching and analyzing data in {es}.\n* <<scalability>>: Understand the basic concepts required for moving your {es} deployment to production.\n\n[[elasticsearch-intro-what-is-es]]\n=== What is {es}?\n\n{es-repo}[{es}] is a distributed search and analytics engine, scalable data store, and vector database built on Apache Lucene.\nIt's optimized for speed and relevance on production-scale workloads.\nUse {es} to search, index, store, and analyze data of all shapes and sizes in near real time.\n\n{es} is the heart of the {estc-welcome-current}/stack-components.html[Elastic Stack]. \nCombined with https://www.elastic.co/kibana[{kib}], it powers the following Elastic solutions:\n\n* https://www.elastic.co/observability[Observability]\n* https://www.elastic.co/enterprise-search[Search]\n* https://www.elastic.co/security[Security]\n\n[TIP]\n====\n{es} has a lot of features. Explore the full list on the https://www.elastic.co/elasticsearch/features[product webpage^].\n====\n\n[discrete]\n[[elasticsearch-intro-elastic-stack]]\n.What is the Elastic Stack?\n*******************************\n{es} is the core component of the Elastic Stack, a suite of products for collecting, storing, searching, and visualizing data.\n{estc-welcome-current}/stack-components.html[Learn more about the Elastic Stack].\n*******************************\n\n[discrete]\n[[elasticsearch-intro-use-cases]]\n==== Use cases\n\n{es} is used for a wide and growing range of use cases. Here are a few examples:\n\n**Observability**\n\n* *Logs, metrics, and traces*: Collect, store, and analyze logs, metrics, and traces from applications, systems, and services.\n* *Application performance monitoring (APM)*: Monitor and analyze the performance of business-critical software applications.\n* *Real user monitoring (RUM)*: Monitor, quantify, and analyze user interactions with web applications.\n* *OpenTelemetry*: Reuse your existing instrumentation to send telemetry data to the Elastic Stack using the OpenTelemetry standard.\n\n**Search**\n\n* *Full-text search*: Build a fast, relevant full-text search solution using inverted indexes, tokenization, and text analysis.\n* *Vector database*: Store and search vectorized data, and create vector embeddings with built-in and third-party natural language processing (NLP) models.\n* *Semantic search*: Understand the intent and contextual meaning behind search queries using tools like synonyms, dense vector embeddings, and learned sparse query-document expansion.\n* *Hybrid search*: Combine full-text search with vector search using state-of-the-art ranking algorithms.\n* *Build search experiences*: Add hybrid search capabilities to apps or websites, or build enterprise search engines over your organization's internal data sources.\n* *Retrieval augmented generation (RAG)*: Use {es} as a retrieval engine to supplement generative AI models with more relevant, up-to-date, or proprietary data for a range of use cases.\n* *Geospatial search*: Search for locations and calculate spatial relationships using geospatial queries.\n\n**Security**\n\n* *Security information and event management (SIEM)*: Collect, store, and analyze security data from applications, systems, and services.\n* *Endpoint security*: Monitor and analyze endpoint security data.\n* *Threat hunting*: Search and analyze data to detect and respond to security threats.\n\nThis is just a sample of search, observability, and security use cases enabled by {es}.\nRefer to Elastic https://www.elastic.co/customers/success-stories[customer success stories] for concrete examples across a range of industries.\n\n[[elasticsearch-intro-deploy]]\n=== Run {es}\n\nTo use {es}, you need a running instance of the {es} service.\nYou can deploy {es} in various ways.\n\n**Quick start option**\n\n* <<run-elasticsearch-locally,*Local development*>>: Get started quickly with a minimal local Docker setup for development and testing.\n\n**Hosted options**\n\n* {cloud}/ec-getting-started-trial.html[*Elastic Cloud Hosted*]: {es} is available as part of the hosted Elastic Stack offering, deployed in the cloud with your provider of choice. Sign up for a https://cloud.elastic.co/registration[14-day free trial].\n* {serverless-docs}/general/sign-up-trial[*Elastic Cloud Serverless* (technical preview)]: Create serverless projects for autoscaled and fully managed {es} deployments. Sign up for a https://cloud.elastic.co/serverless-registration[14-day free trial].\n\n**Advanced options**\n\n* <<elasticsearch-deployment-options,*Self-managed*>>: Install, configure, and run {es} on your own premises.\n* {ece-ref}/Elastic-Cloud-Enterprise-overview.html[*Elastic Cloud Enterprise*]: Deploy Elastic Cloud on public or private clouds, virtual machines, or your own premises.\n* {eck-ref}/k8s-overview.html[*Elastic Cloud on Kubernetes*]: Deploy Elastic Cloud on Kubernetes.\n\n// new html page \n[[documents-indices]]\n=== Indices, documents, and fields\n++++\n<titleabbrev>Indices and documents</titleabbrev>\n++++\n\nThe index is the fundamental unit of storage in {es}, a logical namespace for storing data that share similar characteristics.\nAfter you have {es} <<elasticsearch-intro-deploy,deployed>>, you'll get started by creating an index to store your data.\n\nAn index is a collection of documents uniquely identified by a name or an <<aliases,alias>>.\nThis unique name is important because it's used to target the index in search queries and other operations.\n\n[TIP]\n====\nA closely related concept is a <<data-streams,data stream>>.\nThis index abstraction is optimized for append-only timestamped data, and is made up of hidden, auto-generated backing indices.\nIf you're working with timestamped data, we recommend the {observability-guide}[Elastic Observability] solution for additional tools and optimized content.\n====\n\n[discrete]\n[[elasticsearch-intro-documents-fields]]\n==== Documents and fields\n\n{es} serializes and stores data in the form of JSON documents.\nA document is a set of fields, which are key-value pairs that contain your data.\nEach document has a unique ID, which you can create or have {es} auto-generate.\n\nA simple {es} document might look like this:\n\n[source,js]\n----\n{\n  \"_index\": \"my-first-elasticsearch-index\",\n  \"_id\": \"DyFpo5EBxE8fzbb95DOa\",\n  \"_version\": 1,\n  \"_seq_no\": 0,\n  \"_primary_term\": 1,\n  \"found\": true,\n  \"_source\": {\n    \"email\": \"john@smith.com\",\n    \"first_name\": \"John\",\n    \"last_name\": \"Smith\",\n    \"info\": {\n      \"bio\": \"Eco-warrior and defender of the weak\",\n      \"age\": 25,\n      \"interests\": [\n        \"dolphins\",\n        \"whales\"\n      ]\n    },\n    \"join_date\": \"2024/05/01\"\n  }\n}\n----\n// NOTCONSOLE\n\n[discrete]\n[[elasticsearch-intro-documents-fields-data-metadata]]\n==== Metadata fields\n\nAn indexed document contains data and metadata. <<mapping-fields,Metadata fields>> are system fields that store information about the documents.\nIn {es}, metadata fields are prefixed with an underscore.\nFor example, the following fields are metadata fields:\n\n* `_index`: The name of the index where the document is stored.\n* `_id`: The document's ID. IDs must be unique per index.\n\n[discrete]\n[[elasticsearch-intro-documents-fields-mappings]]\n==== Mappings and data types\n\nEach index has a <<mapping,mapping>> or schema for how the fields in your documents are indexed.\nA mapping defines the <<mapping-types,data type>> for each field, how the field should be indexed,\nand how it should be stored.\nWhen adding documents to {es}, you have two options for mappings:\n\n* <<mapping-dynamic, Dynamic mapping>>: Let {es} automatically detect the data types and create the mappings for you. Dynamic mapping helps you get started quickly, but might yield suboptimal results for your specific use case due to automatic field type inference.\n* <<mapping-explicit, Explicit mapping>>: Define the mappings up front by specifying data types for each field. Recommended for production use cases, because you have full control over how your data is indexed to suit your specific use case.\n\n[TIP]\n====\nYou can use a combination of dynamic and explicit mapping on the same index.\nThis is useful when you have a mix of known and unknown fields in your data.\n====\n\n// New html page\n[[es-ingestion-overview]]\n=== Add data to {es}\n\nThere are multiple ways to ingest data into {es}.\nThe option that you choose depends on whether you're working with timestamped data or non-timestamped data, where the data is coming from, its complexity, and more.\n\n[TIP]\n====\nYou can load {kibana-ref}/connect-to-elasticsearch.html#_add_sample_data[sample data] into your {es} cluster using {kib}, to get started quickly.\n====\n\n[discrete]\n[[es-ingestion-overview-general-content]]\n==== General content\n\nGeneral content is data that does not have a timestamp.\nThis could be data like vector embeddings, website content, product catalogs, and more.\nFor general content, you have the following options for adding data to {es} indices:\n\n* <<docs,API>>: Use the {es} <<docs,Document APIs>> to index documents directly, using the Dev Tools {kibana-ref}/console-kibana.html[Console], or cURL.\n+\nIf you're building a website or app, then you can call Elasticsearch APIs using an https://www.elastic.co/guide/en/elasticsearch/client/index.html[{es} client] in the programming language of your choice. If you use the Python client, then check out the `elasticsearch-labs` repo for various https://github.com/elastic/elasticsearch-labs/tree/main/notebooks/search/python-examples[example notebooks]. \n* {kibana-ref}/connect-to-elasticsearch.html#upload-data-kibana[File upload]: Use the {kib} file uploader to index single files for one-off testing and exploration. The GUI guides you through setting up your index and field mappings.\n* https://github.com/elastic/crawler[Web crawler]: Extract and index web page content into {es} documents.\n* <<es-connectors,Connectors>>: Sync data from various third-party data sources to create searchable, read-only replicas in {es}.\n\n[discrete]\n[[es-ingestion-overview-timestamped]]\n==== Timestamped data\n\nTimestamped data in {es} refers to datasets that include a timestamp field. If you use the {ecs-ref}/ecs-reference.html[Elastic Common Schema (ECS)], this field is named `@timestamp`.\nThis could be data like logs, metrics, and traces.\n\nFor timestamped data, you have the following options for adding data to {es} data streams:\n\n* {fleet-guide}/fleet-overview.html[Elastic Agent and Fleet]: The preferred way to index timestamped data. Each Elastic Agent based integration includes default ingestion rules, dashboards, and visualizations to start analyzing your data right away.\nYou can use the Fleet UI in {kib} to centrally manage Elastic Agents and their policies.\n* {beats-ref}/beats-reference.html[Beats]: If your data source isn't supported by Elastic Agent, use Beats to collect and ship data to Elasticsearch. You install a separate Beat for each type of data to collect.\n* {logstash-ref}/introduction.html[Logstash]: Logstash is an open source data collection engine with real-time pipelining capabilities that supports a wide variety of data sources. You might use this option because neither Elastic Agent nor Beats supports your data source. You can also use Logstash to persist incoming data, or if you need to send the data to multiple destinations. \n* {cloud}/ec-ingest-guides.html[Language clients]: The linked tutorials demonstrate how to use {es} programming language clients to ingest data from an application. In these examples, {es} is running on Elastic Cloud, but the same principles apply to any {es} deployment.\n\n[TIP]\n====\nIf you're interested in data ingestion pipelines for timestamped data, use the decision tree in the {cloud}/ec-cloud-ingest-data.html#ec-data-ingest-pipeline[Elastic Cloud docs] to understand your options.\n====\n\n// New html page\n[[search-analyze]]\n=== Search and analyze data\n\nYou can use {es} as a basic document store to retrieve documents and their\nmetadata.\nHowever, the real power of {es} comes from its advanced search and analytics capabilities.\n\nYou'll use a combination of an API endpoint and a query language to interact with your data.\n\n[discrete]\n[[search-analyze-rest-api]]\n==== REST API\n\nUse REST APIs to manage your {es} cluster, and to index\nand search your data.\nFor testing purposes, you can submit requests\ndirectly from the command line or through the Dev Tools {kibana-ref}/console-kibana.html[Console] in {kib}.\nFrom your applications, you can use a\nhttps://www.elastic.co/guide/en/elasticsearch/client/index.html[client]\nin your programming language of choice.\n\nRefer to <<getting-started,first steps with Elasticsearch>> for a hands-on example of using the `_search` endpoint, adding data to {es}, and running basic searches in Query DSL syntax.\n\n[discrete]\n[[search-analyze-query-languages]]\n==== Query languages\n\n{es} provides a number of query languages for interacting with your data. \n\n*Query DSL* is the primary query language for {es} today.\n\n*{esql}* is a new piped query language and compute engine which was first added in version *8.11*.\n\n{esql} does not yet support all the features of Query DSL, like full-text search and semantic search.\nLook forward to new {esql} features and functionalities in each release.\n\nRefer to <<search-analyze-query-languages>> for a full overview of the query languages available in {es}.\n\n[discrete]\n[[search-analyze-query-dsl]]\n===== Query DSL\n\n<<query-dsl, Query DSL>> is a full-featured JSON-style query language that enables complex searching, filtering, and aggregations.\nIt is the original and most powerful query language for {es} today.\n\nThe <<search-your-data, `_search` endpoint>> accepts queries written in Query DSL syntax.\n\n[discrete]\n[[search-analyze-query-dsl-search-filter]]\n====== Search and filter with Query DSL\n\nQuery DSL support a wide range of search techniques, including the following:\n\n* <<full-text-queries,*Full-text search*>>: Search text that has been analyzed and indexed to support phrase or proximity queries, fuzzy matches, and more.\n* <<keyword,*Keyword search*>>: Search for exact matches using `keyword` fields.\n* <<semantic-search-semantic-text,*Semantic search*>>: Search `semantic_text` fields using dense or sparse vector search on embeddings generated in your {es} cluster.\n* <<knn-search,*Vector search*>>: Search for similar dense vectors using the kNN algorithm for embeddings generated outside of {es}.\n* <<geo-queries,*Geospatial search*>>: Search for locations and calculate spatial relationships using geospatial queries.\n\nLearn about the full range of queries supported by <<query-dsl,Query DSL>>. \n\nYou can also filter data using Query DSL.\nFilters enable you to include or exclude documents by retrieving documents that match specific field-level criteria.\nA query that uses the `filter` parameter indicates <<filter-context,filter context>>.\n\n[discrete]\n[[search-analyze-data-query-dsl]]\n====== Analyze with Query DSL\n\n<<search-aggregations,Aggregations>> are the primary tool for analyzing {es} data using Query DSL.\nAggregrations enable you to build complex summaries of your data and gain\ninsight into key metrics, patterns, and trends.\n\nBecause aggregations leverage the same data structures used for search, they are\nalso very fast. This enables you to analyze and visualize your data in real time.\nYou can search documents, filter results, and perform analytics at the same time, on the same\ndata, in a single request.\nThat means aggregations are calculated in the context of the search query.\n\nThe folowing aggregation types are available:\n\n* <<search-aggregations-metrics,Metric>>: Calculate metrics,\nsuch as a sum or average, from field values.\n* <<search-aggregations-bucket,Bucket>>: Group documents into buckets based on field values, ranges,\nor other criteria.\n* <<search-aggregations-pipeline,Pipeline>>: Run aggregations on the results of other aggregations.\n\nRun aggregations by specifying the <<search-search,search API>>'s `aggs` parameter.\nLearn more in <<run-an-agg,Run an aggregation>>.\n\n[discrete]\n[[search-analyze-data-esql]]\n===== {esql}\n\n<<esql,Elasticsearch Query Language ({esql})>> is a piped query language for filtering, transforming, and analyzing data.\n{esql} is built on top of a new compute engine, where search, aggregation, and transformation functions are\ndirectly executed within {es} itself.\n{esql} syntax can also be used within various {kib} tools.\n\nThe <<esql-rest,`_query` endpoint>> accepts queries written in {esql} syntax.\n\nToday, it supports a subset of the features available in Query DSL, like aggregations, filters, and transformations.\nIt does not yet support full-text search or semantic search.\n\nIt comes with a comprehensive set of <<esql-functions-operators,functions and operators>> for working with data and has robust integration with {kib}'s Discover, dashboards and visualizations.\n\nLearn more in <<esql-getting-started,Getting started with {esql}>>, or try https://www.elastic.co/training/introduction-to-esql[our training course].\n\n[discrete]\n[[search-analyze-data-query-languages-table]]\n==== List of available query languages\n\nThe following table summarizes all available {es} query languages, to help you choose the right one for your use case.\n\n[cols=\"1,2,2,1\", options=\"header\"]\n|===\n| Name | Description | Use cases | API endpoint\n\n| <<query-dsl,Query DSL>>\n| The primary query language for {es}. A powerful and flexible JSON-style language that enables complex queries.\n| Full-text search, semantic search, keyword search, filtering, aggregations, and more.\n| <<search-search,`_search`>>\n\n\n| <<esql,{esql}>>\n| Introduced in *8.11*, the Elasticsearch Query Language ({esql}) is a piped query language language for filtering, transforming, and analyzing data.\n| Initially tailored towards working with time series data like logs and metrics. \nRobust integration with {kib} for querying, visualizing, and analyzing data.\nDoes not yet support full-text search.\n| <<esql-rest,`_query`>>\n\n\n| <<eql,EQL>>\n| Event Query Language (EQL) is a query language for event-based time series data. Data must contain the `@timestamp` field to use EQL.\n| Designed for the threat hunting security use case.\n| <<eql-apis,`_eql`>>\n\n| <<xpack-sql,Elasticsearch SQL>>\n| Allows native, real-time SQL-like querying against {es} data. JDBC and ODBC drivers are available for integration with business intelligence (BI) tools.\n| Enables users familiar with SQL to query {es} data using familiar syntax for BI and reporting.\n| <<sql-apis,`_sql`>>\n\n| {kibana-ref}/kuery-query.html[Kibana Query Language (KQL)]\n| {kib} Query Language (KQL) is a text-based query language for filtering data when you access it through the {kib} UI.\n| Use KQL to filter documents where a value for a field exists, matches a given value, or is within a given range.\n| N/A\n\n|===\n\n// New html page\n[[scalability]]\n=== Get ready for production\n\nMany teams rely on {es} to run their key services. To keep these services running, you can design your {es} deployment\nto keep {es} available, even in case of large-scale outages. To keep it running fast, you also can design your\ndeployment to be responsive to production workloads.\n\n{es} is built to be always available and to scale with your needs. It does this using a distributed architecture.\nBy distributing your cluster, you can keep Elastic online and responsive to requests.\n\nIn case of failure, {es} offers tools for cross-cluster replication and cluster snapshots that can\nhelp you fall back or recover quickly. You can also use cross-cluster replication to serve requests based on the\ngeographic location of your users and your resources.\n\n{es} also offers security and monitoring tools to help you keep your cluster highly available.\n\n[discrete]\n[[use-multiple-nodes-shards]]\n==== Use multiple nodes and shards\n\n[NOTE]\n====\nNodes and shards are what make {es} distributed and scalable.\n\nThese concepts aren\u2019t essential if you\u2019re just getting started. How you <<elasticsearch-intro-deploy,deploy {es}>> in production determines what you need to know:\n\n* *Self-managed {es}*: You are responsible for setting up and managing nodes, clusters, shards, and replicas. This includes\nmanaging the underlying infrastructure, scaling, and ensuring high availability through failover and backup strategies.\n* *Elastic Cloud*: Elastic can autoscale resources in response to workload changes. Choose from different deployment types\nto apply sensible defaults for your use case. A basic understanding of nodes, shards, and replicas is still important.\n* *Elastic Cloud Serverless*: You don\u2019t need to worry about nodes, shards, or replicas. These resources are 100% automated\non the serverless platform, which is designed to scale with your workload.\n====\n\nYou can add servers (_nodes_) to a cluster to increase capacity, and {es} automatically distributes your data and query load\nacross all of the available nodes.\n\nElastic is able to distribute your data across nodes by subdividing an index into _shards_. Each index in {es} is a grouping\nof one or more physical shards, where each shard is a self-contained Lucene index containing a subset of the documents in\nthe index. By distributing the documents in an index across multiple shards, and distributing those shards across multiple\nnodes, {es} increases indexing and query capacity.\n\nThere are two types of shards: _primaries_ and _replicas_. Each document in an index belongs to one primary shard. A replica\nshard is a copy of a primary shard. Replicas maintain redundant copies of your data across the nodes in your cluster. \nThis protects against hardware failure and increases capacity to serve read requests like searching or retrieving a document.\n\n[TIP]\n====\nThe number of primary shards in an index is fixed at the time that an index is created, but the number of replica shards can\nbe changed at any time, without interrupting indexing or query operations.\n====\n\nShard copies in your cluster are automatically balanced across nodes to provide scale and high availability. All nodes are\naware of all the other nodes in the cluster and can forward client requests to the appropriate node. This allows {es}\nto distribute indexing and query load across the cluster.\n\nIf you\u2019re exploring {es} for the first time or working in a development environment, then you can use a cluster with a single node and create indices\nwith only one shard. However, in a production environment, you should build a cluster with multiple nodes and indices\nwith multiple shards to increase performance and resilience.\n\n// TODO - diagram\n\nTo learn about optimizing the number and size of shards in your cluster, refer to <<size-your-shards,Size your shards>>. \nTo learn about how read and write operations are replicated across shards and shard copies, refer to <<docs-replication,Reading and writing documents>>.\nTo adjust how shards are allocated and balanced across nodes, refer to <<shard-allocation-relocation-recovery,Shard allocation, relocation, and recovery>>.\n\n[discrete]\n[[ccr-disaster-recovery-geo-proximity]]\n==== CCR for disaster recovery and geo-proximity\n\nTo effectively distribute read and write operations across nodes, the nodes in a cluster need good, reliable connections\nto each other. To provide better connections, you typically co-locate the nodes in the same data center or nearby data centers.\n\nCo-locating nodes in a single location exposes you to the risk of a single outage taking your entire cluster offline. To\nmaintain high availability, you can prepare a second cluster that can take over in case of disaster by implementing\ncross-cluster replication (CCR).\n\nCCR provides a way to automatically synchronize indices from your primary cluster to a secondary remote cluster that\ncan serve as a hot backup. If the primary cluster fails, the secondary cluster can take over.\n\nYou can also use CCR to create secondary clusters to serve read requests in geo-proximity to your users.\n\nLearn more about <<xpack-ccr,cross-cluster replication>> and about <<high-availability-cluster-design,designing for resilience>>.\n\n[TIP]\n====\nYou can also take <<snapshot-restore,snapshots>> of your cluster that can be restored in case of failure.\n====\n\n[discrete]\n[[security-and-monitoring]]\n==== Security and monitoring\n\nAs with any enterprise system, you need tools to secure, manage, and monitor your {es} clusters. Security,\nmonitoring, and administrative features that are integrated into {es} enable you to use {kibana-ref}/introduction.html[Kibana] as a\ncontrol center for managing a cluster.\n\n<<secure-cluster,Learn about securing an {es} cluster>>.\n\n<<monitor-elasticsearch-cluster,Learn about monitoring your cluster>>.\n\n[discrete]\n[[cluster-design]]\n==== Cluster design\n\n{es} offers many options that allow you to configure your cluster to meet your organization\u2019s goals, requirements,\nand restrictions. You can review the following guides to learn how to tune your cluster to meet your needs:\n\n* <<high-availability-cluster-design,Designing for resilience>>\n* <<tune-for-indexing-speed,Tune for indexing speed>>\n* <<tune-for-search-speed,Tune for search speed>>\n* <<tune-for-disk-usage,Tune for disk usage>>\n* <<use-elasticsearch-for-time-series-data,Tune for time series data>>\n\nMany {es} options come with different performance considerations and trade-offs. The best way to determine the\noptimal configuration for your use case is through https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[testing with your own data and queries].\n"
}