{
    "meta": {
        "size": 4168,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-fingerprint-analyzer.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "analysis-fingerprint-analyzer",
        "version": "8.15"
    },
    "doc": "[[analysis-fingerprint-analyzer]]\n=== Fingerprint analyzer\n++++\n<titleabbrev>Fingerprint</titleabbrev>\n++++\n\nThe `fingerprint` analyzer implements a\nhttps://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth#fingerprint[fingerprinting algorithm]\nwhich is used by the OpenRefine project to assist in clustering.\n\nInput text is lowercased, normalized to remove extended characters, sorted,\ndeduplicated and concatenated into a single token. If a stopword list is\nconfigured, stop words will also be removed.\n\n[discrete]\n=== Example output\n\n[source,console]\n---------------------------\nPOST _analyze\n{\n  \"analyzer\": \"fingerprint\",\n  \"text\": \"Yes yes, G\u00f6del said this sentence is consistent and.\"\n}\n---------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"and consistent godel is said sentence this yes\",\n      \"start_offset\": 0,\n      \"end_offset\": 52,\n      \"type\": \"fingerprint\",\n      \"position\": 0\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe above sentence would produce the following single term:\n\n[source,text]\n---------------------------\n[ and consistent godel is said sentence this yes ]\n---------------------------\n\n[discrete]\n=== Configuration\n\nThe `fingerprint` analyzer accepts the following parameters:\n\n[horizontal]\n`separator`::\n\n    The character to use to concatenate the terms. Defaults to a space.\n\n`max_output_size`::\n\n    The maximum token size to emit. Defaults to `255`. Tokens larger than\n    this size will be discarded.\n\n`stopwords`::\n\n    A pre-defined stop words list like `_english_` or an array containing a\n    list of stop words. Defaults to `_none_`.\n\n`stopwords_path`::\n\n    The path to a file containing stop words.\n\nSee the <<analysis-stop-tokenfilter,Stop Token Filter>> for more information\nabout stop word configuration.\n\n\n[discrete]\n=== Example configuration\n\nIn this example, we configure the `fingerprint` analyzer to use the\npre-defined list of English stop words:\n\n[source,console]\n----------------------------\nPUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_fingerprint_analyzer\": {\n          \"type\": \"fingerprint\",\n          \"stopwords\": \"_english_\"\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_fingerprint_analyzer\",\n  \"text\": \"Yes yes, G\u00f6del said this sentence is consistent and.\"\n}\n----------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"consistent godel said sentence yes\",\n      \"start_offset\": 0,\n      \"end_offset\": 52,\n      \"type\": \"fingerprint\",\n      \"position\": 0\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe above example produces the following term:\n\n[source,text]\n---------------------------\n[ consistent godel said sentence yes ]\n---------------------------\n\n[discrete]\n=== Definition\n\nThe `fingerprint` tokenizer consists of:\n\nTokenizer::\n* <<analysis-standard-tokenizer,Standard Tokenizer>>\n\nToken Filters (in order)::\n* <<analysis-lowercase-tokenfilter,Lower Case Token Filter>>\n* <<analysis-asciifolding-tokenfilter>>\n* <<analysis-stop-tokenfilter,Stop Token Filter>> (disabled by default)\n* <<analysis-fingerprint-tokenfilter>>\n\nIf you need to customize the `fingerprint` analyzer beyond the configuration\nparameters then you need to recreate it as a `custom` analyzer and modify\nit, usually by adding token filters. This would recreate the built-in\n`fingerprint` analyzer and you can use it as a starting point for further\ncustomization:\n\n[source,console]\n----------------------------------------------------\nPUT /fingerprint_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"rebuilt_fingerprint\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"asciifolding\",\n            \"fingerprint\"\n          ]\n        }\n      }\n    }\n  }\n}\n----------------------------------------------------\n// TEST[s/\\n$/\\nstartyaml\\n  - compare_analyzers: {index: fingerprint_example, first: fingerprint, second: rebuilt_fingerprint}\\nendyaml\\n/]\n"
}