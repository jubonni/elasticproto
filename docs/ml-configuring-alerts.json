{
    "meta": {
        "size": 15344,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/ml-configuring-alerts.html",
        "type": "documentation",
        "role": [
            "screenshot",
            "screenshot",
            "screenshot",
            "screenshot",
            "screenshot",
            "screenshot"
        ],
        "has_code": false,
        "title": "ml-configuring-alerts",
        "version": "8.15"
    },
    "doc": "[[ml-configuring-alerts]]\n= Generating alerts for {anomaly-jobs}\n:frontmatter-description: Create {anomaly-detect} alert and {anomaly-jobs} health rules.\n:frontmatter-tags-products: [ml, alerting]\n:frontmatter-tags-content-type: [how-to]\n:frontmatter-tags-user-goals: [configure]\n\n{kib} {alert-features} include support for {ml} rules, which run scheduled \nchecks for anomalies in one or more {anomaly-jobs} or check the health of the \njob with certain conditions. If the conditions of the rule are met, an alert is \ncreated and the associated action is triggered. For example, you can create a \nrule to check an {anomaly-job} every fifteen minutes for critical anomalies and \nto notify you in an email. To learn more about {kib} {alert-features}, refer to \n{kibana-ref}/alerting-getting-started.html#alerting-getting-started[Alerting].\n\nThe following {ml} rules are available:\n\n{anomaly-detect-cap} alert:: \n  Checks if the {anomaly-job} results contain anomalies that match the rule \n  conditions.\n\n{anomaly-jobs-cap} health:: \n  Monitors job health and alerts if an operational issue occurred that may \n  prevent the job from detecting anomalies.\n\nTIP: If you have created rules for specific {anomaly-jobs} and you want to \nmonitor whether these jobs work as expected, {anomaly-jobs} health rules are \nideal for this purpose.\n\nIn *{stack-manage-app} > {rules-ui}*, you can create both types of {ml} rules.\nIn the *{ml-app}* app, you can create only {anomaly-detect} alert rules; create\nthem from the {anomaly-job} wizard after you start the job or from the\n{anomaly-job} list.\n\n[[creating-anomaly-alert-rules]]\n== {anomaly-detect-cap} alert rules\n\nWhen you create an {anomaly-detect} alert rule, you must select the job that\nthe rule applies to.\n\nYou must also select a type of {ml} result. In particular, you can create rules\nbased on bucket, record, or influencer results.\n\n[role=\"screenshot\"]\nimage::images/ml-anomaly-alert-severity.png[\"Selecting result type, severity, and test interval\", 500]\n// NOTE: This is an autogenerated screenshot. Do not edit it directly.\n\nFor each rule, you can configure the `anomaly_score` that triggers the action. \nThe `anomaly_score` indicates the significance of a given anomaly compared to \nprevious anomalies. The default severity threshold is 75 which means every \nanomaly with an `anomaly_score` of 75 or higher triggers the associated action.\n\nYou can select whether you want to include interim results. Interim results are \ncreated by the {anomaly-job} before a bucket is finalized. These results might \ndisappear after the bucket is fully processed. Include interim results if you \nwant to be notified earlier about a potential anomaly even if it might be a \nfalse positive. If you want to get notified only about anomalies of fully \nprocessed buckets, do not include interim results.\n\nYou can also configure advanced settings. _Lookback interval_ sets an interval \nthat is used to query previous anomalies during each condition check. Its value \nis derived from the bucket span of the job and the query delay of the {dfeed} by \ndefault. It is not recommended to set the lookback interval lower than the \ndefault value as it might result in missed anomalies. _Number of latest buckets_ \nsets how many buckets to check to obtain the highest anomaly from all the \nanomalies that are found during the _Lookback interval_. An alert is created \nbased on the anomaly with the highest anomaly score from the most anomalous \nbucket.\n\nYou can also test the configured conditions against your existing data and check \nthe sample results by providing a valid interval for your data. The generated \npreview contains the number of potentially created alerts during the relative \ntime range you defined.\n\nTIP: You must also provide a _check interval_ that defines how often to\nevaluate the rule conditions. It is recommended to select an interval that is\nclose to the bucket span of the job.\n\nAs the last step in the rule creation process, define its <<ml-configuring-alert-actions,actions>>.\n\n[[creating-anomaly-jobs-health-rules]]\n== {anomaly-jobs-cap} health rules\n\nWhen you create an {anomaly-jobs} health rule, you must select the job or group\nthat the rule applies to. If you assign more jobs to the group, they are\nincluded the next time the rule conditions are checked.\n\nYou can also use a special character (`*`) to apply the rule to all your jobs. \nJobs created after the rule are automatically included. You can exclude jobs \nthat are not critically important by using the _Exclude_ field.\n\nEnable the health check types that you want to apply. All checks are enabled by \ndefault. At least one check needs to be enabled to create the rule. The \nfollowing health checks are available:\n\n_Datafeed is not started_:: \n  Notifies if the corresponding {dfeed} of the job is not started but the job is \n  in an opened state. The notification message recommends the necessary \n  actions to solve the error.\n_Model memory limit reached_:: \n  Notifies if the model memory status of the job reaches the soft or hard model \n  memory limit. Optimize your job by following \n  <<detector-configuration,these guidelines>> or consider \n  <<set-model-memory-limit,amending the model memory limit>>. \n_Data delay has occurred_:: \n  Notifies when the job missed some data. You can define the threshold for the \n  amount of missing documents you get alerted on by setting \n  _Number of documents_. You can control the lookback interval for checking \n  delayed data with _Time interval_. Refer to the \n  <<ml-delayed-data-detection>> page to see what to do about delayed data.\n_Errors in job messages_:: \n  Notifies when the job messages contain error messages. Review the \n  notification; it contains the error messages, the corresponding job IDs and \n  recommendations on how to fix the issue. This check looks for job errors \n  that occur after the rule is created; it does not look at historic behavior.\n\n[role=\"screenshot\"]\nimage::images/ml-health-check-config.png[\"Selecting health checkers\",500]\n// NOTE: This is an autogenerated screenshot. Do not edit it directly.\n\nTIP: You must also provide a _check interval_ that defines how often to\nevaluate the rule conditions. It is recommended to select an interval that is\nclose to the bucket span of the job.\n\nAs the last step in the rule creation process, define its actions.\n\n[[ml-configuring-alert-actions]]\n== Actions\n\nYou can optionally send notifications when the rule conditions are met and when\nthey are no longer met. In particular, these rules support:\n\n* alert summaries\n* actions that run when the anomaly score matches the conditions (for {anomaly-detect} alert rules)\n* actions that run when an issue is detected (for {anomaly-jobs} health rules)\n* recovery actions that run when the conditions are no longer met\n\nEach action uses a connector, which stores connection information for a {kib}\nservice or supported third-party integration, depending on where you want to\nsend the notifications. For example, you can use a Slack connector to send a\nmessage to a channel. Or you can use an index connector that writes a JSON\nobject to a specific index. For details about creating connectors, refer to\n{kibana-ref}/action-types.html[Connectors].\n\nAfter you select a connector, you must set the action frequency. You can choose\nto create a summary of alerts on each check interval or on a custom interval.\nFor example, send slack notifications that summarize the new, ongoing, and\nrecovered alerts:\n\n[role=\"screenshot\"]\nimage::images/ml-anomaly-alert-action-summary.png[\"Adding an alert summary action to the rule\",500]\n// NOTE: This is an autogenerated screenshot. Do not edit it directly.\n\nTIP: If you choose a custom action interval, it cannot be shorter than the\nrule's check interval.\n\nAlternatively, you can set the action frequency such that actions run for each\nalert. Choose how often the action runs (at each check interval, only when the\nalert status changes, or at a custom action interval). For {anomaly-detect}\nalert rules, you must also choose whether the action runs when the anomaly score\nmatches the condition or when the alert recovers:\n\n[role=\"screenshot\"]\nimage::images/ml-anomaly-alert-action-score-matched.png[\"Adding an action for each alert in the rule\",500]\n// NOTE: This is an autogenerated screenshot. Do not edit it directly.\n\nIn {anomaly-jobs} health rules, choose whether the action runs when the issue is\ndetected or when it is recovered:\n\n[role=\"screenshot\"]\nimage::images/ml-health-check-action.png[\"Adding an action for each alert in the rule\",500]\n// NOTE: This is an autogenerated screenshot. Do not edit it directly.\n\nYou can further refine the rule by specifying that actions run only when they\nmatch a KQL query or when an alert occurs within a specific time frame.\n\nThere is a set of variables that you can use to customize the notification\nmessages for each action. Click the icon above the message text box to get the\nlist of variables or refer to <<action-variables>>. For example:\n\n[role=\"screenshot\"]\nimage::images/ml-anomaly-alert-messages.png[\"Customizing your message\",500]\n// NOTE: This is an autogenerated screenshot. Do not edit it directly.\n\nAfter you save the configurations, the rule appears in the\n*{stack-manage-app} > {rules-ui}* list; you can check its status and see the\noverview of its configuration information.\n\nWhen an alert occurs for an {anomaly-detect} alert rule, it is always the same\nname as the job ID of the associated {anomaly-job} that triggered it. You can\nreview how the alerts that are occured correlate with the {anomaly-detect}\nresults in the **Anomaly explorer** by using the **Anomaly timeline** swimlane\nand the **Alerts** panel.\n\nIf necessary, you can snooze rules to prevent them from generating actions. For\nmore details, refer to\n{kibana-ref}/create-and-manage-rules.html#controlling-rules[Snooze and disable rules].\n\n[[action-variables]]\n== Action variables\n\nThe following variables are specific to the {ml} rule types. An asterisk (`*`)\nmarks the variables that you can use in actions related to recovered alerts.\n\nYou can also specify {kibana-ref}/rule-action-variables.html[variables common to all rules].\n\n[[anomaly-alert-action-variables]]\n=== {anomaly-detect-cap} alert action variables\n\nEvery {anomaly-detect} alert has the following action variables:\n\n`context`.`anomalyExplorerUrl` ^*^::\nURL to open in the Anomaly Explorer.\n\n`context`.`isInterim`::\nIndicates if top hits contain interim results.\n\n`context`.`jobIds` ^*^::\nList of job IDs that triggered the alert.\n\n`context`.`message` ^*^::\nA preconstructed message for the alert.\n\n`context`.`score`::\nAnomaly score at the time of the notification action.\n\n`context`.`timestamp`::\nThe bucket timestamp of the anomaly.\n\n`context`.`timestampIso8601`::\nThe bucket timestamp of the anomaly in ISO8601 format.\n\n`context`.`topInfluencers`::\nThe list of top influencers.\n+\n.Properties of `context.topInfluencers`\n[%collapsible%open]\n====\n`influencer_field_name`::: \nThe field name of the influencer.\n\n`influencer_field_value`::: \nThe entity that influenced, contributed to, or was to blame for the anomaly.\n\n`score`:::\nThe influencer score. A normalized score between 0-100 which shows the \ninfluencer's overall contribution to the anomalies.\n====\n\n`context`.`topRecords`::\nThe list of top records.\n+\n.Properties of `context.topRecords`\n[%collapsible%open]\n====\n`actual`:::\nThe actual value for the bucket.\n\n`by_field_value`::: \nThe value of the by field.\n\n`field_name`::: \nCertain functions require a field to operate on, for example, `sum()`. For those \nfunctions, this value is the name of the field to be analyzed.\n\n`function`::: \nThe function in which the anomaly occurs, as specified in the detector \nconfiguration. For example, `max`.\n\n`over_field_name`::: \nThe field used to split the data.\n\n`partition_field_value`::: \nThe field used to segment the analysis.\n\n`score`:::\nA normalized score between 0-100, which is based on the probability of the \nanomalousness of this record.\n\n`typical`:::\nThe typical value for the bucket, according to analytical modeling.\n====\n\n[[anomaly-jobs-health-action-variables]]\n=== {anomaly-jobs-cap} health action variables\n\nEvery health check has two main variables: `context.message` and \n`context.results`. The properties of `context.results` may vary based on the \ntype of check. You can find the possible properties for all the checks below.\n\n==== _Datafeed is not started_ \n\n`context.message` ^*^::\nA preconstructed message for the alert.\n\n`context.results`::\nContains the following properties:\n+\n.Properties of `context.results`\n[%collapsible%open]\n====\n`datafeed_id` ^*^:::\nThe {dfeed} identifier.\n\n`datafeed_state` ^*^:::\nThe state of the {dfeed}. It can be `starting`, `started`, \n`stopping`, `stopped`.\n\n`job_id` ^*^:::\nThe job identifier.\n\n`job_state` ^*^:::\nThe state of the job. It can be `opening`, `opened`, `closing`, \n`closed`, or `failed`.\n====\n\n==== _Model memory limit reached_\n\n`context.message` ^*^::\nA preconstructed message for the rule.\n\n`context.results`::\nContains the following properties:\n+\n.Properties of `context.results` \n[%collapsible%open]\n====\n`job_id` ^*^:::\nThe job identifier.\n\n`memory_status` ^*^:::\nThe status of the mathematical model. It can have one of the following values:\n\n* `soft_limit`: The model used more than 60% of the configured memory limit and \n  older unused models will be pruned to free up space. In categorization jobs no \n  further category examples will be stored.\n* `hard_limit`: The model used more space than the configured memory limit. As a \n  result, not all incoming data was processed.\n\nThe `memory_status` is `ok` for recovered alerts.\n\n`model_bytes` ^*^:::\nThe number of bytes of memory used by the models.\n\n`model_bytes_exceeded` ^*^:::\nThe number of bytes over the high limit for memory usage at the last allocation \nfailure.\n\n`model_bytes_memory_limit` ^*^:::\nThe upper limit for model memory usage.\n\n`log_time` ^*^:::\nThe timestamp of the model size statistics according to server time. Time \nformatting is based on the {kib} settings.\n\n`peak_model_bytes` ^*^:::\nThe peak number of bytes of memory ever used by the model.\n====\n\n==== _Data delay has occurred_\n\n`context.message` ^*^::\nA preconstructed message for the rule.\n\n`context.results`::\nFor recovered alerts, `context.results` is either empty (when there is no \ndelayed data) or the same as for an active alert (when the number of missing \ndocuments is less than the _Number of documents_ treshold set by the user). \nContains the following properties:\n+\n.Properties of `context.results`\n[%collapsible%open]\n====\n`annotation` ^*^:::\nThe annotation corresponding to the data delay in the job.\n\n`end_timestamp` ^*^:::\nTimestamp of the latest finalized buckets with missing documents. Time \nformatting is based on the {kib} settings.\n\n`job_id` ^*^:::\nThe job identifier.\n\n`missed_docs_count` ^*^:::\nThe number of missed documents.\n====\n\n==== _Error in job messages_\n\n`context.message` ^*^::\nA preconstructed message for the rule.\n\n`context.results`::\nContains the following properties:\n+\n.Properties of `context.results`\n[%collapsible%open]\n====\n`timestamp`:::\nTimestamp of the latest finalized buckets with missing documents.\n\n`job_id`:::\nThe job identifier.\n\n`message`:::\nThe error message.\n\n`node_name`:::\nThe name of the node that runs the job.\n===="
}