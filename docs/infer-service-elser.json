{
    "meta": {
        "timestamp": "2024-11-01T03:02:53.490598",
        "size": 5537,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/infer-service-elser.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "infer-service-elser",
        "version": "8.15"
    },
    "doc": "[[infer-service-elser]]\n=== ELSER {infer} service\n\nCreates an {infer} endpoint to perform an {infer} task with the `elser` service.\nYou can also deploy ELSER by using the <<infer-service-elasticsearch>>.\n\nNOTE: The API request will automatically download and deploy the ELSER model if\nit isn't already downloaded.\n\n\n[discrete]\n[[infer-service-elser-api-request]]\n==== {api-request-title}\n\n`PUT /_inference/<task_type>/<inference_id>`\n\n[discrete]\n[[infer-service-elser-api-path-params]]\n==== {api-path-parms-title}\n\n`<inference_id>`::\n(Required, string)\ninclude::inference-shared.asciidoc[tag=inference-id]\n\n`<task_type>`::\n(Required, string)\ninclude::inference-shared.asciidoc[tag=task-type]\n+\n--\nAvailable task types:\n\n* `sparse_embedding`.\n--\n\n[discrete]\n[[infer-service-elser-api-request-body]]\n==== {api-request-body-title}\n\n`chunking_settings`::\n(Optional, object)\ninclude::inference-shared.asciidoc[tag=chunking-settings]\n\n`max_chunking_size`:::\n(Optional, integer)\ninclude::inference-shared.asciidoc[tag=chunking-settings-max-chunking-size]\n\n`overlap`:::\n(Optional, integer)\ninclude::inference-shared.asciidoc[tag=chunking-settings-overlap]\n\n`sentence_overlap`:::\n(Optional, integer)\ninclude::inference-shared.asciidoc[tag=chunking-settings-sentence-overlap]\n\n`strategy`:::\n(Optional, string)\ninclude::inference-shared.asciidoc[tag=chunking-settings-strategy]\n\n`service`::\n(Required, string)\nThe type of service supported for the specified task type. In this case,\n`elser`.\n\n`service_settings`::\n(Required, object)\ninclude::inference-shared.asciidoc[tag=service-settings]\n+\n--\nThese settings are specific to the `elser` service.\n--\n\n`adaptive_allocations`:::\n(Optional, object)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation]\n\n`enabled`::::\n(Optional, Boolean)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation-enabled]\n\n`max_number_of_allocations`::::\n(Optional, integer)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation-max-number]\n\n`min_number_of_allocations`::::\n(Optional, integer)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation-min-number] \n\n`num_allocations`:::\n(Required, integer)\nThe total number of allocations this model is assigned across machine learning nodes.\nIncreasing this value generally increases the throughput.\nIf `adaptive_allocations` is enabled, do not set this value, because it's automatically set.\n\n`num_threads`:::\n(Required, integer)\nSets the number of threads used by each model allocation during inference. This generally increases the speed per inference request. The inference process is a compute-bound process; `threads_per_allocations` must not exceed the number of available allocated processors per node.\nMust be a power of 2. Max allowed value is 32.\n\n\n[discrete]\n[[inference-example-elser]]\n==== ELSER service example\n\nThe following example shows how to create an {infer} endpoint called `my-elser-model` to perform a `sparse_embedding` task type.\nRefer to the {ml-docs}/ml-nlp-elser.html[ELSER model documentation] for more info.\n\nNOTE: If you want to optimize your ELSER endpoint for ingest, set the number of threads to `1` (`\"num_threads\": 1`).\nIf you want to optimize your ELSER endpoint for search, set the number of threads to greater than `1`.\n\nThe request below will automatically download the ELSER model if it isn't already downloaded and then deploy the model.\n\n[source,console]\n------------------------------------------------------------\nPUT _inference/sparse_embedding/my-elser-model\n{\n  \"service\": \"elser\",\n  \"service_settings\": {\n    \"num_allocations\": 1,\n    \"num_threads\": 1\n  }\n}\n------------------------------------------------------------\n// TEST[skip:TBD]\n\nExample response:\n\n[source,console-result]\n------------------------------------------------------------\n{\n  \"inference_id\": \"my-elser-model\",\n  \"task_type\": \"sparse_embedding\",\n  \"service\": \"elser\",\n  \"service_settings\": {\n    \"num_allocations\": 1,\n    \"num_threads\": 1\n  },\n  \"task_settings\": {}\n}\n------------------------------------------------------------\n// NOTCONSOLE\n\n[NOTE]\n====\nYou might see a 502 bad gateway error in the response when using the {kib} Console.\nThis error usually just reflects a timeout, while the model downloads in the background.\nYou can check the download progress in the {ml-app} UI.\nIf using the Python client, you can set the `timeout` parameter to a higher value.\n====\n\n[discrete]\n[[inference-example-elser-adaptive-allocation]]\n==== Setting adaptive allocations for the ELSER service\n\nNOTE: For more information on how to optimize your ELSER endpoints, refer to {ml-docs}/ml-nlp-elser.html#elser-recommendations[the ELSER recommendations] section in the model documentation.\nTo learn more about model autoscaling, refer to the {ml-docs}/ml-nlp-auto-scale.html[trained model autoscaling] page.\n\nThe following example shows how to create an {infer} endpoint called `my-elser-model` to perform a `sparse_embedding` task type and configure adaptive allocations.\n\nThe request below will automatically download the ELSER model if it isn't already downloaded and then deploy the model.\n\n[source,console]\n------------------------------------------------------------\nPUT _inference/sparse_embedding/my-elser-model\n{\n  \"service\": \"elser\",\n  \"service_settings\": {\n    \"adaptive_allocations\": {\n      \"enabled\": true,\n      \"min_number_of_allocations\": 3,\n      \"max_number_of_allocations\": 10\n    },\n    \"num_threads\": 1\n  }\n}\n------------------------------------------------------------\n// TEST[skip:TBD]"
}