{
    "meta": {
        "size": 5031,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/semantic-search-deployed-nlp-model.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "semantic-search-deployed-nlp-model",
        "version": "8.15"
    },
    "doc": "[[semantic-search-deployed-nlp-model]]\n=== Tutorial: semantic search with a deployed model\n\n++++\n<titleabbrev>Semantic search with deployed model</titleabbrev>\n++++\n\n[IMPORTANT]\n====\n* For the easiest way to perform semantic search in the {stack}, refer to the <<semantic-search-semantic-text, `semantic_text`>> end-to-end tutorial.\n* This tutorial was written before the <<inference-apis,{infer} endpoint>> and <<semantic-text,`semantic_text` field type>> was introduced.\nToday we have simpler options for performing semantic search. \n====\n\nThis guide shows you how to implement semantic search with models deployed in {es}: from selecting an NLP model, to writing queries.\n\n\n[discrete]\n[[deployed-select-nlp-model]]\n==== Select an NLP model\n\n{es} offers the usage of a {ml-docs}/ml-nlp-model-ref.html#ml-nlp-model-ref-text-embedding[wide range of NLP models], including both dense and sparse vector models.\nYour choice of the language model is critical for implementing semantic search successfully.\n\nWhile it is possible to bring your own text embedding model, achieving good search results through model tuning is challenging.\nSelecting an appropriate model from our third-party model list is the first step.\nTraining the model on your own data is essential to ensure better search results than using only BM25.\nHowever, the model training process requires a team of data scientists and ML experts, making it expensive and time-consuming.\n\nTo address this issue, Elastic provides a pre-trained representational model called {ml-docs}/ml-nlp-elser.html[Elastic Learned Sparse EncodeR (ELSER)].\nELSER, currently available only for English, is an out-of-domain sparse vector model that does not require fine-tuning.\nThis adaptability makes it suitable for various NLP use cases out of the box.\nUnless you have a team of ML specialists, it is highly recommended to use the ELSER model.\n\nIn the case of sparse vector representation, the vectors mostly consist of zero values, with only a small subset containing non-zero values.\nThis representation is commonly used for textual data.\nIn the case of ELSER, each document in an index and the query text itself are represented by high-dimensional sparse vectors.\nEach non-zero element of the vector corresponds to a term in the model vocabulary.\nThe ELSER vocabulary contains around 30000 terms, so the sparse vectors created by ELSER contain about 30000 values, the majority of which are zero.\nEffectively the ELSER model is replacing the terms in the original query with other terms that have been learnt to exist in the documents that best match the original search terms in a training dataset, and weights to control how important each is.\n\n\n[discrete]\n[[deployed-deploy-nlp-model]]\n==== Deploy the model\n\nAfter you decide which model you want to use for implementing semantic search, you need to deploy the model in {es}.\n\ninclude::{es-ref-dir}/tab-widgets/semantic-search/deploy-nlp-model-widget.asciidoc[]\n\n\n[discrete]\n[[deployed-field-mappings]]\n==== Map a field for the text embeddings\n\nBefore you start using the deployed model to generate embeddings based on your input text, you need to prepare your index mapping first.\nThe mapping of the index depends on the type of model.\n\ninclude::{es-ref-dir}/tab-widgets/semantic-search/field-mappings-widget.asciidoc[]\n\n\n[discrete]\n[[deployed-generate-embeddings]]\n==== Generate text embeddings\n\nOnce you have created the mappings for the index, you can generate text embeddings from your input text.\nThis can be done by using an\n<<ingest,ingest pipeline>> with an <<inference-processor,inference processor>>.\nThe ingest pipeline processes the input data and indexes it into the destination index.\nAt index time, the inference ingest processor uses the trained model to infer against the data ingested through the pipeline.\nAfter you created the ingest pipeline with the inference processor, you can ingest your data through it to generate the model output.\n\ninclude::{es-ref-dir}/tab-widgets/semantic-search/generate-embeddings-widget.asciidoc[]\n\nNow it is time to perform semantic search!\n\n\n[discrete]\n[[deployed-search]]\n==== Search the data\n\nDepending on the type of model you have deployed, you can query rank features with a <<query-dsl-sparse-vector-query, sparse vector>> query, or dense vectors with a kNN search.\n\ninclude::{es-ref-dir}/tab-widgets/semantic-search/search-widget.asciidoc[]\n\n\n[discrete]\n[[deployed-hybrid-search]]\n==== Beyond semantic search with hybrid search\n\nIn some situations, lexical search may perform better than semantic search.\nFor example, when searching for single words or IDs, like product numbers.\n\nCombining semantic and lexical search into one hybrid search request using <<rrf,reciprocal rank fusion>> provides the best of both worlds.\nNot only that, but hybrid search using reciprocal rank fusion {blog-ref}improving-information-retrieval-elastic-stack-hybrid[has been shown to perform better in general].\n\ninclude::{es-ref-dir}/tab-widgets/semantic-search/hybrid-search-widget.asciidoc[]"
}