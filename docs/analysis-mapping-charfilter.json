{
    "meta": {
        "timestamp": "2024-11-01T03:02:52.898579",
        "size": 3839,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-mapping-charfilter.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "analysis-mapping-charfilter",
        "version": "8.15"
    },
    "doc": "[[analysis-mapping-charfilter]]\n=== Mapping character filter\n++++\n<titleabbrev>Mapping</titleabbrev>\n++++\n\nThe `mapping` character filter accepts a map of keys and values. Whenever it\nencounters a string of characters that is the same as a key, it replaces them\nwith the value associated with that key.\n\nMatching is greedy; the longest pattern matching at a given point wins.\nReplacements are allowed to be the empty string.\n\nThe `mapping` filter uses Lucene's\n{lucene-analysis-docs}/charfilter/MappingCharFilter.html[MappingCharFilter].\n\n[[analysis-mapping-charfilter-analyze-ex]]\n==== Example\n\nThe following <<indices-analyze,analyze API>> request uses the `mapping` filter\nto convert Hindu-Arabic numerals (\u0660\u200e\u0661\u0662\u0663\u0664\u0665\u0666\u0667\u0668\u200e\u0669\u200e) into their Arabic-Latin\nequivalents (0123456789), changing the text `My license plate is \u0662\u0665\u0660\u0661\u0665` to\n`My license plate is 25015`.\n\n[source,console]\n----\nGET /_analyze\n{\n  \"tokenizer\": \"keyword\",\n  \"char_filter\": [\n    {\n      \"type\": \"mapping\",\n      \"mappings\": [\n        \"\u0660 => 0\",\n        \"\u0661 => 1\",\n        \"\u0662 => 2\",\n        \"\u0663 => 3\",\n        \"\u0664 => 4\",\n        \"\u0665 => 5\",\n        \"\u0666 => 6\",\n        \"\u0667 => 7\",\n        \"\u0668 => 8\",\n        \"\u0669 => 9\"\n      ]\n    }\n  ],\n  \"text\": \"My license plate is \u0662\u0665\u0660\u0661\u0665\"\n}\n----\n\nThe filter produces the following text:\n\n[source,text]\n----\n[ My license plate is 25015 ]\n----\n\n////\n[source,console-result]\n----\n{\n  \"tokens\": [\n    {\n      \"token\": \"My license plate is 25015\",\n      \"start_offset\": 0,\n      \"end_offset\": 25,\n      \"type\": \"word\",\n      \"position\": 0\n    }\n  ]\n}\n----\n////\n\n[[analysis-mapping-charfilter-configure-parms]]\n==== Configurable parameters\n\n`mappings`::\n(Required*, array of strings)\nArray of mappings, with each element having the form `key => value`.\n+\nEither this or the `mappings_path` parameter must be specified.\n\n`mappings_path`::\n(Required*, string)\nPath to a file containing `key => value` mappings.\n+\nThis path must be absolute or relative to the `config` location, and the file\nmust be UTF-8 encoded. Each mapping in the file must be separated by a line\nbreak.\n+\nEither this or the `mappings` parameter must be specified.\n\n[[analysis-mapping-charfilter-customize]]\n==== Customize and add to an analyzer\n\nTo customize the `mappings` filter, duplicate it to create the basis for a new\ncustom character filter. You can modify the filter using its configurable\nparameters.\n\nThe following <<indices-create-index,create index API>> request\nconfigures a new <<analysis-custom-analyzer,custom analyzer>> using a custom\n`mappings` filter, `my_mappings_char_filter`.\n\nThe `my_mappings_char_filter` filter replaces the `:)` and `:(` emoticons\nwith a text equivalent.\n\n[source,console]\n----\nPUT /my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"standard\",\n          \"char_filter\": [\n            \"my_mappings_char_filter\"\n          ]\n        }\n      },\n      \"char_filter\": {\n        \"my_mappings_char_filter\": {\n          \"type\": \"mapping\",\n          \"mappings\": [\n            \":) => _happy_\",\n            \":( => _sad_\"\n          ]\n        }\n      }\n    }\n  }\n}\n----\n\nThe following <<indices-analyze,analyze API>> request uses the custom\n`my_mappings_char_filter` to replace `:(` with `_sad_` in\nthe text `I'm delighted about it :(`.\n\n[source,console]\n----\nGET /my-index-000001/_analyze\n{\n  \"tokenizer\": \"keyword\",\n  \"char_filter\": [ \"my_mappings_char_filter\" ],\n  \"text\": \"I'm delighted about it :(\"\n}\n----\n// TEST[continued]\n\nThe filter produces the following text:\n\n[source,text]\n---------------------------\n[ I'm delighted about it _sad_ ]\n---------------------------\n\n////\n[source,console-result]\n----\n{\n  \"tokens\": [\n    {\n      \"token\": \"I'm delighted about it _sad_\",\n      \"start_offset\": 0,\n      \"end_offset\": 25,\n      \"type\": \"word\",\n      \"position\": 0\n    }\n  ]\n}\n----\n////\n"
}