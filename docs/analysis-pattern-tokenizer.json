{
    "meta": {
        "size": 5661,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pattern-tokenizer.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "analysis-pattern-tokenizer",
        "version": "8.15"
    },
    "doc": "[[analysis-pattern-tokenizer]]\n=== Pattern tokenizer\n++++\n<titleabbrev>Pattern</titleabbrev>\n++++\n\nThe `pattern` tokenizer uses a regular expression to either split text into\nterms whenever it matches a word separator, or to capture matching text as\nterms.\n\nThe default pattern is `\\W+`, which splits text whenever it encounters\nnon-word characters.\n\n[WARNING]\n.Beware of Pathological Regular Expressions\n========================================\n\nThe pattern tokenizer uses\nhttps://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html[Java Regular Expressions].\n\nA badly written regular expression could run very slowly or even throw a\nStackOverflowError and cause the node it is running on to exit suddenly.\n\nRead more about https://www.regular-expressions.info/catastrophic.html[pathological regular expressions and how to avoid them].\n\n========================================\n\n[discrete]\n=== Example output\n\n[source,console]\n---------------------------\nPOST _analyze\n{\n  \"tokenizer\": \"pattern\",\n  \"text\": \"The foo_bar_size's default is 5.\"\n}\n---------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"The\",\n      \"start_offset\": 0,\n      \"end_offset\": 3,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"foo_bar_size\",\n      \"start_offset\": 4,\n      \"end_offset\": 16,\n      \"type\": \"word\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"s\",\n      \"start_offset\": 17,\n      \"end_offset\": 18,\n      \"type\": \"word\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"default\",\n      \"start_offset\": 19,\n      \"end_offset\": 26,\n      \"type\": \"word\",\n      \"position\": 3\n    },\n    {\n      \"token\": \"is\",\n      \"start_offset\": 27,\n      \"end_offset\": 29,\n      \"type\": \"word\",\n      \"position\": 4\n    },\n    {\n      \"token\": \"5\",\n      \"start_offset\": 30,\n      \"end_offset\": 31,\n      \"type\": \"word\",\n      \"position\": 5\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe above sentence would produce the following terms:\n\n[source,text]\n---------------------------\n[ The, foo_bar_size, s, default, is, 5 ]\n---------------------------\n\n[discrete]\n=== Configuration\n\nThe `pattern` tokenizer accepts the following parameters:\n\n[horizontal]\n`pattern`::\n\n    A https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html[Java regular expression], defaults to `\\W+`.\n\n`flags`::\n\n    Java regular expression https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html#field.summary[flags].\n    Flags should be pipe-separated, eg `\"CASE_INSENSITIVE|COMMENTS\"`.\n\n`group`::\n\n    Which capture group to extract as tokens. Defaults to `-1` (split).\n\n[discrete]\n=== Example configuration\n\nIn this example, we configure the `pattern` tokenizer to break text into\ntokens when it encounters commas:\n\n[source,console]\n----------------------------\nPUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"my_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"pattern\",\n          \"pattern\": \",\"\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"comma,separated,values\"\n}\n----------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"comma\",\n      \"start_offset\": 0,\n      \"end_offset\": 5,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"separated\",\n      \"start_offset\": 6,\n      \"end_offset\": 15,\n      \"type\": \"word\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"values\",\n      \"start_offset\": 16,\n      \"end_offset\": 22,\n      \"type\": \"word\",\n      \"position\": 2\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe above example produces the following terms:\n\n[source,text]\n---------------------------\n[ comma, separated, values ]\n---------------------------\n\nIn the next example, we configure the `pattern` tokenizer to capture values\nenclosed in double quotes (ignoring embedded escaped quotes `\\\"`). The regex\nitself looks like this:\n\n    \"((?:\\\\\"|[^\"]|\\\\\")*)\"\n\nAnd reads as follows:\n\n* A literal `\"`\n* Start capturing:\n** A literal `\\\"` OR any character except `\"`\n** Repeat until no more characters match\n* A literal closing `\"`\n\nWhen the pattern is specified in JSON, the `\"` and `\\` characters need to be\nescaped, so the pattern ends up looking like:\n\n    \\\"((?:\\\\\\\\\\\"|[^\\\"]|\\\\\\\\\\\")+)\\\"\n\n[source,console]\n----------------------------\nPUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"my_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"pattern\",\n          \"pattern\": \"\\\"((?:\\\\\\\\\\\"|[^\\\"]|\\\\\\\\\\\")+)\\\"\",\n          \"group\": 1\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"\\\"value\\\", \\\"value with embedded \\\\\\\" quote\\\"\"\n}\n----------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"value\",\n      \"start_offset\": 1,\n      \"end_offset\": 6,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"value with embedded \\\\\\\" quote\",\n      \"start_offset\": 10,\n      \"end_offset\": 38,\n      \"type\": \"word\",\n      \"position\": 1\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\nThe above example produces the following two terms:\n\n[source,text]\n---------------------------\n[ value, value with embedded \\\" quote ]\n---------------------------\n"
}