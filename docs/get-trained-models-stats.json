{
    "meta": {
        "timestamp": "2024-11-01T03:07:09.366272",
        "size": 12621,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/get-trained-models-stats.html",
        "type": "documentation",
        "role": [
            "xpack",
            "child_attributes"
        ],
        "has_code": true,
        "title": "get-trained-models-stats",
        "version": "8.15"
    },
    "doc": "[role=\"xpack\"]\n[[get-trained-models-stats]]\n= Get trained models statistics API\n[subs=\"attributes\"]\n++++\n<titleabbrev>Get trained models stats</titleabbrev>\n++++\n\nRetrieves usage information for trained models.\n\n\n[[ml-get-trained-models-stats-request]]\n== {api-request-title}\n\n`GET _ml/trained_models/_stats` +\n\n`GET _ml/trained_models/_all/_stats` +\n\n`GET _ml/trained_models/<model_id_or_deployment_id>/_stats` +\n\n`GET _ml/trained_models/<model_id_or_deployment_id>,<model_id_2_or_deployment_id_2>/_stats` +\n\n`GET _ml/trained_models/<model_id_pattern*_or_deployment_id_pattern*>,<model_id_2_or_deployment_id_2>/_stats`\n\n\n[[ml-get-trained-models-stats-prereq]]\n== {api-prereq-title}\n\nRequires the `monitor_ml` cluster privilege. This privilege is included in the\n`machine_learning_user` built-in role.\n\n\n[[ml-get-trained-models-stats-desc]]\n== {api-description-title}\n\nYou can get usage information for multiple trained models or trained model \ndeployments in a single API request by using a comma-separated list of model \nIDs, deployment IDs, or a wildcard expression.\n\n\n[[ml-get-trained-models-stats-path-params]]\n== {api-path-parms-title}\n\n`<model_id_or_deployment_id>`::\n(Optional, string)\nThe unique identifier of the model or the deployment. If a model has multiple \ndeployments, and the ID of one of the deployments matches the model ID, then the \nmodel ID takes precedence; the results are returned for all deployments of the \nmodel.\n\n[[ml-get-trained-models-stats-query-params]]\n== {api-query-parms-title}\n\n`allow_no_match`::\n(Optional, Boolean)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=allow-no-match-models]\n\n`from`::\n(Optional, integer)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=from-models]\n\n`size`::\n(Optional, integer)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=size-models]\n\n[role=\"child_attributes\"]\n[[ml-get-trained-models-stats-results]]\n== {api-response-body-title}\n\n`count`::\n(integer)\nThe total number of trained model statistics that matched the requested ID\npatterns. Could be higher than the number of items in the `trained_model_stats`\narray as the size of the array is restricted by the supplied `size` parameter.\n\n`trained_model_stats`::\n(array)\nAn array of trained model statistics, which are sorted by the `model_id` value\nin ascending order.\n+\n.Properties of trained model stats\n[%collapsible%open]\n====\n`deployment_stats`:::\n(list)\nA collection of deployment stats if one of the provided `model_id` values\nis deployed\n+\n.Properties of deployment stats\n[%collapsible%open]\n=====\n`allocation_status`:::\n(object)\nThe detailed allocation status given the deployment configuration.\n+\n.Properties of allocation stats\n[%collapsible%open]\n======\n`allocation_count`:::\n(integer)\nThe current number of nodes where the model is allocated.\n\n`cache_size`:::\n(<<byte-units,byte value>>)\nThe inference cache size (in memory outside the JVM heap) per node for the model.\n\n`state`:::\n(string)\nThe detailed allocation state related to the nodes.\n+\n--\n* `starting`: Allocations are being attempted but no node currently has the model allocated.\n* `started`: At least one node has the model allocated.\n* `fully_allocated`: The deployment is fully allocated and satisfies the `target_allocation_count`.\n--\n\n`target_allocation_count`:::\n(integer)\nThe desired number of nodes for model allocation.\n======\n\n`deployment_id`:::\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=deployment-id]\n\n`error_count`:::\n(integer)\nThe sum of `error_count` for all nodes in the deployment.\n\n`inference_count`:::\n(integer)\nThe sum of `inference_count` for all nodes in the deployment.\n\n`model_id`:::\n(string)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=model-id]\n\n`nodes`:::\n(array of objects)\nThe deployment stats for each node that currently has the model allocated.\n+\n.Properties of node stats\n[%collapsible%open]\n======\n`average_inference_time_ms`:::\n(double)\nThe average time for each inference call to complete on this node.\nThe average is calculated over the lifetime of the deployment.\n\n`average_inference_time_ms_excluding_cache_hits`:::\n(double)\nThe average time to perform inference on the trained model excluding\noccasions where the response comes from the cache. Cached inference\ncalls return very quickly as the model is not evaluated, by excluding\ncache hits this value is an accurate measure of the average time taken\nto evaluate the model.\n\n`average_inference_time_ms_last_minute`:::\n(double)\nThe average time for each inference call to complete on this node\nin the last minute.\n\n`error_count`:::\n(integer)\nThe number of errors when evaluating the trained model.\n\n`inference_cache_hit_count`:::\n(integer)\nThe total number of inference calls made against this node for this\nmodel that were served from the inference cache.\n\n`inference_cache_hit_count_last_minute`:::\n(integer)\nThe number of inference calls made against this node for this model\nin the last minute that were served from the inference cache.\n\n`inference_count`:::\n(integer)\nThe total number of inference calls made against this node for this model.\n\n`last_access`:::\n(long)\nThe epoch time stamp of the last inference call for the model on this node.\n\n`node`:::\n(object)\nInformation pertaining to the node.\n+\n.Properties of node\n[%collapsible%open]\n========\n`attributes`:::\n(object)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=node-attributes]\n\n`ephemeral_id`:::\n(string)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=node-ephemeral-id]\n\n`id`:::\n(string)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=node-id]\n\n`name`:::\n(string) The node name.\n\n`transport_address`:::\n(string)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=node-transport-address]\n========\n\n`number_of_allocations`:::\n(integer)\nThe number of allocations assigned to this node.\n\n`number_of_pending_requests`:::\n(integer)\nThe number of inference requests queued to be processed.\n\n`peak_throughput_per_minute`:::\n(integer)\nThe peak number of requests processed in a 1 minute period.\n\n`routing_state`:::\n(object)\nThe current routing state and reason for the current routing state for this allocation.\n+\n.Properties of routing_state\n[%collapsible%open]\n========\n`reason`:::\n(string)\nThe reason for the current state. Usually only populated when the `routing_state` is `failed`.\n\n`routing_state`:::\n(string)\nThe current routing state.\n--\n* `starting`: The model is attempting to allocate on this model, inference calls are not yet accepted.\n* `started`: The model is allocated and ready to accept inference requests.\n* `stopping`: The model is being deallocated from this node.\n* `stopped`: The model is fully deallocated from this node.\n* `failed`: The allocation attempt failed, see `reason` field for the potential cause.\n--\n========\n\n`rejected_execution_count`:::\n(integer)\nThe number of inference requests that were not processed because the\nqueue was full.\n\n`start_time`:::\n(long)\nThe epoch timestamp when the allocation started.\n\n`threads_per_allocation`:::\n(integer)\nThe number of threads for each allocation during inference.\nThis value is limited by the number of hardware threads on the node;\nit might therefore differ from the `threads_per_allocation` value in the <<start-trained-model-deployment>> API.\n\n`timeout_count`:::\n(integer)\nThe number of inference requests that timed out before being processed.\n\n`throughput_last_minute`:::\n(integer)\nThe number of requests processed in the last 1 minute.\n======\n\n`number_of_allocations`:::\n(integer)\nThe requested number of allocations for the trained model deployment.\n\n`peak_throughput_per_minute`:::\n(integer)\nThe peak number of requests processed in a 1 minute period for\nall nodes in the deployment. This is calculated as the sum of\neach node's `peak_throughput_per_minute` value.\n\n`priority`:::\n(string)\nThe deployment priority.\n\n`rejected_execution_count`:::\n(integer)\nThe sum of `rejected_execution_count` for all nodes in the deployment.\nIndividual nodes reject an inference request if the inference queue is full.\nThe queue size is controlled by the `queue_capacity` setting in the\n<<start-trained-model-deployment>> API.\n\n`reason`:::\n(string)\nThe reason for the current deployment state.\nUsually only populated when the model is not deployed to a node.\n\n`start_time`:::\n(long)\nThe epoch timestamp when the deployment started.\n\n`state`:::\n(string)\nThe overall state of the deployment. The values may be:\n+\n--\n* `starting`: The deployment has recently started but is not yet usable as the model is not allocated on any nodes.\n* `started`: The deployment is usable as at least one node has the model allocated.\n* `stopping`: The deployment is preparing to stop and deallocate the model from the relevant nodes.\n--\n\n`threads_per_allocation`:::\n(integer)\nThe number of threads per allocation used by the inference process.\n\n`timeout_count`:::\n(integer)\nThe sum of `timeout_count` for all nodes in the deployment.\n\n`queue_capacity`:::\n(integer)\nThe number of inference requests that may be queued before new requests are\nrejected.\n\n=====\n\n`inference_stats`:::\n(object)\nA collection of inference stats fields.\n+\n.Properties of inference stats\n[%collapsible%open]\n=====\n\n`missing_all_fields_count`:::\n(integer)\nThe number of inference calls where all the training features for the model\nwere missing.\n\n`inference_count`:::\n(integer)\nThe total number of times the model has been called for inference.\nThis is across all inference contexts, including all pipelines.\n\n`cache_miss_count`:::\n(integer)\nThe number of times the model was loaded for inference and was not retrieved\nfrom the cache. If this number is close to the `inference_count`, then the cache\nis not being appropriately used. This can be solved by increasing the cache size\nor its time-to-live (TTL). See <<general-ml-settings>> for the appropriate\nsettings.\n\n`failure_count`:::\n(integer)\nThe number of failures when using the model for inference.\n\n`timestamp`:::\n(<<time-units,time units>>)\nThe time when the statistics were last updated.\n=====\n\n`ingest`:::\n(object)\nA collection of ingest stats for the model across all nodes. The values are\nsummations of the individual node statistics. The format matches the `ingest`\nsection in <<cluster-nodes-stats>>.\n\n`model_id`:::\n(string)\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=model-id]\n\n`model_size_stats`:::\n(object)\nA collection of model size stats fields.\n+\n.Properties of model size stats\n[%collapsible%open]\n=====\n\n`model_size_bytes`:::\n(integer)\nThe size of the model in bytes.\n\n`required_native_memory_bytes`:::\n(integer)\nThe amount of memory required to load the model in bytes.\n=====\n\n`pipeline_count`:::\n(integer)\nThe number of ingest pipelines that currently refer to the model.\n====\n\n[[ml-get-trained-models-stats-response-codes]]\n== {api-response-codes-title}\n\n`404` (Missing resources)::\n  If `allow_no_match` is `false`, this code indicates that there are no\n  resources that match the request or only partial matches for the request.\n\n[[ml-get-trained-models-stats-example]]\n== {api-examples-title}\n\nThe following example gets usage information for all the trained models:\n\n[source,console]\n--------------------------------------------------\nGET _ml/trained_models/_stats\n--------------------------------------------------\n// TEST[skip:TBD]\n\n\nThe API returns the following results:\n\n[source,console-result]\n----\n{\n  \"count\": 2,\n  \"trained_model_stats\": [\n    {\n      \"model_id\": \"flight-delay-prediction-1574775339910\",\n      \"pipeline_count\": 0,\n      \"inference_stats\": {\n        \"failure_count\": 0,\n        \"inference_count\": 4,\n        \"cache_miss_count\": 3,\n        \"missing_all_fields_count\": 0,\n        \"timestamp\": 1592399986979\n      }\n    },\n    {\n      \"model_id\": \"regression-job-one-1574775307356\",\n      \"pipeline_count\": 1,\n      \"inference_stats\": {\n        \"failure_count\": 0,\n        \"inference_count\": 178,\n        \"cache_miss_count\": 3,\n        \"missing_all_fields_count\": 0,\n        \"timestamp\": 1592399986979\n      },\n      \"ingest\": {\n        \"total\": {\n          \"count\": 178,\n          \"time_in_millis\": 8,\n          \"current\": 0,\n          \"failed\": 0\n        },\n        \"pipelines\": {\n          \"flight-delay\": {\n            \"count\": 178,\n            \"time_in_millis\": 8,\n            \"current\": 0,\n            \"failed\": 0,\n            \"processors\": [\n              {\n                \"inference\": {\n                  \"type\": \"inference\",\n                  \"stats\": {\n                    \"count\": 178,\n                    \"time_in_millis\": 7,\n                    \"current\": 0,\n                    \"failed\": 0\n                  }\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  ]\n}\n----\n// NOTCONSOLE\n"
}