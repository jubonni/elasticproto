{
    "meta": {
        "size": 7939,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/semantic-reranking.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "semantic-reranking",
        "version": "8.15"
    },
    "doc": "[[semantic-reranking]]\n== Semantic re-ranking\n\npreview::[]\n\n[TIP]\n====\nThis overview focuses more on the high-level concepts and use cases for semantic re-ranking. For full implementation details on how to set up and use semantic re-ranking in {es}, see the <<text-similarity-reranker-retriever,reference documentation>> in the Search API docs.\n====\n\nRe-rankers improve the relevance of results from earlier-stage retrieval mechanisms.\n_Semantic_ re-rankers use machine learning models to reorder search results based on their semantic similarity to a query.\n\nSemantic re-ranking requires relatively large and complex machine learning models and operates in real-time in response to queries.\nThis technique makes sense on a small _top-k_ result set, as one the of the final steps in a pipeline.\nThis is a powerful technique for improving search relevance that works equally well with keyword, semantic, or hybrid retrieval algorithms.\n\nThe next sections provide more details on the benefits, use cases, and model types used for semantic re-ranking.\nThe final sections include a practical, high-level overview of how to implement <<semantic-reranking-in-es,semantic re-ranking in {es}>> and links to the full reference documentation.\n\n[discrete]\n[[semantic-reranking-use-cases]]\n=== Use cases\n\nSemantic re-ranking enables a variety of use cases:\n\n* *Lexical (BM25) retrieval results re-ranking*\n** Out-of-the-box semantic search by adding a simple API call to any lexical/BM25 retrieval pipeline.\n** Adds semantic search capabilities on top of existing indices without reindexing, perfect for quick improvements.\n** Ideal for environments with complex existing indices.\n\n* *Semantic retrieval results re-ranking*\n** Improves results from semantic retrievers using ELSER sparse vector embeddings or dense vector embeddings by using more powerful models.\n** Adds a refinement layer on top of hybrid retrieval with <<rrf, reciprocal rank fusion (RRF)>>.\n\n* *General applications*\n** Supports automatic and transparent chunking, eliminating the need for pre-chunking at index time.\n** Provides explicit control over document relevance in retrieval-augmented generation (RAG) uses cases or other scenarios involving language model (LLM) inputs.\n\nNow that we've outlined the value of semantic re-ranking, we'll explore the specific models that power this process and how they differ.\n\n[discrete]\n[[semantic-reranking-models]]\n=== Cross-encoder and bi-encoder models\n\nAt a high level, two model types are used for semantic re-ranking: cross-encoders and bi-encoders.\n\nNOTE: In this version, {es} *only supports cross-encoders* for semantic re-ranking.\n\n* A *cross-encoder model* can be thought of as a more powerful, all-in-one solution, because it generates query-aware document representations.\nIt takes the query and document texts as a single, concatenated input.\n* A *bi-encoder model* takes as input either document or query text.\nDocuments and query embeddings are computed separately, so they aren't aware of each other.\n** To compute a ranking score, an external operation is required. This typically involves computing dot-product or cosine similarity between the query and document embeddings.\n\nIn brief, cross-encoders provide high accuracy but are more resource-intensive.\nBi-encoders are faster and more cost-effective but less precise.\n\nIn future versions, {es} will also support bi-encoders.\nIf you're interested in a more detailed analysis of the practical differences between cross-encoders and bi-encoders, untoggle the next section.\n\n.Comparisons between cross-encoder and bi-encoder\n[%collapsible]\n==============\nThe following is a non-exhaustive list of considerations when choosing between cross-encoders and bi-encoders for semantic re-ranking:\n\n* Because a cross-encoder model simultaneously processes both query and document texts, it can better infer their relevance, making it more effective as a reranker than a bi-encoder.\n* Cross-encoder models are generally larger and more computationally intensive, resulting in higher latencies and increased computational costs.\n* There are significantly fewer open-source cross-encoders, while bi-encoders offer a wide variety of sizes, languages, and other trade-offs.\n* The effectiveness of cross-encoders can also improve the relevance of semantic retrievers.\nFor example, their ability to take word order into account can improve on dense or sparse embedding retrieval.\n* When trained in tandem with specific retrievers (like lexical/BM25), cross-encoders can \u201ccorrect\u201d typical errors made by those retrievers.\n* Cross-encoders output scores that are consistent across queries.\nThis enables you to maintain high relevance in result sets, by setting a minimum score threshold for all queries.\nFor example, this is important when using results in a RAG workflow or if you're otherwise feeding results to LLMs.\nNote that similarity scores from bi-encoders/embedding similarities are _query-dependent_, meaning you cannot set universal cut-offs.\n* Bi-encoders rerank using embeddings. You can improve your re-ranking latency by creating embeddings at ingest-time. These embeddings can be stored for re-ranking without being indexed for retrieval, reducing your memory footprint.\n==============\n\n[discrete]\n[[semantic-reranking-in-es]]\n=== Semantic re-ranking in {es}\n\nIn {es}, semantic re-rankers are implemented using the {es} <<inference-apis,Inference API>> and a <<retriever,retriever>>.\n\nTo use semantic re-ranking in {es}, you need to:\n\n. *Choose a re-ranking model*.\nCurrently you can:\n\n** Integrate directly with the <<infer-service-cohere,Cohere Rerank inference endpoint>> using the `rerank` task type\n** Integrate directly with the <<infer-service-google-vertex-ai,Google Vertex AI inference endpoint>> using the `rerank` task type\n** Upload a model to {es} from Hugging Face with {eland-docs}/machine-learning.html#ml-nlp-pytorch[Eland]. You'll need to use the `text_similarity` NLP task type when loading the model using Eland. Refer to {ml-docs}/ml-nlp-model-ref.html#ml-nlp-model-ref-text-similarity[the Elastic NLP model reference] for a list of third party text similarity models supported by {es} for semantic re-ranking.\n*** Then set up an <<inference-example-eland,{es} service inference endpoint>> with the `rerank` task type\n. *Create a `rerank` task using the <<put-inference-api,{es} Inference API>>*.\nThe Inference API creates an inference endpoint and configures your chosen machine learning model to perform the re-ranking task.\n. *Define a `text_similarity_reranker` retriever in your search request*.\nThe retriever syntax makes it simple to configure both the retrieval and re-ranking of search results in a single API call.\n\n.*Example search request* with semantic reranker\n[%collapsible]\n==============\nThe following example shows a search request that uses a semantic reranker to reorder the top-k documents based on their semantic similarity to the query.\n[source,console]\n----\nPOST _search\n{\n  \"retriever\": {\n    \"text_similarity_reranker\": {\n      \"retriever\": {\n        \"standard\": {\n          \"query\": {\n            \"match\": {\n              \"text\": \"How often does the moon hide the sun?\"\n            }\n          }\n        }\n      },\n      \"field\": \"text\",\n      \"inference_id\": \"my-cohere-rerank-model\",\n      \"inference_text\": \"How often does the moon hide the sun?\",\n      \"rank_window_size\": 100,\n      \"min_score\": 0.5\n    }\n  }\n}\n----\n// TEST[skip:TBD]\n==============\n\n[discrete]\n[[semantic-reranking-learn-more]]\n=== Learn more\n\n* Read the <<retriever,retriever reference documentation>> for syntax and implementation details\n* Learn more about the <<retrievers-overview,retrievers>> abstraction\n* Learn more about the Elastic <<inference-apis,Inference APIs>>\n* Check out our https://github.com/elastic/elasticsearch-labs/blob/main/notebooks/integrations/cohere/cohere-elasticsearch.ipynb[Python notebook] for using Cohere with {es}"
}