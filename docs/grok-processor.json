{
    "meta": {
        "timestamp": "2024-11-01T02:49:25.306075",
        "size": 11241,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/grok-processor.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "grok-processor",
        "version": "8.15"
    },
    "doc": "[[grok-processor]]\n=== Grok processor\n++++\n<titleabbrev>Grok</titleabbrev>\n++++\n\nExtracts structured fields out of a single text field within a document. You choose which field to\nextract matched fields from, as well as the grok pattern you expect will match. A grok pattern is like a regular\nexpression that supports aliased expressions that can be reused.\n\nThis processor comes packaged with many\nhttps://github.com/elastic/elasticsearch/blob/{branch}/libs/grok/src/main/resources/patterns[reusable patterns].\n\nIf you need help building patterns to match your logs, you will find the\n{kibana-ref}/xpack-grokdebugger.html[Grok Debugger] tool quite useful!\nThe https://grokconstructor.appspot.com[Grok Constructor] is also a useful tool.\n\n[[using-grok]]\n==== Using the Grok Processor in a Pipeline\n\n[[grok-options]]\n.Grok Options\n[options=\"header\"]\n|======\n| Name                   | Required  | Default             | Description\n| `field`                | yes       | -                   | The field to use for grok expression parsing\n| `patterns`             | yes       | -                   | An ordered list of grok expression to match and extract named captures with. Returns on the first expression in the list that matches.\n| `pattern_definitions`  | no        | -                   | A map of pattern-name and pattern tuples defining custom patterns to be used by the current processor. Patterns matching existing names will override the pre-existing definition.\n| `ecs_compatibility`    | no        | `disabled`          | Must be `disabled` or `v1`. If `v1`, the processor uses patterns with {ecs-ref}/ecs-field-reference.html[Elastic Common Schema (ECS)] field names.\n| `trace_match`          | no        | false               | when true, `_ingest._grok_match_index` will be inserted into your matched document's metadata with the index into the pattern found in `patterns` that matched.\n| `ignore_missing`       | no        | false               | If `true` and `field` does not exist or is `null`, the processor quietly exits without modifying the document\ninclude::common-options.asciidoc[]\n|======\n\nHere is an example of using the provided patterns to extract out and name structured fields from a string field in\na document.\n\n[source,console]\n--------------------------------------------------\nPOST _ingest/pipeline/_simulate\n{\n  \"pipeline\": {\n    \"description\" : \"...\",\n    \"processors\": [\n      {\n        \"grok\": {\n          \"field\": \"message\",\n          \"patterns\": [\"%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes:int} %{NUMBER:duration:double}\"]\n        }\n      }\n    ]\n  },\n  \"docs\":[\n    {\n      \"_source\": {\n        \"message\": \"55.3.244.1 GET /index.html 15824 0.043\"\n      }\n    }\n  ]\n}\n--------------------------------------------------\n\nThis pipeline will insert these named captures as new fields within the document, like so:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"docs\": [\n    {\n      \"doc\": {\n        \"_index\": \"_index\",\n        \"_id\": \"_id\",\n        \"_version\": \"-3\",\n        \"_source\" : {\n          \"duration\" : 0.043,\n          \"request\" : \"/index.html\",\n          \"method\" : \"GET\",\n          \"bytes\" : 15824,\n          \"client\" : \"55.3.244.1\",\n          \"message\" : \"55.3.244.1 GET /index.html 15824 0.043\"\n        },\n        \"_ingest\": {\n          \"timestamp\": \"2016-11-08T19:43:03.850+0000\"\n        }\n      }\n    }\n  ]\n}\n--------------------------------------------------\n// TESTRESPONSE[s/2016-11-08T19:43:03.850\\+0000/$body.docs.0.doc._ingest.timestamp/]\n\n[[custom-patterns]]\n==== Custom Patterns\n\nThe Grok processor comes pre-packaged with a base set of patterns. These patterns may not always have\nwhat you are looking for. Patterns have a very basic format. Each entry has a name and the pattern itself.\n\nYou can add your own patterns to a processor definition under the `pattern_definitions` option.\nHere is an example of a pipeline specifying custom pattern definitions:\n\n[source,js]\n--------------------------------------------------\n{\n  \"description\" : \"...\",\n  \"processors\": [\n    {\n      \"grok\": {\n        \"field\": \"message\",\n        \"patterns\": [\"my %{FAVORITE_DOG:dog} is colored %{RGB:color}\"],\n        \"pattern_definitions\" : {\n          \"FAVORITE_DOG\" : \"beagle\",\n          \"RGB\" : \"RED|GREEN|BLUE\"\n        }\n      }\n    }\n  ]\n}\n--------------------------------------------------\n// NOTCONSOLE\n\n[[trace-match]]\n==== Providing Multiple Match Patterns\n\nSometimes one pattern is not enough to capture the potential structure of a field. Let's assume we\nwant to match all messages that contain your favorite pet breeds of either cats or dogs. One way to accomplish\nthis is to provide two distinct patterns that can be matched, instead of one really complicated expression capturing\nthe same `or` behavior.\n\nHere is an example of such a configuration executed against the simulate API:\n\n[source,console]\n--------------------------------------------------\nPOST _ingest/pipeline/_simulate\n{\n  \"pipeline\": {\n  \"description\" : \"parse multiple patterns\",\n  \"processors\": [\n    {\n      \"grok\": {\n        \"field\": \"message\",\n        \"patterns\": [\"%{FAVORITE_DOG:pet}\", \"%{FAVORITE_CAT:pet}\"],\n        \"pattern_definitions\" : {\n          \"FAVORITE_DOG\" : \"beagle\",\n          \"FAVORITE_CAT\" : \"burmese\"\n        }\n      }\n    }\n  ]\n},\n\"docs\":[\n  {\n    \"_source\": {\n      \"message\": \"I love burmese cats!\"\n    }\n  }\n  ]\n}\n--------------------------------------------------\n\nresponse:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"docs\": [\n    {\n      \"doc\": {\n        \"_index\": \"_index\",\n        \"_id\": \"_id\",\n        \"_version\": \"-3\",\n        \"_source\": {\n          \"message\": \"I love burmese cats!\",\n          \"pet\": \"burmese\"\n        },\n        \"_ingest\": {\n          \"timestamp\": \"2016-11-08T19:43:03.850+0000\"\n        }\n      }\n    }\n  ]\n}\n--------------------------------------------------\n// TESTRESPONSE[s/2016-11-08T19:43:03.850\\+0000/$body.docs.0.doc._ingest.timestamp/]\n\nBoth patterns will set the field `pet` with the appropriate match, but what if we want to trace which of our\npatterns matched and populated our fields? We can do this with the `trace_match` parameter. Here is the output of\nthat same pipeline, but with `\"trace_match\": true` configured:\n\n////\nHidden setup for example:\n[source,console]\n--------------------------------------------------\nPOST _ingest/pipeline/_simulate\n{\n  \"pipeline\": {\n  \"description\" : \"parse multiple patterns\",\n  \"processors\": [\n    {\n      \"grok\": {\n        \"field\": \"message\",\n        \"patterns\": [\"%{FAVORITE_DOG:pet}\", \"%{FAVORITE_CAT:pet}\"],\n        \"trace_match\": true,\n        \"pattern_definitions\" : {\n          \"FAVORITE_DOG\" : \"beagle\",\n          \"FAVORITE_CAT\" : \"burmese\"\n        }\n      }\n    }\n  ]\n},\n\"docs\":[\n  {\n    \"_source\": {\n      \"message\": \"I love burmese cats!\"\n    }\n  }\n  ]\n}\n--------------------------------------------------\n////\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"docs\": [\n    {\n      \"doc\": {\n        \"_index\": \"_index\",\n        \"_id\": \"_id\",\n        \"_version\": \"-3\",\n        \"_source\": {\n          \"message\": \"I love burmese cats!\",\n          \"pet\": \"burmese\"\n        },\n        \"_ingest\": {\n          \"_grok_match_index\": \"1\",\n          \"timestamp\": \"2016-11-08T19:43:03.850+0000\"\n        }\n      }\n    }\n  ]\n}\n--------------------------------------------------\n// TESTRESPONSE[s/2016-11-08T19:43:03.850\\+0000/$body.docs.0.doc._ingest.timestamp/]\n\nIn the above response, you can see that the index of the pattern that matched was `\"1\"`. This is to say that it was the\nsecond (index starts at zero) pattern in `patterns` to match.\n\nThis trace metadata enables debugging which of the patterns matched. This information is stored in the ingest\nmetadata and will not be indexed.\n\n[[grok-processor-rest-get]]\n==== Retrieving patterns from REST endpoint\n\nThe Grok processor comes packaged with its own REST endpoint for retrieving the patterns included with the processor.\n\n[source,console]\n--------------------------------------------------\nGET _ingest/processor/grok\n--------------------------------------------------\n\nThe above request will return a response body containing a key-value representation of the built-in patterns dictionary.\n\n[source,js]\n--------------------------------------------------\n{\n  \"patterns\" : {\n    \"BACULA_CAPACITY\" : \"%{INT}{1,3}(,%{INT}{3})*\",\n    \"PATH\" : \"(?:%{UNIXPATH}|%{WINPATH})\",\n    ...\n}\n--------------------------------------------------\n// NOTCONSOLE\n\nBy default, the API returns a list of legacy Grok patterns. These legacy\npatterns predate the {ecs-ref}/ecs-field-reference.html[Elastic Common Schema\n(ECS)] and don't use ECS field names. To return patterns that extract ECS field\nnames, specify `v1` in the optional `ecs_compatibility` query parameter.\n\n[source,console]\n----\nGET _ingest/processor/grok?ecs_compatibility=v1\n----\n\nBy default, the API returns patterns in the order they are read from disk. This\nsort order preserves groupings of related patterns. For example, all patterns\nrelated to parsing Linux syslog lines stay grouped together.\n\nYou can use the optional boolean `s` query parameter to sort returned patterns\nby key name instead.\n\n[source,console]\n--------------------------------------------------\nGET _ingest/processor/grok?s\n--------------------------------------------------\n\nThe API returns the following response.\n\n[source,js]\n--------------------------------------------------\n{\n  \"patterns\" : {\n    \"BACULA_CAPACITY\" : \"%{INT}{1,3}(,%{INT}{3})*\",\n    \"BACULA_DEVICE\" : \"%{USER}\",\n    \"BACULA_DEVICEPATH\" : \"%{UNIXPATH}\",\n    ...\n}\n--------------------------------------------------\n// NOTCONSOLE\n\nThis can be useful to reference as the built-in patterns change across versions.\n\n[[grok-watchdog]]\n==== Grok watchdog\n\nGrok expressions that take too long to execute are interrupted and\nthe grok processor then fails with an exception. The grok\nprocessor has a watchdog thread that determines when evaluation of\na grok expression takes too long and is controlled by the following\nsettings:\n\n[[grok-watchdog-options]]\n.Grok watchdog settings\n[options=\"header\"]\n|======\n| Name                                      | Default | Description\n| `ingest.grok.watchdog.interval`           | 1s      | How often to check whether there are grok evaluations that take longer than the maximum allowed execution time.\n| `ingest.grok.watchdog.max_execution_time` | 1s      | The maximum allowed execution of a grok expression evaluation.\n|======\n\n[[grok-debugging]]\n==== Grok debugging\n\nIt is advised to use the {kibana-ref}/xpack-grokdebugger.html[Grok Debugger] to debug grok patterns. From there you can test one or more\npatterns in the UI against sample data. Under the covers it uses the same engine as ingest node processor.\n\nAdditionally, it is recommended to enable debug logging for Grok so that any additional messages may also be seen in the Elasticsearch\nserver log.\n\n[source,js]\n--------------------------------------------------\nPUT _cluster/settings\n{\n  \"persistent\": {\n    \"logger.org.elasticsearch.ingest.common.GrokProcessor\": \"debug\"\n  }\n}\n--------------------------------------------------\n// NOTCONSOLE\n"
}