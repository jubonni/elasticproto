{
    "meta": {
        "timestamp": "2024-11-01T02:49:24.233803",
        "size": 9159,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/add-elasticsearch-nodes.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "add-elasticsearch-nodes",
        "version": "8.15"
    },
    "doc": "[[add-elasticsearch-nodes]]\n== Add and remove nodes in your cluster\n\nWhen you start an instance of {es}, you are starting a _node_. An {es} _cluster_\nis a group of nodes that have the same `cluster.name` attribute. As nodes join\nor leave a cluster, the cluster automatically reorganizes itself to evenly\ndistribute the data across the available nodes.\n\nIf you are running a single instance of {es}, you have a cluster of one node.\nAll primary shards reside on the single node. No replica shards can be\nallocated, therefore the cluster state remains yellow. The cluster is fully\nfunctional but is at risk of data loss in the event of a failure.\n\nimage::setup/images/elas_0202.png[\"A cluster with one node and three primary shards\"]\n\nYou add nodes to a cluster to increase its capacity and reliability. By default,\na node is both a data node and eligible to be elected as the master node that\ncontrols the cluster. You can also configure a new node for a specific purpose,\nsuch as handling ingest requests. For more information, see\n<<modules-node,Nodes>>.\n\nWhen you add more nodes to a cluster, it automatically allocates replica shards.\nWhen all primary and replica shards are active, the cluster state changes to\ngreen.\n\nimage::setup/images/elas_0204.png[\"A cluster with three nodes\"]\n\n[discrete]\n=== Enroll nodes in an existing cluster\nYou can enroll additional nodes on your local machine to experiment with how an\n{es} cluster with multiple nodes behaves. \n\n[NOTE]\n====\nTo add a node to a cluster running on multiple machines, you must also set\n<<unicast.hosts,`discovery.seed_hosts`>> so that the new node can discover\nthe rest of its cluster.\n====\n\ninclude::{es-ref-dir}/security/enroll-nodes.asciidoc[]\n\nFor more information about discovery and shard allocation, refer to\n<<modules-discovery>> and <<modules-cluster>>.\n\n[discrete]\n[[add-elasticsearch-nodes-master-eligible]]\n=== Master-eligible nodes\n\nAs nodes are added or removed Elasticsearch maintains an optimal level of fault\ntolerance by automatically updating the cluster's _voting configuration_, which\nis the set of <<master-node,master-eligible nodes>> whose responses are counted\nwhen making decisions such as electing a new master or committing a new cluster\nstate.\n\nIt is recommended to have a small and fixed number of master-eligible nodes in a\ncluster, and to scale the cluster up and down by adding and removing\nmaster-ineligible nodes only. However there are situations in which it may be\ndesirable to add or remove some master-eligible nodes to or from a cluster.\n\n[discrete]\n[[modules-discovery-adding-nodes]]\n==== Adding master-eligible nodes\n\nIf you wish to add some nodes to your cluster, simply configure the new nodes\nto find the existing cluster and start them up. Elasticsearch adds the new nodes\nto the voting configuration if it is appropriate to do so.\n\nDuring master election or when joining an existing formed cluster, a node\nsends a join request to the master in order to be officially added to the\ncluster.\n\n[discrete]\n[[modules-discovery-removing-nodes]]\n==== Removing master-eligible nodes\n\nWhen removing master-eligible nodes, it is important not to remove too many all\nat the same time. For instance, if there are currently seven master-eligible\nnodes and you wish to reduce this to three, it is not possible simply to stop\nfour of the nodes at once: to do so would leave only three nodes remaining,\nwhich is less than half of the voting configuration, which means the cluster\ncannot take any further actions.\n\nMore precisely, if you shut down half or more of the master-eligible nodes all\nat the same time then the cluster will normally become unavailable. If this\nhappens then you can bring the cluster back online by starting the removed\nnodes again.\n\nAs long as there are at least three master-eligible nodes in the cluster, as a\ngeneral rule it is best to remove nodes one-at-a-time, allowing enough time for\nthe cluster to <<modules-discovery-quorums,automatically adjust>> the voting\nconfiguration and adapt the fault tolerance level to the new set of nodes.\n\nIf there are only two master-eligible nodes remaining then neither node can be\nsafely removed since both are required to reliably make progress. To remove one\nof these nodes you must first inform {es} that it should not be part of the\nvoting configuration, and that the voting power should instead be given to the\nother node. You can then take the excluded node offline without preventing the\nother node from making progress. A node which is added to a voting\nconfiguration exclusion list still works normally, but {es} tries to remove it\nfrom the voting configuration so its vote is no longer required. Importantly,\n{es} will never automatically move a node on the voting exclusions list back\ninto the voting configuration. Once an excluded node has been successfully\nauto-reconfigured out of the voting configuration, it is safe to shut it down\nwithout affecting the cluster's master-level availability. A node can be added\nto the voting configuration exclusion list using the\n<<voting-config-exclusions>> API. For example:\n\n[source,console]\n--------------------------------------------------\n# Add node to voting configuration exclusions list and wait for the system\n# to auto-reconfigure the node out of the voting configuration up to the\n# default timeout of 30 seconds\nPOST /_cluster/voting_config_exclusions?node_names=node_name\n\n# Add node to voting configuration exclusions list and wait for\n# auto-reconfiguration up to one minute\nPOST /_cluster/voting_config_exclusions?node_names=node_name&timeout=1m\n--------------------------------------------------\n// TEST[skip:this would break the test cluster if executed]\n\nThe nodes that should be added to the exclusions list are specified by name\nusing the `?node_names` query parameter, or by their persistent node IDs using\nthe `?node_ids` query parameter. If a call to the voting configuration\nexclusions API fails, you can safely retry it. Only a successful response\nguarantees that the node has actually been removed from the voting configuration\nand will not be reinstated. If the elected master node is excluded from the\nvoting configuration then it will abdicate to another master-eligible node that\nis still in the voting configuration if such a node is available.\n\nAlthough the voting configuration exclusions API is most useful for down-scaling\na two-node to a one-node cluster, it is also possible to use it to remove\nmultiple master-eligible nodes all at the same time. Adding multiple nodes to\nthe exclusions list has the system try to auto-reconfigure all of these nodes\nout of the voting configuration, allowing them to be safely shut down while\nkeeping the cluster available. In the example described above, shrinking a\nseven-master-node cluster down to only have three master nodes, you could add\nfour nodes to the exclusions list, wait for confirmation, and then shut them\ndown simultaneously.\n\nNOTE: Voting exclusions are only required when removing at least half of the\nmaster-eligible nodes from a cluster in a short time period. They are not\nrequired when removing master-ineligible nodes, nor are they required when\nremoving fewer than half of the master-eligible nodes.\n\nAdding an exclusion for a node creates an entry for that node in the voting\nconfiguration exclusions list, which has the system automatically try to\nreconfigure the voting configuration to remove that node and prevents it from\nreturning to the voting configuration once it has removed. The current list of\nexclusions is stored in the cluster state and can be inspected as follows:\n\n[source,console]\n--------------------------------------------------\nGET /_cluster/state?filter_path=metadata.cluster_coordination.voting_config_exclusions\n--------------------------------------------------\n\nThis list is limited in size by the `cluster.max_voting_config_exclusions` \nsetting, which defaults to `10`. See <<modules-discovery-settings>>. Since\nvoting configuration exclusions are persistent and limited in number, they must\nbe cleaned up. Normally an exclusion is added when performing some maintenance\non the cluster, and the exclusions should be cleaned up when the maintenance is\ncomplete. Clusters should have no voting configuration exclusions in normal\noperation.\n\nIf a node is excluded from the voting configuration because it is to be shut\ndown permanently, its exclusion can be removed after it is shut down and removed\nfrom the cluster. Exclusions can also be cleared if they were created in error\nor were only required temporarily by specifying `?wait_for_removal=false`.\n\n[source,console]\n--------------------------------------------------\n# Wait for all the nodes with voting configuration exclusions to be removed from\n# the cluster and then remove all the exclusions, allowing any node to return to\n# the voting configuration in the future.\nDELETE /_cluster/voting_config_exclusions\n\n# Immediately remove all the voting configuration exclusions, allowing any node\n# to return to the voting configuration in the future.\nDELETE /_cluster/voting_config_exclusions?wait_for_removal=false\n--------------------------------------------------\n"
}