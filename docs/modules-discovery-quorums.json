{
    "meta": {
        "size": 4334,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-quorums.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "modules-discovery-quorums",
        "version": "8.15"
    },
    "doc": "[[modules-discovery-quorums]]\n=== Quorum-based decision making\n\nElecting a master node and changing the cluster state are the two fundamental\ntasks that master-eligible nodes must work together to perform. It is important\nthat these activities work robustly even if some nodes have failed.\nElasticsearch achieves this robustness by considering each action to have\nsucceeded on receipt of responses from a _quorum_, which is a subset of the\nmaster-eligible nodes in the cluster. The advantage of requiring only a subset\nof the nodes to respond is that it means some of the nodes can fail without\npreventing the cluster from making progress. The quorums are carefully chosen so\nthe cluster does not have a \"split brain\" scenario where it's partitioned into\ntwo pieces such that each piece may make decisions that are inconsistent with\nthose of the other piece.\n\nElasticsearch allows you to add and remove master-eligible nodes to a running\ncluster. In many cases you can do this simply by starting or stopping the nodes\nas required. See <<add-elasticsearch-nodes>> for more information.\n\nAs nodes are added or removed Elasticsearch maintains an optimal level of fault\ntolerance by updating the cluster's <<modules-discovery-voting,voting\nconfiguration>>, which is the set of master-eligible nodes whose responses are\ncounted when making decisions such as electing a new master or committing a new\ncluster state. A decision is made only after more than half of the nodes in the\nvoting configuration have responded. Usually the voting configuration is the\nsame as the set of all the master-eligible nodes that are currently in the\ncluster. However, there are some situations in which they may be different.\n\n// tag::quorums-and-availability[]\n[IMPORTANT]\n====\nTo be sure that the cluster remains available you **must not stop half or more\nof the nodes in the voting configuration at the same time**. As long as more\nthan half of the voting nodes are available the cluster can still work normally.\nThis means that if there are three or four master-eligible nodes, the cluster\ncan tolerate one of them being unavailable. If there are two or fewer\nmaster-eligible nodes, they must all remain available.\n\nIf you stop half or more of the nodes in the voting configuration at the same\ntime then the cluster will be unavailable until you bring enough nodes back\nonline to form a quorum again. While the cluster is unavailable, any remaining\nnodes will report in their logs that they cannot discover or elect a master\nnode. See <<discovery-troubleshooting>> for more information.\n====\n// end::quorums-and-availability[]\n\nAfter a master-eligible node has joined or left the cluster the elected master\nmay issue a cluster-state update that adjusts the voting configuration to match,\nand this can take a short time to complete. It is important to wait for this\nadjustment to complete before removing more nodes from the cluster. See\n<<modules-discovery-removing-nodes>> for more information.\n\n[discrete]\n==== Master elections\n\nElasticsearch uses an election process to agree on an elected master node, both\nat startup and if the existing elected master fails. Any master-eligible node\ncan start an election, and normally the first election that takes place will\nsucceed. Elections only usually fail when two nodes both happen to start their\nelections at about the same time, so elections are scheduled randomly on each\nnode to reduce the probability of this happening. Nodes will retry elections\nuntil a master is elected, backing off on failure, so that eventually an\nelection will succeed (with arbitrarily high probability). The scheduling of\nmaster elections are controlled by the <<master-election-settings,master\nelection settings>>.\n\n[discrete]\n==== Cluster maintenance, rolling restarts and migrations\n\nMany cluster maintenance tasks involve temporarily shutting down one or more\nnodes and then starting them back up again. By default {es} can remain\navailable if one of its master-eligible nodes is taken offline, such as during a\nrolling upgrade. Furthermore, if multiple nodes are stopped\nand then started again then it will automatically recover, such as during a\nfull cluster restart. There is no need to take any further\naction with the APIs described here in these cases, because the set of master\nnodes is not changing permanently.\n\n"
}