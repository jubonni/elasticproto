{
    "meta": {
        "size": 18925,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-search-speed.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "tune-for-search-speed",
        "version": "8.15"
    },
    "doc": "[[tune-for-search-speed]]\n== Tune for search speed\n\n[discrete]\n=== Give memory to the filesystem cache\n\nElasticsearch heavily relies on the filesystem cache in order to make search\nfast. In general, you should make sure that at least half the available memory\ngoes to the filesystem cache so that Elasticsearch can keep hot regions of the\nindex in physical memory.\n\n[discrete]\n// tag::readahead[]\n=== Avoid page cache thrashing by using modest readahead values on Linux\n\nSearch can cause a lot of randomized read I/O. When the underlying block\ndevice has a high readahead value, there may be a lot of unnecessary\nread I/O done, especially when files are accessed using memory mapping\n(see <<file-system,storage types>>).\n\nMost Linux distributions use a sensible readahead value of `128KiB` for a\nsingle plain device, however, when using software raid, LVM or dm-crypt the\nresulting block device (backing Elasticsearch <<path-settings,path.data>>)\nmay end up having a very large readahead value (in the range of several MiB).\nThis usually results in severe page (filesystem) cache thrashing adversely\naffecting search (or <<docs,update>>) performance.\n\nYou can check the current value in `KiB` using\n`lsblk -o NAME,RA,MOUNTPOINT,TYPE,SIZE`.\nConsult the documentation of your distribution on how to alter this value\n(for example with a `udev` rule to persist across reboots, or via\nhttps://man7.org/linux/man-pages/man8/blockdev.8.html[blockdev --setra]\nas a transient setting). We recommend a value of `128KiB` for readahead.\n\nWARNING: `blockdev` expects values in 512 byte sectors whereas `lsblk` reports\nvalues in `KiB`. As an example, to temporarily set readahead to `128KiB`\nfor `/dev/nvme0n1`, specify `blockdev --setra 256 /dev/nvme0n1`.\n// end::readahead[]\n\n[discrete]\n[[search-use-faster-hardware]]\n=== Use faster hardware\n\nIf your searches are I/O-bound, consider increasing the size of the filesystem\ncache (see above) or using faster storage. Each search involves a mix of\nsequential and random reads across multiple files, and there may be many\nsearches running concurrently on each shard, so SSD drives tend to perform\nbetter than spinning disks.\n\nIf your searches are CPU-bound, consider using a larger number of faster CPUs.\n\n[discrete]\n==== Local vs. remote storage\n\ninclude::./remote-storage.asciidoc[]\n\n[discrete]\n=== Document modeling\n\nDocuments should be modeled so that search-time operations are as cheap as possible.\n\nIn particular, joins should be avoided. <<nested,`nested`>> can make queries\nseveral times slower and <<parent-join,parent-child>> relations can make\nqueries hundreds of times slower. So if the same questions can be answered without\njoins by denormalizing documents, significant speedups can be expected.\n\n[discrete]\n[[search-as-few-fields-as-possible]]\n=== Search as few fields as possible\n\nThe more fields a <<query-dsl-query-string-query,`query_string`>> or\n<<query-dsl-multi-match-query,`multi_match`>> query targets, the slower it is.\nA common technique to improve search speed over multiple fields is to copy\ntheir values into a single field at index time, and then use this field at\nsearch time. This can be automated with the <<copy-to,`copy-to`>> directive of\nmappings without having to change the source of documents. Here is an example\nof an index containing movies that optimizes queries that search over both the\nname and the plot of the movie by indexing both values into the `name_and_plot`\nfield.\n\n[source,console]\n--------------------------------------------------\nPUT movies\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name_and_plot\": {\n        \"type\": \"text\"\n      },\n      \"name\": {\n        \"type\": \"text\",\n        \"copy_to\": \"name_and_plot\"\n      },\n      \"plot\": {\n        \"type\": \"text\",\n        \"copy_to\": \"name_and_plot\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n[discrete]\n=== Pre-index data\n\nYou should leverage patterns in your queries to optimize the way data is indexed.\nFor instance, if all your documents have a `price` field and most queries run\n<<search-aggregations-bucket-range-aggregation,`range`>> aggregations on a fixed\nlist of ranges, you could make this aggregation faster by pre-indexing the ranges\ninto the index and using a <<search-aggregations-bucket-terms-aggregation,`terms`>>\naggregations.\n\nFor instance, if documents look like:\n\n[source,console]\n--------------------------------------------------\nPUT index/_doc/1\n{\n  \"designation\": \"spoon\",\n  \"price\": 13\n}\n--------------------------------------------------\n\nand search requests look like:\n\n[source,console]\n--------------------------------------------------\nGET index/_search\n{\n  \"aggs\": {\n    \"price_ranges\": {\n      \"range\": {\n        \"field\": \"price\",\n        \"ranges\": [\n          { \"to\": 10 },\n          { \"from\": 10, \"to\": 100 },\n          { \"from\": 100 }\n        ]\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[continued]\n\nThen documents could be enriched by a `price_range` field at index time, which\nshould be mapped as a <<keyword,`keyword`>>:\n\n[source,console]\n--------------------------------------------------\nPUT index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"price_range\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\n\nPUT index/_doc/1\n{\n  \"designation\": \"spoon\",\n  \"price\": 13,\n  \"price_range\": \"10-100\"\n}\n--------------------------------------------------\n\nAnd then search requests could aggregate this new field rather than running a\n`range` aggregation on the `price` field.\n\n[source,console]\n--------------------------------------------------\nGET index/_search\n{\n  \"aggs\": {\n    \"price_ranges\": {\n      \"terms\": {\n        \"field\": \"price_range\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[continued]\n\n[discrete]\n[[map-ids-as-keyword]]\n=== Consider mapping identifiers as `keyword`\n\ninclude::../mapping/types/numeric.asciidoc[tag=map-ids-as-keyword]\n\n[discrete]\n=== Avoid scripts\n\nIf possible, avoid using <<modules-scripting,script>>-based sorting, scripts in\naggregations, and the <<query-dsl-script-score-query,`script_score`>> query. See\n<<scripts-and-search-speed>>.\n\n\n[discrete]\n=== Search rounded dates\n\nQueries on date fields that use `now` are typically not cacheable since the\nrange that is being matched changes all the time. However switching to a\nrounded date is often acceptable in terms of user experience, and has the\nbenefit of making better use of the query cache.\n\nFor instance the below query:\n\n[source,console]\n--------------------------------------------------\nPUT index/_doc/1\n{\n  \"my_date\": \"2016-05-11T16:30:55.328Z\"\n}\n\nGET index/_search\n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"range\": {\n          \"my_date\": {\n            \"gte\": \"now-1h\",\n            \"lte\": \"now\"\n          }\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\ncould be replaced with the following query:\n\n[source,console]\n--------------------------------------------------\nGET index/_search\n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"range\": {\n          \"my_date\": {\n            \"gte\": \"now-1h/m\",\n            \"lte\": \"now/m\"\n          }\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[continued]\n\nIn that case we rounded to the minute, so if the current time is `16:31:29`,\nthe range query will match everything whose value of the `my_date` field is\nbetween `15:31:00` and `16:31:59`. And if several users run a query that\ncontains this range in the same minute, the query cache could help speed things\nup a bit. The longer the interval that is used for rounding, the more the query\ncache can help, but beware that too aggressive rounding might also hurt user\nexperience.\n\n\nNOTE: It might be tempting to split ranges into a large cacheable part and\nsmaller not cacheable parts in order to be able to leverage the query cache,\nas shown below:\n\n[source,console]\n--------------------------------------------------\nGET index/_search\n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"bool\": {\n          \"should\": [\n            {\n              \"range\": {\n                \"my_date\": {\n                  \"gte\": \"now-1h\",\n                  \"lte\": \"now-1h/m\"\n                }\n              }\n            },\n            {\n              \"range\": {\n                \"my_date\": {\n                  \"gt\": \"now-1h/m\",\n                  \"lt\": \"now/m\"\n                }\n              }\n            },\n            {\n              \"range\": {\n                \"my_date\": {\n                  \"gte\": \"now/m\",\n                  \"lte\": \"now\"\n                }\n              }\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[continued]\n\nHowever such practice might make the query run slower in some cases since the\noverhead introduced by the `bool` query may defeat the savings from better\nleveraging the query cache.\n\n[discrete]\n=== Force-merge read-only indices\n\nIndices that are read-only may benefit from being <<indices-forcemerge,merged\ndown to a single segment>>. This is typically the case with time-based indices:\nonly the index for the current time frame is getting new documents while older\nindices are read-only. Shards that have been force-merged into a single segment\ncan use simpler and more efficient data structures to perform searches.\n\nIMPORTANT: Do not force-merge indices to which you are still writing, or to\nwhich you will write again in the future. Instead, rely on the automatic\nbackground merge process to perform merges as needed to keep the index running\nsmoothly. If you continue to write to a force-merged index then its performance\nmay become much worse.\n\n[discrete]\n=== Warm up global ordinals\n\n<<eager-global-ordinals,Global ordinals>> are a data structure that is used to\noptimize the performance of aggregations. They are calculated lazily and stored in\nthe JVM heap as part of the <<modules-fielddata, field data cache>>. For fields\nthat are heavily used for bucketing aggregations, you can tell {es} to construct\nand cache the global ordinals before requests are received. This should be done\ncarefully because it will increase heap usage and can make <<indices-refresh, refreshes>>\ntake longer. The option can be updated dynamically on an existing mapping by\nsetting the <<eager-global-ordinals, eager global ordinals>> mapping parameter:\n\n[source,console]\n--------------------------------------------------\nPUT index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"foo\": {\n        \"type\": \"keyword\",\n        \"eager_global_ordinals\": true\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n// tag::warm-fs-cache[]\n[discrete]\n=== Warm up the filesystem cache\n\nIf the machine running Elasticsearch is restarted, the filesystem cache will be\nempty, so it will take some time before the operating system loads hot regions\nof the index into memory so that search operations are fast. You can explicitly\ntell the operating system which files should be loaded into memory eagerly\ndepending on the file extension using the\n<<preload-data-to-file-system-cache,`index.store.preload`>> setting.\n\nWARNING: Loading data into the filesystem cache eagerly on too many indices or\ntoo many files will make search _slower_ if the filesystem cache is not large\nenough to hold all the data. Use with caution.\n// end::warm-fs-cache[]\n\n[discrete]\n=== Use index sorting to speed up conjunctions\n\n<<index-modules-index-sorting,Index sorting>> can be useful in order to make\nconjunctions faster at the cost of slightly slower indexing. Read more about it\nin the <<index-modules-index-sorting-conjunctions,index sorting documentation>>.\n\n[discrete]\n[[preference-cache-optimization]]\n=== Use `preference` to optimize cache utilization\n\nThere are multiple caches that can help with search performance, such as the\n{wikipedia}/Page_cache[filesystem cache], the\n<<shard-request-cache,request cache>> or the <<query-cache,query cache>>. Yet\nall these caches are maintained at the node level, meaning that if you run the\nsame request twice in a row, have 1 replica or more\nand use {wikipedia}/Round-robin_DNS[round-robin], the default\nrouting algorithm, then those two requests will go to different shard copies,\npreventing node-level caches from helping.\n\nSince it is common for users of a search application to run similar requests\none after another, for instance in order to analyze a narrower subset of the\nindex, using a preference value that identifies the current user or session\ncould help optimize usage of the caches.\n\n[discrete]\n=== Replicas might help with throughput, but not always\n\nIn addition to improving resiliency, replicas can help improve throughput. For\ninstance if you have a single-shard index and three nodes, you will need to\nset the number of replicas to 2 in order to have 3 copies of your shard in\ntotal so that all nodes are utilized.\n\nNow imagine that you have a 2-shards index and two nodes. In one case, the\nnumber of replicas is 0, meaning that each node holds a single shard. In the\nsecond case the number of replicas is 1, meaning that each node has two shards.\nWhich setup is going to perform best in terms of search performance? Usually,\nthe setup that has fewer shards per node in total will perform better. The\nreason for that is that it gives a greater share of the available filesystem\ncache to each shard, and the filesystem cache is probably Elasticsearch's\nnumber 1 performance factor. At the same time, beware that a setup that does\nnot have replicas is subject to failure in case of a single node failure, so\nthere is a trade-off between throughput and availability.\n\nSo what is the right number of replicas? If you have a cluster that has\n`num_nodes` nodes, `num_primaries` primary shards _in total_ and if you want to\nbe able to cope with `max_failures` node failures at once at most, then the\nright number of replicas for you is\n`max(max_failures, ceil(num_nodes / num_primaries) - 1)`.\n\n[discrete]\n=== Tune your queries with the Search Profiler\n\nThe {ref}/search-profile.html[Profile API] provides detailed information about\nhow each component of your queries and aggregations impacts the time it takes\nto process the request.\n\nThe {kibana-ref}/xpack-profiler.html[Search Profiler] in {kib}\nmakes it easy to navigate and analyze the profile results and\ngive you insight into how to tune your queries to improve performance and reduce load.\n\nBecause the Profile API itself adds significant overhead to the query,\nthis information is best used to understand the relative cost of the various\nquery components. It does not provide a reliable measure of actual processing time.\n\n[discrete]\n[[faster-phrase-queries]]\n=== Faster phrase queries with `index_phrases`\n\nThe <<text,`text`>> field has an <<index-phrases,`index_phrases`>> option that\nindexes 2-shingles and is automatically leveraged by query parsers to run phrase\nqueries that don't have a slop. If your use-case involves running lots of phrase\nqueries, this can speed up queries significantly.\n\n[discrete]\n[[faster-prefix-queries]]\n=== Faster prefix queries with `index_prefixes`\n\nThe <<text,`text`>> field has an <<index-prefixes,`index_prefixes`>> option that\nindexes prefixes of all terms and is automatically leveraged by query parsers to\nrun prefix queries. If your use-case involves running lots of prefix queries,\nthis can speed up queries significantly.\n\n[discrete]\n[[faster-filtering-with-constant-keyword]]\n=== Use `constant_keyword` to speed up filtering\n\nThere is a general rule that the cost of a filter is mostly a function of the\nnumber of matched documents. Imagine that you have an index containing cycles.\nThere are a large number of bicycles and many searches perform a filter on\n`cycle_type: bicycle`. This very common filter is unfortunately also very costly\nsince it matches most documents. There is a simple way to avoid running this\nfilter: move bicycles to their own index and filter bicycles by searching this\nindex instead of adding a filter to the query.\n\nUnfortunately this can make client-side logic tricky, which is where\n`constant_keyword` helps. By mapping `cycle_type` as a `constant_keyword` with\nvalue `bicycle` on the index that contains bicycles, clients can keep running\nthe exact same queries as they used to run on the monolithic index and\nElasticsearch will do the right thing on the bicycles index by ignoring filters\non `cycle_type` if the value is `bicycle` and returning no hits otherwise.\n\nHere is what mappings could look like:\n\n[source,console]\n--------------------------------------------------\nPUT bicycles\n{\n  \"mappings\": {\n    \"properties\": {\n      \"cycle_type\": {\n        \"type\": \"constant_keyword\",\n        \"value\": \"bicycle\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\n\nPUT other_cycles\n{\n  \"mappings\": {\n    \"properties\": {\n      \"cycle_type\": {\n        \"type\": \"keyword\"\n      },\n      \"name\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n\nWe are splitting our index in two: one that will contain only bicycles, and\nanother one that contains other cycles: unicycles, tricycles, etc. Then at\nsearch time, we need to search both indices, but we don't need to modify\nqueries.\n\n\n[source,console]\n--------------------------------------------------\nGET bicycles,other_cycles/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": {\n        \"match\": {\n          \"description\": \"dutch\"\n        }\n      },\n      \"filter\": {\n        \"term\": {\n          \"cycle_type\": \"bicycle\"\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[continued]\n\nOn the `bicycles` index, Elasticsearch will simply ignore the `cycle_type`\nfilter and rewrite the search request to the one below:\n\n[source,console]\n--------------------------------------------------\nGET bicycles,other_cycles/_search\n{\n  \"query\": {\n    \"match\": {\n      \"description\": \"dutch\"\n    }\n  }\n}\n--------------------------------------------------\n// TEST[continued]\n\nOn the `other_cycles` index, Elasticsearch will quickly figure out that\n`bicycle` doesn't exist in the terms dictionary of the `cycle_type` field and\nreturn a search response with no hits.\n\nThis is a powerful way of making queries cheaper by putting common values in a\ndedicated index. This idea can also be combined across multiple fields: for\ninstance if you track the color of each cycle and your `bicycles` index ends up\nhaving a majority of black bikes, you could split it into a `bicycles-black`\nand a `bicycles-other-colors` indices.\n\nThe `constant_keyword` is not strictly required for this optimization: it is\nalso possible to update the client-side logic in order to route queries to the\nrelevant indices based on filters. However `constant_keyword` makes it\ntransparently and allows to decouple search requests from the index topology in\nexchange of very little overhead.\n"
}