{
    "meta": {
        "size": 2611,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-simplepatternsplit-tokenizer.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "analysis-simplepatternsplit-tokenizer",
        "version": "8.15"
    },
    "doc": "[[analysis-simplepatternsplit-tokenizer]]\n=== Simple pattern split tokenizer\n++++\n<titleabbrev>Simple pattern split</titleabbrev>\n++++\n\nThe `simple_pattern_split` tokenizer uses a regular expression to split the\ninput into terms at pattern matches. The set of regular expression features it\nsupports is more limited than the <<analysis-pattern-tokenizer,`pattern`>>\ntokenizer, but the tokenization is generally faster.\n\nThis tokenizer does not produce terms from the matches themselves. To produce\nterms from matches using patterns in the same restricted regular expression\nsubset, see the <<analysis-simplepattern-tokenizer,`simple_pattern`>>\ntokenizer.\n\nThis tokenizer uses {lucene-core-javadoc}/org/apache/lucene/util/automaton/RegExp.html[Lucene regular expressions].\nFor an explanation of the supported features and syntax, see <<regexp-syntax,Regular Expression Syntax>>.\n\nThe default pattern is the empty string, which produces one term containing the\nfull input. This tokenizer should always be configured with a non-default\npattern.\n\n[discrete]\n=== Configuration\n\nThe `simple_pattern_split` tokenizer accepts the following parameters:\n\n[horizontal]\n`pattern`::\n    A {lucene-core-javadoc}/org/apache/lucene/util/automaton/RegExp.html[Lucene regular expression], defaults to the empty string.\n\n[discrete]\n=== Example configuration\n\nThis example configures the `simple_pattern_split` tokenizer to split the input\ntext on underscores.\n\n[source,console]\n----------------------------\nPUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"my_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"simple_pattern_split\",\n          \"pattern\": \"_\"\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"an_underscored_phrase\"\n}\n----------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\" : [\n    {\n      \"token\" : \"an\",\n      \"start_offset\" : 0,\n      \"end_offset\" : 2,\n      \"type\" : \"word\",\n      \"position\" : 0\n    },\n    {\n      \"token\" : \"underscored\",\n      \"start_offset\" : 3,\n      \"end_offset\" : 14,\n      \"type\" : \"word\",\n      \"position\" : 1\n    },\n    {\n      \"token\" : \"phrase\",\n      \"start_offset\" : 15,\n      \"end_offset\" : 21,\n      \"type\" : \"word\",\n      \"position\" : 2\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\nThe above example produces these terms:\n\n[source,text]\n---------------------------\n[ an, underscored, phrase ]\n---------------------------\n"
}