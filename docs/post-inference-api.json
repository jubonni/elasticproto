{
    "meta": {
        "timestamp": "2024-11-01T03:02:53.192584",
        "size": 7318,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/post-inference-api.html",
        "type": "documentation",
        "role": [
            "xpack"
        ],
        "has_code": true,
        "title": "post-inference-api",
        "version": "8.15"
    },
    "doc": "[role=\"xpack\"]\n[[post-inference-api]]\n=== Perform inference API\n\nPerforms an inference task on an input text by using an {infer} endpoint.\n\nIMPORTANT: The {infer} APIs enable you to use certain services, such as built-in {ml} models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure, Google AI Studio, Google Vertex AI, Anthropic, Watsonx.ai, or Hugging Face.\nFor built-in models and models uploaded through Eland, the {infer} APIs offer an alternative way to use and manage trained models.\nHowever, if you do not plan to use the {infer} APIs to use these models or if you want to use non-NLP models, use the <<ml-df-trained-models-apis>>.\n\n\n[discrete]\n[[post-inference-api-request]]\n==== {api-request-title}\n\n`POST /_inference/<inference_id>`\n\n`POST /_inference/<task_type>/<inference_id>`\n\n\n[discrete]\n[[post-inference-api-prereqs]]\n==== {api-prereq-title}\n\n* Requires the `monitor_inference` <<privileges-list-cluster,cluster privilege>>\n(the built-in `inference_admin` and `inference_user` roles grant this privilege)\n\n[discrete]\n[[post-inference-api-desc]]\n==== {api-description-title}\n\nThe perform {infer} API enables you to use {ml} models to perform specific tasks\non data that you provide as an input. The API returns a response with the\nresults of the tasks. The {infer} endpoint you use can perform one specific task\nthat has been defined when the endpoint was created with the\n<<put-inference-api>>.\n\n\n[discrete]\n[[post-inference-api-path-params]]\n==== {api-path-parms-title}\n\n`<inference_id>`::\n(Required, string)\nThe unique identifier of the {infer} endpoint.\n\n\n`<task_type>`::\n(Optional, string)\nThe type of {infer} task that the model performs.\n\n\n[discrete]\n[[post-inference-api-query-params]]\n==== {api-query-parms-title}\n\n`timeout`::\n(Optional, timeout)\nControls the amount of time to wait for the inference to complete. Defaults to 30\nseconds.\n\n[discrete]\n[[post-inference-api-request-body]]\n==== {api-request-body-title}\n\n`input`::\n(Required, string or array of strings)\nThe text on which you want to perform the {infer} task.\n`input` can be a single string or an array.\n+\n--\n[NOTE]\n====\nInference endpoints for the `completion` task type currently only support a\nsingle string as input.\n====\n--\n\n`query`::\n(Required, string)\nOnly for `rerank` {infer} endpoints. The search query text.\n\n`task_settings`::\n(Optional, object)\nTask settings for the individual {infer} request.\nThese settings are specific to the `<task_type>` you specified and override the task settings specified when initializing the service.\n\n[discrete]\n[[post-inference-api-example]]\n==== {api-examples-title}\n\n\n[discrete]\n[[inference-example-completion]]\n===== Completion example\n\nThe following example performs a completion on the example question.\n\n\n[source,console]\n------------------------------------------------------------\nPOST _inference/completion/openai_chat_completions\n{\n  \"input\": \"What is Elastic?\"\n}\n------------------------------------------------------------\n// TEST[skip:TBD]\n\n\nThe API returns the following response:\n\n\n[source,console-result]\n------------------------------------------------------------\n{\n  \"completion\": [\n    {\n      \"result\": \"Elastic is a company that provides a range of software solutions for search, logging, security, and analytics. Their flagship product is Elasticsearch, an open-source, distributed search engine that allows users to search, analyze, and visualize large volumes of data in real-time. Elastic also offers products such as Kibana, a data visualization tool, and Logstash, a log management and pipeline tool, as well as various other tools and solutions for data analysis and management.\"\n    }\n  ]\n}\n------------------------------------------------------------\n// NOTCONSOLE\n\n[discrete]\n[[inference-example-rerank]]\n===== Rerank example\n\nThe following example performs reranking on the example input.\n\n[source,console]\n------------------------------------------------------------\nPOST _inference/rerank/cohere_rerank\n{\n  \"input\": [\"luke\", \"like\", \"leia\", \"chewy\",\"r2d2\", \"star\", \"wars\"],\n  \"query\": \"star wars main character\"\n}\n------------------------------------------------------------\n// TEST[skip:TBD]\n\nThe API returns the following response:\n\n\n[source,console-result]\n------------------------------------------------------------\n{\n  \"rerank\": [\n    {\n      \"index\": \"2\",\n      \"relevance_score\": \"0.011597361\",\n      \"text\": \"leia\"\n    },\n    {\n      \"index\": \"0\",\n      \"relevance_score\": \"0.006338922\",\n      \"text\": \"luke\"\n    },\n    {\n      \"index\": \"5\",\n      \"relevance_score\": \"0.0016166499\",\n      \"text\": \"star\"\n    },\n    {\n      \"index\": \"4\",\n      \"relevance_score\": \"0.0011695103\",\n      \"text\": \"r2d2\"\n    },\n    {\n      \"index\": \"1\",\n      \"relevance_score\": \"5.614787E-4\",\n      \"text\": \"like\"\n    },\n    {\n      \"index\": \"6\",\n      \"relevance_score\": \"3.7850367E-4\",\n      \"text\": \"wars\"\n    },\n    {\n      \"index\": \"3\",\n      \"relevance_score\": \"1.2508839E-5\",\n      \"text\": \"chewy\"\n    }\n  ]\n}\n------------------------------------------------------------\n\n\n[discrete]\n[[inference-example-sparse]]\n===== Sparse embedding example\n\nThe following example performs sparse embedding on the example sentence.\n\n\n[source,console]\n------------------------------------------------------------\nPOST _inference/sparse_embedding/my-elser-model\n{\n  \"input\": \"The sky above the port was the color of television tuned to a dead channel.\"\n}\n------------------------------------------------------------\n// TEST[skip:TBD]\n\n\nThe API returns the following response:\n\n\n[source,console-result]\n------------------------------------------------------------\n{\n  \"sparse_embedding\": [\n    {\n      \"port\": 2.1259406,\n      \"sky\": 1.7073475,\n      \"color\": 1.6922266,\n      \"dead\": 1.6247464,\n      \"television\": 1.3525393,\n      \"above\": 1.2425821,\n      \"tuned\": 1.1440028,\n      \"colors\": 1.1218185,\n      \"tv\": 1.0111054,\n      \"ports\": 1.0067928,\n      \"poem\": 1.0042328,\n      \"channel\": 0.99471164,\n      \"tune\": 0.96235967,\n      \"scene\": 0.9020516,\n      (...)\n    },\n    (...)\n  ]\n}\n------------------------------------------------------------\n// NOTCONSOLE\n\n[discrete]\n[[inference-example-text-embedding]]\n===== Text embedding example\n\nThe following example performs text embedding on the example sentence using the Cohere integration.\n\n\n[source,console]\n------------------------------------------------------------\nPOST _inference/text_embedding/my-cohere-endpoint\n{\n  \"input\": \"The sky above the port was the color of television tuned to a dead channel.\",\n  \"task_settings\": {\n    \"input_type\": \"ingest\"\n  }\n}\n------------------------------------------------------------\n// TEST[skip:TBD]\n\n\nThe API returns the following response:\n\n\n[source,console-result]\n------------------------------------------------------------\n{\n  \"text_embedding\": [\n    {\n      \"embedding\": [\n        {\n          0.018569946,\n          -0.036895752,\n          0.01486969,\n          -0.0045204163,\n          -0.04385376,\n          0.0075950623,\n          0.04260254,\n          -0.004005432,\n          0.007865906,\n          0.030792236,\n          -0.050476074,\n          0.011795044,\n          -0.011642456,\n          -0.010070801,\n          (...)\n        },\n        (...)\n      ]\n    }\n  ]\n}\n------------------------------------------------------------\n// NOTCONSOLE\n"
}