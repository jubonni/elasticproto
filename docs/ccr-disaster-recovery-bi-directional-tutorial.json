{
    "meta": {
        "timestamp": "2024-11-01T03:07:08.793274",
        "size": 9165,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-disaster-recovery-bi-directional-tutorial.html",
        "type": "documentation",
        "role": [
            "xpack"
        ],
        "has_code": false,
        "title": "ccr-disaster-recovery-bi-directional-tutorial",
        "version": "8.15"
    },
    "doc": "[role=\"xpack\"]\n[[ccr-disaster-recovery-bi-directional-tutorial]]\n=== Tutorial: Disaster recovery based on bi-directional {ccr}\n++++\n<titleabbrev>Bi-directional disaster recovery</titleabbrev>\n++++\n\n////\n[source,console]\n----\nPUT _data_stream/logs-generic-default\n----\n// TESTSETUP\n\n[source,console]\n----\nDELETE /_data_stream/*\n----\n// TEARDOWN\n////\n\nLearn how to set up disaster recovery between two clusters based on\nbi-directional {ccr}. The following tutorial is designed for data streams which support\n<<update-docs-in-a-data-stream-by-query,update by query>> and <<delete-docs-in-a-data-stream-by-query,delete by query>>. You can only perform these actions on the leader index.\n\nThis tutorial works with {ls} as the source of ingestion. It takes advantage of a {ls} feature where {logstash-ref}/plugins-outputs-elasticsearch.html[the {ls} output to {es}] can be load balanced across an array of hosts specified. {beats} and {agents} currently do not\nsupport multiple outputs. It should also be possible to set up a proxy\n(load balancer) to redirect traffic without {ls} in this tutorial.\n\n* Setting up a remote cluster on `clusterA` and `clusterB`.\n* Setting up bi-directional cross-cluster replication with exclusion patterns.\n* Setting up {ls} with multiple hosts to allow automatic load balancing and switching during disasters.\n\nimage::images/ccr-bi-directional-disaster-recovery.png[Bi-directional cross cluster replication failover and failback]\n\n[[ccr-tutorial-initial-setup]]\n==== Initial setup\n. Set up a remote cluster on both clusters.\n+\n[source,console]\n----\n### On cluster A ###\nPUT _cluster/settings\n{\n  \"persistent\": {\n    \"cluster\": {\n      \"remote\": {\n        \"clusterB\": {\n          \"mode\": \"proxy\",\n          \"skip_unavailable\": true,\n          \"server_name\": \"clusterb.es.region-b.gcp.elastic-cloud.com\",\n          \"proxy_socket_connections\": 18,\n          \"proxy_address\": \"clusterb.es.region-b.gcp.elastic-cloud.com:9400\"\n        }\n      }\n    }\n  }\n}\n### On cluster B ###\nPUT _cluster/settings\n{\n  \"persistent\": {\n    \"cluster\": {\n      \"remote\": {\n        \"clusterA\": {\n          \"mode\": \"proxy\",\n          \"skip_unavailable\": true,\n          \"server_name\": \"clustera.es.region-a.gcp.elastic-cloud.com\",\n          \"proxy_socket_connections\": 18,\n          \"proxy_address\": \"clustera.es.region-a.gcp.elastic-cloud.com:9400\"\n        }\n      }\n    }\n  }\n}\n----\n// TEST[setup:host]\n// TEST[s/\"server_name\": \"clustera.es.region-a.gcp.elastic-cloud.com\",//]\n// TEST[s/\"server_name\": \"clusterb.es.region-b.gcp.elastic-cloud.com\",//]\n// TEST[s/\"proxy_socket_connections\": 18,//]\n// TEST[s/clustera.es.region-a.gcp.elastic-cloud.com:9400/\\${transport_host}/]\n// TEST[s/clusterb.es.region-b.gcp.elastic-cloud.com:9400/\\${transport_host}/]\n\n. Set up bi-directional cross-cluster replication.\n+\n[source,console]\n----\n### On cluster A ###\nPUT /_ccr/auto_follow/logs-generic-default\n{\n  \"remote_cluster\": \"clusterB\",\n  \"leader_index_patterns\": [\n    \".ds-logs-generic-default-20*\"\n  ],\n  \"leader_index_exclusion_patterns\":\"*-replicated_from_clustera\",\n  \"follow_index_pattern\": \"{{leader_index}}-replicated_from_clusterb\"\n}\n\n### On cluster B ###\nPUT /_ccr/auto_follow/logs-generic-default\n{\n  \"remote_cluster\": \"clusterA\",\n  \"leader_index_patterns\": [\n    \".ds-logs-generic-default-20*\"\n  ],\n  \"leader_index_exclusion_patterns\":\"*-replicated_from_clusterb\",\n  \"follow_index_pattern\": \"{{leader_index}}-replicated_from_clustera\"\n}\n----\n// TEST[setup:remote_cluster]\n// TEST[s/clusterA/remote_cluster/]\n// TEST[s/clusterB/remote_cluster/]\n+\nIMPORTANT: Existing data on the cluster will not be replicated by\n`_ccr/auto_follow` even though the patterns may match. This function will only\nreplicate newly created backing indices (as part of the data stream).\n+\nIMPORTANT: Use `leader_index_exclusion_patterns` to avoid recursion.\n+\nTIP: `follow_index_pattern` allows lowercase characters only.\n+\nTIP: This step cannot be executed via the {kib} UI due to the lack of an exclusion\npattern in the UI. Use the API in this step.\n\n. Set up the {ls} configuration file.\n+\nThis example uses the input generator to demonstrate the document\ncount in the clusters. Reconfigure this section\nto suit your own use case.\n+\n[source,logstash]\n----\n### On Logstash server ###\n### This is a logstash config file ###\ninput {\n  generator{\n    message => 'Hello World'\n    count => 100\n  }\n}\noutput {\n  elasticsearch {\n    hosts => [\"https://clustera.es.region-a.gcp.elastic-cloud.com:9243\",\"https://clusterb.es.region-b.gcp.elastic-cloud.com:9243\"]\n    user => \"logstash-user\"\n    password => \"same_password_for_both_clusters\"\n  }\n}\n----\n+\nIMPORTANT: The key point is that when `cluster A` is down, all traffic will be\nautomatically redirected to `cluster B`. Once `cluster A` comes back, traffic\nis automatically redirected back to `cluster A` again. This is achieved by the\noption `hosts` where multiple ES cluster endpoints are specified in the\narray `[clusterA, clusterB]`.\n+\nTIP: Set up the same password for the same user on both clusters to use this load-balancing feature.\n\n. Start {ls} with the earlier configuration file.\n+\n[source,sh]\n----\n### On Logstash server ###\nbin/logstash -f multiple_hosts.conf\n----\n\n. Observe document counts in data streams.\n+\nThe setup creates a data stream named `logs-generic-default` on each of the clusters. {ls} will write 50% of the documents to `cluster A` and 50% of the documents to `cluster B` when both clusters are up.\n+\nBi-directional {ccr} will create one more data stream on each of the clusters\nwith the `-replication_from_cluster{a|b}` suffix. At the end of this step:\n+\n* data streams on cluster A contain:\n** 50 documents in `logs-generic-default-replicated_from_clusterb`\n** 50 documents in `logs-generic-default`\n* data streams on cluster B contain:\n** 50 documents in `logs-generic-default-replicated_from_clustera`\n** 50 documents in `logs-generic-default`\n\n. Queries should be set up to search across both data streams.\nA query on `logs*`, on either of the clusters, returns 100\nhits in total.\n+\n[source,console]\n----\nGET logs*/_search?size=0\n----\n\n\n==== Failover when `clusterA` is down\n. You can simulate this by shutting down either of the clusters. Let's shut down\n`cluster A` in this tutorial.\n. Start {ls} with the same configuration file. (This step is not required in real\nuse cases where {ls} ingests continuously.)\n+\n[source,sh]\n----\n### On Logstash server ###\nbin/logstash -f multiple_hosts.conf\n----\n\n. Observe all {ls} traffic will be redirected to `cluster B` automatically.\n+\nTIP: You should also redirect all search traffic to the `clusterB` cluster during this time.\n\n. The two data streams on `cluster B` now contain a different number of documents.\n+\n* data streams on cluster A (down)\n** 50 documents in `logs-generic-default-replicated_from_clusterb`\n** 50 documents in `logs-generic-default`\n* data streams On cluster B (up)\n** 50 documents in `logs-generic-default-replicated_from_clustera`\n** 150 documents in `logs-generic-default`\n\n\n==== Failback when `clusterA` comes back\n. You can simulate this by turning `cluster A` back on.\n. Data ingested to `cluster B` during `cluster A` 's downtime will be\nautomatically replicated.\n+\n* data streams on cluster A\n** 150 documents in `logs-generic-default-replicated_from_clusterb`\n** 50 documents in `logs-generic-default`\n* data streams on cluster B\n** 50 documents in `logs-generic-default-replicated_from_clustera`\n** 150 documents in `logs-generic-default`\n\n. If you have {ls} running at this time, you will also observe traffic is\nsent to both clusters.\n\n==== Perform update or delete by query\nIt is possible to update or delete the documents but you can only perform these actions on the leader index.\n\n. First identify which backing index contains the document you want to update.\n+\n[source,console]\n----\n### On either of the cluster ###\nGET logs-generic-default*/_search?filter_path=hits.hits._index\n{\n\"query\": {\n    \"match\": {\n      \"event.sequence\": \"97\"\n    }\n  }\n}\n----\n+\n* If the hits returns `\"_index\": \".ds-logs-generic-default-replicated_from_clustera-<yyyy.MM.dd>-*\"`, then you need to proceed to the next step on `cluster A`.\n* If the hits returns `\"_index\": \".ds-logs-generic-default-replicated_from_clusterb-<yyyy.MM.dd>-*\"`, then you need to proceed to the next step on `cluster B`.\n* If the hits returns `\"_index\": \".ds-logs-generic-default-<yyyy.MM.dd>-*\"`, then you need to proceed to the next step on the same cluster where you performed the search query.\n\n. Perform the update (or delete) by query:\n+\n[source,console]\n----\n### On the cluster identified from the previous step ###\nPOST logs-generic-default/_update_by_query\n{\n  \"query\": {\n    \"match\": {\n      \"event.sequence\": \"97\"\n    }\n  },\n  \"script\": {\n    \"source\": \"ctx._source.event.original = params.new_event\",\n    \"lang\": \"painless\",\n    \"params\": {\n      \"new_event\": \"FOOBAR\"\n    }\n  }\n}\n----\n+\nTIP: If a soft delete is merged away before it can be replicated to a follower the following process will fail due to incomplete history on the leader, see <<ccr-index-soft-deletes-retention-period, index.soft_deletes.retention_lease.period>> for more details.\n"
}