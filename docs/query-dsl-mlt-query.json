{
    "meta": {
        "timestamp": "2024-11-01T03:02:52.983580",
        "size": 9423,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-mlt-query.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "query-dsl-mlt-query",
        "version": "8.15"
    },
    "doc": "[[query-dsl-mlt-query]]\n=== More like this query\n++++\n<titleabbrev>More like this</titleabbrev>\n++++\n\nThe More Like This Query finds documents that are \"like\" a given\nset of documents. In order to do so, MLT selects a set of representative terms\nof these input documents, forms a query using these terms, executes the query\nand returns the results. The user controls the input documents, how the terms\nshould be selected and how the query is formed.\n\nThe simplest use case consists of asking for documents that are similar to a\nprovided piece of text. Here, we are asking for all movies that have some text\nsimilar to \"Once upon a time\" in their \"title\" and in their \"description\"\nfields, limiting the number of selected terms to 12.\n\n[source,console]\n--------------------------------------------------\nGET /_search\n{\n  \"query\": {\n    \"more_like_this\" : {\n      \"fields\" : [\"title\", \"description\"],\n      \"like\" : \"Once upon a time\",\n      \"min_term_freq\" : 1,\n      \"max_query_terms\" : 12\n    }\n  }\n}\n--------------------------------------------------\n\nA more complicated use case consists of mixing texts with documents already\nexisting in the index. In this case, the syntax to specify a document is\nsimilar to the one used in the <<docs-multi-get,Multi GET API>>.\n\n[source,console]\n--------------------------------------------------\nGET /_search\n{\n  \"query\": {\n    \"more_like_this\": {\n      \"fields\": [ \"title\", \"description\" ],\n      \"like\": [\n        {\n          \"_index\": \"imdb\",\n          \"_id\": \"1\"\n        },\n        {\n          \"_index\": \"imdb\",\n          \"_id\": \"2\"\n        },\n        \"and potentially some more text here as well\"\n      ],\n      \"min_term_freq\": 1,\n      \"max_query_terms\": 12\n    }\n  }\n}\n--------------------------------------------------\n\nFinally, users can mix some texts, a chosen set of documents but also provide\ndocuments not necessarily present in the index. To provide documents not\npresent in the index, the syntax is similar to <<docs-termvectors-artificial-doc,artificial documents>>.\n\n[source,console]\n--------------------------------------------------\nGET /_search\n{\n  \"query\": {\n    \"more_like_this\": {\n      \"fields\": [ \"name.first\", \"name.last\" ],\n      \"like\": [\n        {\n          \"_index\": \"marvel\",\n          \"doc\": {\n            \"name\": {\n              \"first\": \"Ben\",\n              \"last\": \"Grimm\"\n            },\n            \"_doc\": \"You got no idea what I'd... what I'd give to be invisible.\"\n          }\n        },\n        {\n          \"_index\": \"marvel\",\n          \"_id\": \"2\"\n        }\n      ],\n      \"min_term_freq\": 1,\n      \"max_query_terms\": 12\n    }\n  }\n}\n--------------------------------------------------\n\n==== How it Works\n\nSuppose we wanted to find all documents similar to a given input document.\nObviously, the input document itself should be its best match for that type of\nquery. And the reason would be mostly, according to\nlink:https://lucene.apache.org/core/4_9_0/core/org/apache/lucene/search/similarities/TFIDFSimilarity.html[Lucene scoring formula],\ndue to the terms with the highest tf-idf. Therefore, the terms of the input\ndocument that have the highest tf-idf are good representatives of that\ndocument, and could be used within a disjunctive query (or `OR`) to retrieve similar\ndocuments. The MLT query simply extracts the text from the input document,\nanalyzes it, usually using the same analyzer at the field, then selects the\ntop K terms with highest tf-idf to form a disjunctive query of these terms.\n\nIMPORTANT: The fields on which to perform MLT must be indexed and of type\n`text` or `keyword`. Additionally, when using `like` with documents, either\n`_source` must be enabled or the fields must be `stored` or store\n`term_vector`. In order to speed up analysis, it could help to store term\nvectors at index time.\n\nFor example, if we wish to perform MLT on the \"title\" and \"tags.raw\" fields,\nwe can explicitly store their `term_vector` at index time. We can still\nperform MLT on the \"description\" and \"tags\" fields, as `_source` is enabled by\ndefault, but there will be no speed up on analysis for these fields.\n\n[source,console]\n--------------------------------------------------\nPUT /imdb\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\",\n        \"term_vector\": \"yes\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      },\n      \"tags\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"raw\": {\n            \"type\": \"text\",\n            \"analyzer\": \"keyword\",\n            \"term_vector\": \"yes\"\n          }\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n==== Parameters\n\nThe only required parameter is `like`, all other parameters have sensible\ndefaults. There are three types of parameters: one to specify the document\ninput, the other one for term selection and for query formation.\n\n[discrete]\n==== Document Input Parameters\n\n[horizontal]\n`like`::\nThe only *required* parameter of the MLT query is `like` and follows a\nversatile syntax, in which the user can specify free form text and/or a single\nor multiple documents (see examples above). The syntax to specify documents is\nsimilar to the one used by the <<docs-multi-get,Multi GET API>>. When\nspecifying documents, the text is fetched from `fields` unless overridden in\neach document request. The text is analyzed by the analyzer at the field, but\ncould also be overridden. The syntax to override the analyzer at the field\nfollows a similar syntax to the `per_field_analyzer` parameter of the\n<<docs-termvectors-per-field-analyzer,Term Vectors API>>.\nAdditionally, to provide documents not necessarily present in the index,\n<<docs-termvectors-artificial-doc,artificial documents>> are also supported.\n\n`unlike`::\nThe `unlike` parameter is used in conjunction with `like` in order not to\nselect terms found in a chosen set of documents. In other words, we could ask\nfor documents `like: \"Apple\"`, but `unlike: \"cake crumble tree\"`. The syntax\nis the same as `like`.\n\n`fields`::\nA list of fields to fetch and analyze the text from. Defaults to the\n`index.query.default_field` index setting, which has a default value of `*`. The\n`*` value matches all fields eligible for <<term-level-queries,term-level\nqueries>>, excluding metadata fields.\n\n[discrete]\n[[mlt-query-term-selection]]\n==== Term Selection Parameters\n\n[horizontal]\n`max_query_terms`::\nThe maximum number of query terms that will be selected. Increasing this value\ngives greater accuracy at the expense of query execution speed. Defaults to\n`25`.\n\n`min_term_freq`::\nThe minimum term frequency below which the terms will be ignored from the\ninput document. Defaults to `2`.\n\n`min_doc_freq`::\nThe minimum document frequency below which the terms will be ignored from the\ninput document. Defaults to `5`.\n\n`max_doc_freq`::\nThe maximum document frequency above which the terms will be ignored from the\ninput document. This could be useful in order to ignore highly frequent words\nsuch as stop words. Defaults to unbounded (`Integer.MAX_VALUE`, which is `2^31-1`\nor 2147483647).\n\n`min_word_length`::\nThe minimum word length below which the terms will be ignored. Defaults to `0`.\n\n`max_word_length`::\nThe maximum word length above which the terms will be ignored. Defaults to\nunbounded (`0`).\n\n`stop_words`::\nAn array of stop words. Any word in this set is considered \"uninteresting\" and\nignored. If the analyzer allows for stop words, you might want to tell MLT to\nexplicitly ignore them, as for the purposes of document similarity it seems\nreasonable to assume that \"a stop word is never interesting\".\n\n`analyzer`::\nThe analyzer that is used to analyze the free form text. Defaults to the\nanalyzer associated with the first field in `fields`.\n\n[discrete]\n==== Query Formation Parameters\n\n[horizontal]\n`minimum_should_match`::\nAfter the disjunctive query has been formed, this parameter controls the\nnumber of terms that must match.\nThe syntax is the same as the <<query-dsl-minimum-should-match,minimum should match>>.\n(Defaults to `\"30%\"`).\n\n`fail_on_unsupported_field`::\nControls whether the query should fail (throw an exception) if any of the \nspecified fields are not of the supported types\n(`text` or `keyword`). Set this to `false` to ignore the field and continue\nprocessing. Defaults to `true`.\n\n`boost_terms`::\nEach term in the formed query could be further boosted by their tf-idf score.\nThis sets the boost factor to use when using this feature. Defaults to\ndeactivated (`0`). Any other positive value activates terms boosting with the\ngiven boost factor.\n\n`include`::\nSpecifies whether the input documents should also be included in the search\nresults returned. Defaults to `false`.\n\n`boost`::\nSets the boost value of the whole query. Defaults to `1.0`.\n\n==== Alternative\nTo take more control over the construction of a query for similar documents it is worth considering writing custom client code to assemble selected terms from an example document into a Boolean query with the desired settings. The logic in `more_like_this` that selects \"interesting\" words from a piece of text is also accessible via the <<docs-termvectors,TermVectors API>>. For example, using the termvectors API it would be possible to present users with a selection of topical keywords found in a document's text, allowing them to select words of interest to drill down on, rather than using the more \"black-box\" approach of matching used by `more_like_this`.\n"
}