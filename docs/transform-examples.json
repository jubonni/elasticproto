{
    "meta": {
        "timestamp": "2024-11-01T02:49:25.071066",
        "size": 21402,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/transform-examples.html",
        "type": "documentation",
        "role": [
            "xpack",
            "screenshot",
            "screenshot",
            "screenshot"
        ],
        "has_code": false,
        "title": "transform-examples",
        "version": "8.15"
    },
    "doc": "[role=\"xpack\"]\n[[transform-examples]]\n= {transform-cap} examples\n++++\n<titleabbrev>Examples</titleabbrev>\n++++\n\nThese examples demonstrate how to use {transforms} to derive useful insights \nfrom your data. All the examples use one of the \n{kibana-ref}/add-sample-data.html[{kib} sample datasets]. For a more detailed, \nstep-by-step example, see <<ecommerce-transforms>>.\n\n* <<example-best-customers>> \n* <<example-airline>> \n* <<example-clientips>> \n* <<example-last-log>> \n* <<example-bytes>> \n* <<example-customer-names>>\n\n[[example-best-customers]]\n== Finding your best customers\n\nThis example uses the eCommerce orders sample data set to find the customers who \nspent the most in a hypothetical webshop. Let's use the `pivot` type of \n{transform} such that the destination index contains the number of orders, the \ntotal price of the orders, the amount of unique products and the average price \nper order, and the total amount of ordered products for each customer.\n\n[role=\"screenshot\"]\nimage::images/transform-ex1-1.jpg[\"Finding your best customers with {transforms} in {kib}\"]\n\nAlternatively, you can use the <<preview-transform, preview {transform}>> and \nthe <<put-transform, create {transform} API>>.\n\n.API example\n[%collapsible]\n====\n\n[source,console]\n----------------------------------\nPOST _transform/_preview\n{\n  \"source\": {\n    \"index\": \"kibana_sample_data_ecommerce\"\n  },\n  \"dest\" : { <1>\n    \"index\" : \"sample_ecommerce_orders_by_customer\"\n  },\n  \"pivot\": {\n    \"group_by\": { <2>\n      \"user\": { \"terms\": { \"field\": \"user\" }}, \n      \"customer_id\": { \"terms\": { \"field\": \"customer_id\" }}\n    },\n    \"aggregations\": {\n      \"order_count\": { \"value_count\": { \"field\": \"order_id\" }},\n      \"total_order_amt\": { \"sum\": { \"field\": \"taxful_total_price\" }},\n      \"avg_amt_per_order\": { \"avg\": { \"field\": \"taxful_total_price\" }},\n      \"avg_unique_products_per_order\": { \"avg\": { \"field\": \"total_unique_products\" }},\n      \"total_unique_products\": { \"cardinality\": { \"field\": \"products.product_id\" }}\n    }\n  }\n}\n----------------------------------\n// TEST[skip:setup kibana sample data]\n\n<1> The destination index for the {transform}. It is ignored by `_preview`.\n<2> Two `group_by` fields is selected. This means the {transform} contains a \nunique row per `user` and `customer_id` combination. Within this data set, both \nthese fields are unique. By including both in the {transform}, it gives more \ncontext to the final results.\n\nNOTE: In the example above, condensed JSON formatting is used for easier \nreadability of the pivot object.\n\nThe preview {transforms} API enables you to see the layout of the {transform} in \nadvance, populated with some sample values. For example:\n\n[source,js]\n----------------------------------\n{\n  \"preview\" : [\n    {\n      \"total_order_amt\" : 3946.9765625,\n      \"order_count\" : 59.0,\n      \"total_unique_products\" : 116.0,\n      \"avg_unique_products_per_order\" : 2.0,\n      \"customer_id\" : \"10\",\n      \"user\" : \"recip\",\n      \"avg_amt_per_order\" : 66.89790783898304\n    },\n    ...\n    ]\n  }\n----------------------------------\n// NOTCONSOLE\n\n====\n\n\nThis {transform} makes it easier to answer questions such as:\n\n* Which customers spend the most?\n\n* Which customers spend the most per order?\n\n* Which customers order most often?\n\n* Which customers ordered the least number of different products?\n\nIt's possible to answer these questions using aggregations alone, however \n{transforms} allow us to persist this data as a customer centric index. This \nenables us to analyze data at scale and gives more flexibility to explore and \nnavigate data from a customer centric perspective. In some cases, it can even \nmake creating visualizations much simpler.\n\n\n[[example-airline]]\n== Finding air carriers with the most delays\n\nThis example uses the Flights sample data set to find out which air carrier \nhad the most delays. First, filter the source data such that it excludes all \nthe cancelled flights by using a query filter. Then transform the data to \ncontain the distinct number of flights, the sum of delayed minutes, and the sum \nof the flight minutes by air carrier. Finally, use a \n<<search-aggregations-pipeline-bucket-script-aggregation,`bucket_script`>>\nto determine what percentage of the flight time was actually delay.\n\n[source,console]\n----------------------------------\nPOST _transform/_preview\n{\n  \"source\": {\n    \"index\": \"kibana_sample_data_flights\",\n    \"query\": { <1>\n      \"bool\": {\n        \"filter\": [\n          { \"term\":  { \"Cancelled\": false } }\n        ]\n      }\n    }\n  },\n  \"dest\" : { <2>\n    \"index\" : \"sample_flight_delays_by_carrier\"\n  },\n  \"pivot\": {\n    \"group_by\": { <3>\n      \"carrier\": { \"terms\": { \"field\": \"Carrier\" }}\n    },\n    \"aggregations\": {\n      \"flights_count\": { \"value_count\": { \"field\": \"FlightNum\" }},\n      \"delay_mins_total\": { \"sum\": { \"field\": \"FlightDelayMin\" }},\n      \"flight_mins_total\": { \"sum\": { \"field\": \"FlightTimeMin\" }},\n      \"delay_time_percentage\": { <4>\n        \"bucket_script\": {\n          \"buckets_path\": {\n            \"delay_time\": \"delay_mins_total.value\",\n            \"flight_time\": \"flight_mins_total.value\"\n          },\n          \"script\": \"(params.delay_time / params.flight_time) * 100\"\n        }\n      }\n    }\n  }\n}\n----------------------------------\n// TEST[skip:setup kibana sample data]\n\n<1> Filter the source data to select only flights that are not cancelled.\n<2> The destination index for the {transform}. It is ignored by `_preview`.\n<3> The data is grouped by the `Carrier` field which contains the airline name.\n<4> This `bucket_script` performs calculations on the results that are returned \nby the aggregation. In this particular example, it calculates what percentage of \ntravel time was taken up by delays.\n\nThe preview shows you that the new index would contain data like this for each \ncarrier:\n\n[source,js]\n----------------------------------\n{\n  \"preview\" : [\n    {\n      \"carrier\" : \"ES-Air\",\n      \"flights_count\" : 2802.0,\n      \"flight_mins_total\" : 1436927.5130677223,\n      \"delay_time_percentage\" : 9.335543983955839,\n      \"delay_mins_total\" : 134145.0\n    },\n    ...\n  ]\n}\n----------------------------------\n// NOTCONSOLE\n\nThis {transform} makes it easier to answer questions such as:\n\n* Which air carrier has the most delays as a percentage of flight time?\n\nNOTE: This data is fictional and does not reflect actual delays or flight stats \nfor any of the featured destination or origin airports.\n\n\n[[example-clientips]]\n== Finding suspicious client IPs\n\nThis example uses the web log sample data set to identify suspicious client IPs. \nIt transforms the data such that the new index contains the sum of bytes and the \nnumber of distinct URLs, agents, incoming requests by location, and geographic \ndestinations for each client IP. It also uses filter aggregations to count the \nspecific types of HTTP responses that each client IP receives. Ultimately, the \nexample below transforms web log data into an entity centric index where the \nentity is `clientip`.\n\n[source,console]\n----------------------------------\nPUT _transform/suspicious_client_ips\n{\n  \"source\": {\n    \"index\": \"kibana_sample_data_logs\"\n  },\n  \"dest\" : { <1>\n    \"index\" : \"sample_weblogs_by_clientip\"\n  },\n  \"sync\" : { <2>\n    \"time\": {\n      \"field\": \"timestamp\",\n      \"delay\": \"60s\"\n    }\n  },\n  \"pivot\": {\n    \"group_by\": {  <3>\n      \"clientip\": { \"terms\": { \"field\": \"clientip\" } }\n      },\n    \"aggregations\": {\n      \"url_dc\": { \"cardinality\": { \"field\": \"url.keyword\" }},\n      \"bytes_sum\": { \"sum\": { \"field\": \"bytes\" }},\n      \"geo.src_dc\": { \"cardinality\": { \"field\": \"geo.src\" }},\n      \"agent_dc\": { \"cardinality\": { \"field\": \"agent.keyword\" }},\n      \"geo.dest_dc\": { \"cardinality\": { \"field\": \"geo.dest\" }},\n      \"responses.total\": { \"value_count\": { \"field\": \"timestamp\" }},\n      \"success\" : { <4>\n         \"filter\": { \n            \"term\": { \"response\" : \"200\"}} \n        },\n      \"error404\" : {\n         \"filter\": { \n            \"term\": { \"response\" : \"404\"}}\n        },\n      \"error5xx\" : {\n         \"filter\": {\n            \"range\": { \"response\" : { \"gte\": 500, \"lt\": 600}}}\n        },\n      \"timestamp.min\": { \"min\": { \"field\": \"timestamp\" }},\n      \"timestamp.max\": { \"max\": { \"field\": \"timestamp\" }},\n      \"timestamp.duration_ms\": { <5>\n        \"bucket_script\": {\n          \"buckets_path\": {\n            \"min_time\": \"timestamp.min.value\",\n            \"max_time\": \"timestamp.max.value\"\n          },\n          \"script\": \"(params.max_time - params.min_time)\"\n        }\n      }\n    }\n  }\n}\n----------------------------------\n// TEST[skip:setup kibana sample data]\n\n<1> The destination index for the {transform}.\n<2> Configures the {transform} to run continuously. It uses the `timestamp` \nfield to synchronize the source and destination indices. The worst case \ningestion delay is 60 seconds.\n<3> The data is grouped by the `clientip` field.\n<4> Filter aggregation that counts the occurrences of successful (`200`)\nresponses in the `response` field. The following two aggregations (`error404`\nand `error5xx`) count the error responses by error codes, matching an exact\nvalue or a range of response codes.\n<5> This `bucket_script` calculates the duration of the `clientip` access based\non the results of the aggregation.\n\n\nAfter you create the {transform}, you must start it:\n\n[source,console]\n----------------------------------\nPOST _transform/suspicious_client_ips/_start\n----------------------------------\n// TEST[skip:setup kibana sample data]\n\n\nShortly thereafter, the first results should be available in the destination\nindex:\n\n[source,console]\n----------------------------------\nGET sample_weblogs_by_clientip/_search\n----------------------------------\n// TEST[skip:setup kibana sample data]\n\n\nThe search result shows you data like this for each client IP:\n\n[source,js]\n----------------------------------\n    \"hits\" : [\n      {\n        \"_index\" : \"sample_weblogs_by_clientip\",\n        \"_id\" : \"MOeHH_cUL5urmartKj-b5UQAAAAAAAAA\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"geo\" : {\n            \"src_dc\" : 2.0,\n            \"dest_dc\" : 2.0\n          },\n          \"success\" : 2,\n          \"error404\" : 0,\n          \"error503\" : 0,\n          \"clientip\" : \"0.72.176.46\",\n          \"agent_dc\" : 2.0,\n          \"bytes_sum\" : 4422.0,\n          \"responses\" : {\n            \"total\" : 2.0\n          },\n          \"url_dc\" : 2.0,\n          \"timestamp\" : {\n            \"duration_ms\" : 5.2191698E8,\n            \"min\" : \"2020-03-16T07:51:57.333Z\",\n            \"max\" : \"2020-03-22T08:50:34.313Z\"\n          }\n        }\n      }\n    ]\n----------------------------------\n// NOTCONSOLE\n\nNOTE: Like other Kibana sample data sets, the web log sample dataset contains\ntimestamps relative to when you installed it, including timestamps in the \nfuture. The {ctransform} will pick up the data points once they are in the past. \nIf you installed the web log sample dataset some time ago, you can uninstall and \nreinstall it and the timestamps will change.\n\n\nThis {transform} makes it easier to answer questions such as:\n\n* Which client IPs are transferring the most amounts of data?\n\n* Which client IPs are interacting with a high number of different URLs?\n\n* Which client IPs have high error rates?\n\n* Which client IPs are interacting with a high number of destination countries?\n\n\n[[example-last-log]]\n== Finding the last log event for each IP address\n\nThis example uses the web log sample data set to find the last log from an IP \naddress. Let's use the `latest` type of {transform} in continuous mode. It \ncopies the most recent document for each unique key from the source index to the \ndestination index and updates the destination index as new data comes into the \nsource index. \n\nPick the `clientip` field as the unique key; the data is grouped by this field. \nSelect `timestamp` as the date field that sorts the data chronologically. For \ncontinuous mode, specify a date field that is used to identify new documents, \nand an interval between checks for changes in the source index.\n\n[role=\"screenshot\"]\nimage::images/transform-ex4-1.jpg[\"Finding the last log event for each IP address with {transforms} in {kib}\"]\n\nLet's assume that we're interested in retaining documents only for IP addresses \nthat appeared recently in the log. You can define a retention policy and specify \na date field that is used to calculate the age of a document. This example uses \nthe same date field that is used to sort the data. Then set the maximum age of a \ndocument; documents that are older than the value you set will be removed from \nthe destination index.\n\n[role=\"screenshot\"]\nimage::images/transform-ex4-2.jpg[\"Defining retention policy for {transforms} in {kib}\"]\n\nThis {transform} creates the destination index that contains the latest login \ndate for each client IP. As the {transform} runs in continuous mode, the \ndestination index will be updated as new data that comes into the source index. \nFinally, every document that is older than 30 days will be removed from the \ndestination index due to the applied retention policy.\n\n\n.API example\n[%collapsible]\n====\n\n[source,console]\n----------------------------------\nPUT _transform/last-log-from-clientip\n{\n  \"source\": {\n    \"index\": [\n      \"kibana_sample_data_logs\"\n    ]\n  },\n  \"latest\": {\n    \"unique_key\": [ <1>\n      \"clientip\"\n    ],\n    \"sort\": \"timestamp\" <2>\n  },\n  \"frequency\": \"1m\", <3>\n  \"dest\": {\n    \"index\": \"last-log-from-clientip\"\n  },\n  \"sync\": { <4>\n    \"time\": {\n      \"field\": \"timestamp\",\n      \"delay\": \"60s\"\n    }\n  },\n  \"retention_policy\": { <5>\n    \"time\": {\n      \"field\": \"timestamp\",\n      \"max_age\": \"30d\"\n    }\n  },\n  \"settings\": {\n    \"max_page_search_size\": 500\n  }\n}\n\n----------------------------------\n// TEST[skip:setup kibana sample data]\n\n<1> Specifies the field for grouping the data.\n<2> Specifies the date field that is used for sorting the data.\n<3> Sets the interval for the {transform} to check for changes in the source \nindex.\n<4> Contains the time field and delay settings used to synchronize the source \nand destination indices.\n<5> Specifies the retention policy for the transform. Documents that are older \nthan the configured value will be removed from the destination index. \n\n\nAfter you create the {transform}, start it:\n\n[source,console]\n----------------------------------\nPOST _transform/last-log-from-clientip/_start\n----------------------------------\n// TEST[skip:setup kibana sample data]\n\n====\n\nAfter the {transform} processes the data, search the destination index:\n\n[source,console]\n----------------------------------\nGET last-log-from-clientip/_search\n----------------------------------\n// TEST[skip:setup kibana sample data]\n\n\nThe search result shows you data like this for each client IP:\n\n[source,js]\n----------------------------------\n{\n  \"_index\" : \"last-log-from-clientip\",\n  \"_id\" : \"MOeHH_cUL5urmartKj-b5UQAAAAAAAAA\",\n  \"_score\" : 1.0,\n  \"_source\" : {\n    \"referer\" : \"http://twitter.com/error/don-lind\",\n    \"request\" : \"/elasticsearch\",\n    \"agent\" : \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\",\n    \"extension\" : \"\",\n    \"memory\" : null,\n    \"ip\" : \"0.72.176.46\",\n    \"index\" : \"kibana_sample_data_logs\",\n    \"message\" : \"0.72.176.46 - - [2018-09-18T06:31:00.572Z] \\\"GET /elasticsearch HTTP/1.1\\\" 200 7065 \\\"-\\\" \\\"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\\\"\",\n    \"url\" : \"https://www.elastic.co/downloads/elasticsearch\",\n    \"tags\" : [\n      \"success\",\n      \"info\"\n    ],\n    \"geo\" : {\n      \"srcdest\" : \"IN:PH\",\n      \"src\" : \"IN\",\n      \"coordinates\" : {\n        \"lon\" : -124.1127917,\n        \"lat\" : 40.80338889\n      },\n      \"dest\" : \"PH\"\n    },\n    \"utc_time\" : \"2021-05-04T06:31:00.572Z\",\n    \"bytes\" : 7065,\n    \"machine\" : {\n      \"os\" : \"ios\",\n      \"ram\" : 12884901888\n    },\n    \"response\" : 200,\n    \"clientip\" : \"0.72.176.46\",\n    \"host\" : \"www.elastic.co\",\n    \"event\" : {\n      \"dataset\" : \"sample_web_logs\"\n    },\n    \"phpmemory\" : null,\n    \"timestamp\" : \"2021-05-04T06:31:00.572Z\"\n  }\n}\n----------------------------------\n// NOTCONSOLE\n\n\nThis {transform} makes it easier to answer questions such as:\n\n* What was the most recent log event associated with a specific IP address?\n\n\n[[example-bytes]]\n== Finding client IPs that sent the most bytes to the server\n\nThis example uses the web log sample data set to find the client IP that sent \nthe most bytes to the server in every hour. The example uses a `pivot` \n{transform} with a <<search-aggregations-metrics-top-metrics,`top_metrics`>> \naggregation.\n\nGroup the data by a <<_date_histogram,date histogram>> on the time field with an \ninterval of one hour. Use a \n<<search-aggregations-metrics-max-aggregation,max aggregation>> on the `bytes` \nfield to get the maximum amount of data that is sent to the server. Without \nthe `max` aggregation, the API call still returns the client IP that sent the \nmost bytes, however, the amount of bytes that it sent is not returned. In the \n`top_metrics` property, specify `clientip` and `geo.src`, then sort them by the \n`bytes` field in descending order. The {transform} returns the client IP that \nsent the biggest amount of data and the 2-letter ISO code of the corresponding \nlocation.\n\n[source,console]\n----------------------------------\nPOST _transform/_preview\n{\n  \"source\": {\n    \"index\": \"kibana_sample_data_logs\"\n  },\n  \"pivot\": {\n    \"group_by\": { <1>\n      \"timestamp\": {\n        \"date_histogram\": {\n          \"field\": \"timestamp\",\n          \"fixed_interval\": \"1h\"\n        }\n      }\n    },\n    \"aggregations\": {\n      \"bytes.max\": { <2>\n        \"max\": {\n          \"field\": \"bytes\"\n        }\n      },\n      \"top\": {\n        \"top_metrics\": { <3>\n          \"metrics\": [\n            {\n              \"field\": \"clientip\"\n            },\n            {\n              \"field\": \"geo.src\"\n            }\n          ],\n          \"sort\": {\n            \"bytes\": \"desc\"\n          }\n        }\n      }\n    }\n  }\n}\n----------------------------------\n// TEST[skip:setup kibana sample data]\n\n<1> The data is grouped by a date histogram of the time field with a one hour \ninterval.\n<2> Calculates the maximum value of the `bytes` field. \n<3> Specifies the fields (`clientip` and `geo.src`) of the top document to \nreturn and the sorting method (document with the highest `bytes` value).\n\nThe API call above returns a response similar to this:\n\n[source,js]\n----------------------------------\n{\n  \"preview\" : [\n    {\n      \"top\" : {\n        \"clientip\" : \"223.87.60.27\",\n        \"geo.src\" : \"IN\"\n      },\n      \"bytes\" : {\n        \"max\" : 6219\n      },\n      \"timestamp\" : \"2021-04-25T00:00:00.000Z\"\n    },\n    {\n      \"top\" : {\n        \"clientip\" : \"99.74.118.237\",\n        \"geo.src\" : \"LK\"\n      },\n      \"bytes\" : {\n        \"max\" : 14113\n      },\n      \"timestamp\" : \"2021-04-25T03:00:00.000Z\"\n    },\n    {\n      \"top\" : {\n        \"clientip\" : \"218.148.135.12\",\n        \"geo.src\" : \"BR\"\n      },\n      \"bytes\" : {\n        \"max\" : 4531\n      },\n      \"timestamp\" : \"2021-04-25T04:00:00.000Z\"\n    },\n    ...\n  ]\n}\n----------------------------------\n// NOTCONSOLE\n\n[[example-customer-names]]\n== Getting customer name and email address by customer ID\n\nThis example uses the ecommerce sample data set to create an entity-centric \nindex based on customer ID, and to get the customer name and email address by \nusing the `top_metrics` aggregation.\n\nGroup the data by `customer_id`, then add a `top_metrics` aggregation where the \n`metrics` are the `email`, the `customer_first_name.keyword`, and the \n`customer_last_name.keyword` fields. Sort the `top_metrics` by `order_date` in \ndescending order. The API call looks like this:\n\n[source,console]\n----------------------------------\nPOST _transform/_preview \n{\n  \"source\": {\n    \"index\": \"kibana_sample_data_ecommerce\"\n  },\n  \"pivot\": {\n    \"group_by\": { <1>\n      \"customer_id\": {\n        \"terms\": {\n          \"field\": \"customer_id\"\n        }\n      }\n    },\n    \"aggregations\": {\n      \"last\": {\n        \"top_metrics\": { <2>\n          \"metrics\": [\n            {\n              \"field\": \"email\"\n            },\n            {\n              \"field\": \"customer_first_name.keyword\"\n            },\n            {\n              \"field\": \"customer_last_name.keyword\"\n            }\n          ],\n          \"sort\": {\n            \"order_date\": \"desc\"\n          }\n        }\n      }\n    }\n  }\n}\n----------------------------------\n// TEST[skip:setup kibana sample data]\n\n<1> The data is grouped by a `terms` aggregation on the `customer_id` field.\n<2> Specifies the fields to return (email and name fields) in a descending order \nby the order date.\n\nThe API returns a response that is similar to this:\n\n[source,js]\n----------------------------------\n { \n  \"preview\" : [\n    {\n      \"last\" : {\n        \"customer_last_name.keyword\" : \"Long\",\n        \"customer_first_name.keyword\" : \"Recip\",\n        \"email\" : \"recip@long-family.zzz\"\n      },\n      \"customer_id\" : \"10\"\n    },\n    {\n      \"last\" : {\n        \"customer_last_name.keyword\" : \"Jackson\",\n        \"customer_first_name.keyword\" : \"Fitzgerald\",\n        \"email\" : \"fitzgerald@jackson-family.zzz\"\n      },\n      \"customer_id\" : \"11\"\n    },\n    {\n      \"last\" : {\n        \"customer_last_name.keyword\" : \"Cross\",\n        \"customer_first_name.keyword\" : \"Brigitte\",\n        \"email\" : \"brigitte@cross-family.zzz\"\n      },\n      \"customer_id\" : \"12\"\n    },\n    ...\n  ]\n}\n----------------------------------\n// NOTCONSOLE\n"
}