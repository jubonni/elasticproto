{
    "meta": {
        "timestamp": "2024-11-01T03:07:09.918274",
        "size": 9552,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/point-in-time-api.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "point-in-time-api",
        "version": "8.15"
    },
    "doc": "[[point-in-time-api]]\n=== Point in time API\n++++\n<titleabbrev>Point in time</titleabbrev>\n++++\n\nA search request by default executes against the most recent visible data of\nthe target indices, which is called point in time. Elasticsearch pit (point in time)\nis a lightweight view into the state of the data as it existed when initiated.\nIn some cases, it's preferred to perform multiple search requests using\nthe same point in time. For example, if <<indices-refresh,refreshes>> happen between\nsearch_after requests, then the results of those requests might not be consistent as\nchanges happening between searches are only visible to the more recent point in time.\n\n[[point-in-time-api-prereqs]]\n==== {api-prereq-title}\n\n* If the {es} {security-features} are enabled, you must have the `read`\n<<privileges-list-indices,index privilege>> for the target data stream, index,\nor alias.\n+\nTo search a <<point-in-time-api,point in time (PIT)>> for an alias, you\nmust have the `read` index privilege for the alias's data streams or indices.\n\n[[point-in-time-api-request-body]]\n==== {api-request-body-title}\n\n`index_filter`::\n(Optional,  <<query-dsl,query object>> Allows to filter indices if the provided\nquery rewrites to `match_none` on every shard.\n\n[[point-in-time-api-example]]\n==== {api-examples-title}\n\nA point in time must be opened explicitly before being used in search requests. The\nkeep_alive parameter tells Elasticsearch how long it should keep a point in time alive,\ne.g. `?keep_alive=5m`.\n\n[source,console]\n--------------------------------------------------\nPOST /my-index-000001/_pit?keep_alive=1m\n--------------------------------------------------\n// TEST[setup:my_index]\n\nThe result from the above request includes a `id`, which should\nbe passed to the `id` of the `pit` parameter of a search request.\n\n[source,console]\n--------------------------------------------------\nPOST /_search  <1>\n{\n    \"size\": 100,  <2>\n    \"query\": {\n        \"match\" : {\n            \"title\" : \"elasticsearch\"\n        }\n    },\n    \"pit\": {\n\t    \"id\":  \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\", <3>\n\t    \"keep_alive\": \"1m\"  <4>\n    }\n}\n--------------------------------------------------\n// TEST[catch:unavailable]\n\n<1> A search request with the `pit` parameter must not specify `index`, `routing`,\nor <<search-preference,`preference`>>\nas these parameters are copied from the point in time.\n<2> Just like regular searches, you can <<paginate-search-results,use `from` and\n`size` to page through search results>>, up to the first 10,000 hits. If you\nwant to retrieve more hits, use PIT with <<search-after,`search_after`>>.\n<3> The `id` parameter tells Elasticsearch to execute the request using contexts\nfrom this point in time.\n<4> The `keep_alive` parameter tells Elasticsearch how long it should extend\nthe time to live of the point in time.\n\nIMPORTANT: The open point in time request and each subsequent search request can\nreturn different `id`; thus always use the most recently received `id` for the\nnext search request.\n\nIn addition to the `keep_alive` parameter, the `allow_partial_search_results` parameter\ncan also be defined.\nThis parameter determines whether the <<point-in-time-api, point in time (PIT)>>\nshould tolerate unavailable shards or <<shard-failures, shard failures>> when\ninitially creating the PIT.\nIf set to true, the PIT will be created with the available shards, along with a\nreference to any missing ones.\nIf set to false, the operation will fail if any shard is unavailable.\nThe default value is false.\n\nThe PIT response includes a summary of the total number of shards, as well as the number\nof successful shards when creating the PIT.\n\n[source,console]\n--------------------------------------------------\nPOST /my-index-000001/_pit?keep_alive=1m&allow_partial_search_results=true\n--------------------------------------------------\n// TEST[setup:my_index]\n\n[source,js]\n--------------------------------------------------\n{\n  \"id\": \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA=\",\n  \"_shards\": {\n    \"total\": 10,\n    \"successful\": 10,\n    \"skipped\": 0,\n    \"failed\": 0\n  }\n}\n--------------------------------------------------\n// NOTCONSOLE\n\nWhen a PIT that contains shard failures is used in a search request, the missing are\nalways reported in the search response as a NoShardAvailableActionException exception.\nTo get rid of these exceptions, a new PIT needs to be created so that shards missing\nfrom the previous PIT can be handled, assuming they become available in the meantime.\n\n[[point-in-time-keep-alive]]\n==== Keeping point in time alive\nThe `keep_alive` parameter, which is passed to a open point in time request and\nsearch request, extends the time to live of the corresponding point in time.\nThe value (e.g. `1m`, see <<time-units>>) does not need to be long enough to\nprocess all data -- it just needs to be long enough for the next request.\n\nNormally, the background merge process optimizes the index by merging together\nsmaller segments to create new, bigger segments. Once the smaller segments are\nno longer needed they are deleted. However, open point-in-times prevent the\nold segments from being deleted since they are still in use.\n\nTIP: Keeping older segments alive means that more disk space and file handles\nare needed. Ensure that you have configured your nodes to have ample free file\nhandles. See <<file-descriptors>>.\n\nAdditionally, if a segment contains deleted or updated documents then the\npoint in time must keep track of whether each document in the segment was live at\nthe time of the initial search request. Ensure that your nodes have sufficient heap\nspace if you have many open point-in-times on an index that is subject to ongoing\ndeletes or updates. Note that a point-in-time doesn't prevent its associated indices\nfrom being deleted.\n\nYou can check how many point-in-times (i.e, search contexts) are open with the\n<<cluster-nodes-stats,nodes stats API>>:\n\n[source,console]\n---------------------------------------\nGET /_nodes/stats/indices/search\n---------------------------------------\n\n[[close-point-in-time-api]]\n==== Close point in time API\n\nPoint-in-time is automatically closed when its `keep_alive` has\nbeen elapsed. However keeping point-in-times has a cost, as discussed in the\n<<point-in-time-keep-alive,previous section>>. Point-in-times should be closed\nas soon as they are no longer used in search requests.\n\n[source,console]\n---------------------------------------\nDELETE /_pit\n{\n    \"id\" : \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\"\n}\n---------------------------------------\n// TEST[catch:missing]\n\nThe API returns the following response:\n\n[source,console-result]\n--------------------------------------------------\n{\n   \"succeeded\": true, <1>\n   \"num_freed\": 3     <2>\n}\n--------------------------------------------------\n// TESTRESPONSE[s/\"succeeded\": true/\"succeeded\": $body.succeeded/]\n// TESTRESPONSE[s/\"num_freed\": 3/\"num_freed\": $body.num_freed/]\n\n<1> If true, all search contexts associated with the point-in-time id are successfully closed\n<2> The number of search contexts have been successfully closed\n\n[discrete]\n[[search-slicing]]\n=== Search slicing\n\nWhen paging through a large number of documents, it can be helpful to split the search into multiple slices\nto consume them independently:\n\n[source,console]\n--------------------------------------------------\nGET /_search\n{\n  \"slice\": {\n    \"id\": 0,                      <1>\n    \"max\": 2                      <2>\n  },\n  \"query\": {\n    \"match\": {\n      \"message\": \"foo\"\n    }\n  },\n  \"pit\": {\n    \"id\": \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\"\n  }\n}\n\nGET /_search\n{\n  \"slice\": {\n    \"id\": 1,\n    \"max\": 2\n  },\n  \"pit\": {\n    \"id\": \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\"\n  },\n  \"query\": {\n    \"match\": {\n      \"message\": \"foo\"\n    }\n  }\n}\n--------------------------------------------------\n// TEST[skip:both calls will throw errors]\n\n<1> The id of the slice\n<2> The maximum number of slices\n\nThe result from the first request returns documents belonging to the first slice (id: 0) and the\nresult from the second request returns documents in the second slice. Since the maximum number of\nslices is set to 2 the union of the results of the two requests is equivalent to the results of a\npoint-in-time search without slicing. By default the splitting is done first on the shards, then\nlocally on each shard. The local splitting partitions the shard into contiguous ranges based on\nLucene document IDs.\n\nFor instance if the number of shards is equal to 2 and the user requested 4 slices then the slices\n0 and 2 are assigned to the first shard and the slices 1 and 3 are assigned to the second shard.\n\nIMPORTANT: The same point-in-time ID should be used for all slices. If different PIT IDs are used,\nthen slices can overlap and miss documents. This is because the splitting criterion is based on\nLucene document IDs, which are not stable across changes to the index.\n"
}