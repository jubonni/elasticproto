{
    "meta": {
        "timestamp": "2024-11-01T02:49:26.866082",
        "size": 15696,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-word-delimiter-graph-tokenfilter.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "analysis-word-delimiter-graph-tokenfilter",
        "version": "8.15"
    },
    "doc": "[[analysis-word-delimiter-graph-tokenfilter]]\n=== Word delimiter graph token filter\n++++\n<titleabbrev>Word delimiter graph</titleabbrev>\n++++\n\nSplits tokens at non-alphanumeric characters. The `word_delimiter_graph` filter\nalso performs optional token normalization based on a set of rules. By default,\nthe filter uses the following rules:\n\n* Split tokens at non-alphanumeric characters.\n  The filter uses these characters as delimiters.\n  For example: `Super-Duper` -> `Super`, `Duper`\n* Remove leading or trailing delimiters from each token.\n  For example: `XL---42+'Autocoder'` -> `XL`, `42`, `Autocoder`\n* Split tokens at letter case transitions.\n  For example: `PowerShot` -> `Power`, `Shot`\n* Split tokens at letter-number transitions.\n  For example: `XL500` -> `XL`, `500`\n* Remove the English possessive (`'s`) from the end of each token.\n  For example: `Neil's` -> `Neil`\n\nThe `word_delimiter_graph` filter uses Lucene's\n{lucene-analysis-docs}/miscellaneous/WordDelimiterGraphFilter.html[WordDelimiterGraphFilter].\n\n[TIP]\n====\nThe `word_delimiter_graph` filter was designed to remove punctuation from\ncomplex identifiers, such as product IDs or part numbers. For these use cases,\nwe recommend using the `word_delimiter_graph` filter with the\n<<analysis-keyword-tokenizer,`keyword`>> tokenizer.\n\nAvoid using the `word_delimiter_graph` filter to split hyphenated words, such as\n`wi-fi`. Because users often search for these words both with and without\nhyphens, we recommend using the\n<<analysis-synonym-graph-tokenfilter,`synonym_graph`>> filter instead.\n====\n\n[[analysis-word-delimiter-graph-tokenfilter-analyze-ex]]\n==== Example\n\nThe following <<indices-analyze,analyze API>> request uses the\n`word_delimiter_graph` filter to split `Neil's-Super-Duper-XL500--42+AutoCoder`\ninto normalized tokens using the filter's default rules:\n\n[source,console]\n----\nGET /_analyze\n{\n  \"tokenizer\": \"keyword\",\n  \"filter\": [ \"word_delimiter_graph\" ],\n  \"text\": \"Neil's-Super-Duper-XL500--42+AutoCoder\"\n}\n----\n\nThe filter produces the following tokens:\n\n[source,txt]\n----\n[ Neil, Super, Duper, XL, 500, 42, Auto, Coder ]\n----\n\n////\n[source,console-result]\n----\n{\n  \"tokens\": [\n    {\n      \"token\": \"Neil\",\n      \"start_offset\": 0,\n      \"end_offset\": 4,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"Super\",\n      \"start_offset\": 7,\n      \"end_offset\": 12,\n      \"type\": \"word\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"Duper\",\n      \"start_offset\": 13,\n      \"end_offset\": 18,\n      \"type\": \"word\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"XL\",\n      \"start_offset\": 19,\n      \"end_offset\": 21,\n      \"type\": \"word\",\n      \"position\": 3\n    },\n    {\n      \"token\": \"500\",\n      \"start_offset\": 21,\n      \"end_offset\": 24,\n      \"type\": \"word\",\n      \"position\": 4\n    },\n    {\n      \"token\": \"42\",\n      \"start_offset\": 26,\n      \"end_offset\": 28,\n      \"type\": \"word\",\n      \"position\": 5\n    },\n    {\n      \"token\": \"Auto\",\n      \"start_offset\": 29,\n      \"end_offset\": 33,\n      \"type\": \"word\",\n      \"position\": 6\n    },\n    {\n      \"token\": \"Coder\",\n      \"start_offset\": 33,\n      \"end_offset\": 38,\n      \"type\": \"word\",\n      \"position\": 7\n    }\n  ]\n}\n----\n////\n\n[[analysis-word-delimiter-graph-tokenfilter-analyzer-ex]]\n==== Add to an analyzer\n\nThe following <<indices-create-index,create index API>> request uses the\n`word_delimiter_graph` filter to configure a new\n<<analysis-custom-analyzer,custom analyzer>>.\n\n[source,console]\n----\nPUT /my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"keyword\",\n          \"filter\": [ \"word_delimiter_graph\" ]\n        }\n      }\n    }\n  }\n}\n----\n\n[WARNING]\n====\nAvoid using the `word_delimiter_graph` filter with tokenizers that remove\npunctuation, such as the <<analysis-standard-tokenizer,`standard`>> tokenizer.\nThis could prevent the `word_delimiter_graph` filter from splitting tokens\ncorrectly. It can also interfere with the filter's configurable parameters, such\nas <<word-delimiter-graph-tokenfilter-catenate-all,`catenate_all`>> or\n<<word-delimiter-graph-tokenfilter-preserve-original,`preserve_original`>>. We\nrecommend using the <<analysis-keyword-tokenizer,`keyword`>> or\n<<analysis-whitespace-tokenizer,`whitespace`>> tokenizer instead.\n====\n\n[[word-delimiter-graph-tokenfilter-configure-parms]]\n==== Configurable parameters\n\n[[word-delimiter-graph-tokenfilter-adjust-offsets]]\n`adjust_offsets`::\n+\n--\n(Optional, Boolean)\nIf `true`, the filter adjusts the offsets of split or catenated tokens to better\nreflect their actual position in the token stream. Defaults to `true`.\n\n[WARNING]\n====\nSet `adjust_offsets` to `false` if your analyzer uses filters, such as the\n<<analysis-trim-tokenfilter,`trim`>> filter, that change the length of tokens\nwithout changing their offsets. Otherwise, the `word_delimiter_graph` filter\ncould produce tokens with illegal offsets.\n====\n--\n\n[[word-delimiter-graph-tokenfilter-catenate-all]]\n`catenate_all`::\n+\n--\n(Optional, Boolean)\nIf `true`, the filter produces catenated tokens for chains of alphanumeric\ncharacters separated by non-alphabetic delimiters. For example:\n`super-duper-xl-500` -> [ **`superduperxl500`**, `super`, `duper`, `xl`, `500` ].\nDefaults to `false`.\n\n[WARNING]\n====\nSetting this parameter to `true` produces multi-position tokens, which are not\nsupported by indexing.\n\nIf this parameter is `true`, avoid using this filter in an index analyzer or\nuse the <<analysis-flatten-graph-tokenfilter,`flatten_graph`>> filter after\nthis filter to make the token stream suitable for indexing.\n\nWhen used for search analysis, catenated tokens can cause problems for the\n<<query-dsl-match-query-phrase,`match_phrase`>> query and other queries that\nrely on token position for matching. Avoid setting this parameter to `true` if\nyou plan to use these queries.\n====\n--\n\n[[word-delimiter-graph-tokenfilter-catenate-numbers]]\n`catenate_numbers`::\n+\n--\n(Optional, Boolean)\nIf `true`, the filter produces catenated tokens for chains of numeric characters\nseparated by non-alphabetic delimiters. For example: `01-02-03` ->\n[ **`010203`**, `01`, `02`, `03` ]. Defaults to `false`.\n\n[WARNING]\n====\nSetting this parameter to `true` produces multi-position tokens, which are not\nsupported by indexing.\n\nIf this parameter is `true`, avoid using this filter in an index analyzer or\nuse the <<analysis-flatten-graph-tokenfilter,`flatten_graph`>> filter after\nthis filter to make the token stream suitable for indexing.\n\nWhen used for search analysis, catenated tokens can cause problems for the\n<<query-dsl-match-query-phrase,`match_phrase`>> query and other queries that\nrely on token position for matching. Avoid setting this parameter to `true` if\nyou plan to use these queries.\n====\n--\n\n[[word-delimiter-graph-tokenfilter-catenate-words]]\n`catenate_words`::\n+\n--\n(Optional, Boolean)\nIf `true`, the filter produces catenated tokens for chains of alphabetical\ncharacters separated by non-alphabetic delimiters. For example: `super-duper-xl`\n-> [ **`superduperxl`**, `super`, `duper`, `xl` ]. Defaults to `false`.\n\n[WARNING]\n====\nSetting this parameter to `true` produces multi-position tokens, which are not\nsupported by indexing.\n\nIf this parameter is `true`, avoid using this filter in an index analyzer or\nuse the <<analysis-flatten-graph-tokenfilter,`flatten_graph`>> filter after\nthis filter to make the token stream suitable for indexing.\n\nWhen used for search analysis, catenated tokens can cause problems for the\n<<query-dsl-match-query-phrase,`match_phrase`>> query and other queries that\nrely on token position for matching. Avoid setting this parameter to `true` if\nyou plan to use these queries.\n====\n--\n\n`generate_number_parts`::\n(Optional, Boolean)\nIf `true`, the filter includes tokens consisting of only numeric characters in\nthe output. If `false`, the filter excludes these tokens from the output.\nDefaults to `true`.\n\n`generate_word_parts`::\n(Optional, Boolean)\nIf `true`, the filter includes tokens consisting of only alphabetical characters\nin the output. If `false`, the filter excludes these tokens from the output.\nDefaults to `true`.\n\n`ignore_keywords`::\n(Optional, Boolean)\nIf `true`, the filter skips tokens with\na `keyword` attribute of `true`.\nDefaults to `false`.\n\n[[word-delimiter-graph-tokenfilter-preserve-original]]\n`preserve_original`::\n+\n--\n(Optional, Boolean)\nIf `true`, the filter includes the original version of any split tokens in the\noutput. This original version includes non-alphanumeric delimiters. For example:\n`super-duper-xl-500` -> [ **`super-duper-xl-500`**, `super`, `duper`, `xl`,\n`500` ]. Defaults to `false`.\n\n[WARNING]\n====\nSetting this parameter to `true` produces multi-position tokens, which are not\nsupported by indexing.\n\nIf this parameter is `true`, avoid using this filter in an index analyzer or\nuse the <<analysis-flatten-graph-tokenfilter,`flatten_graph`>> filter after\nthis filter to make the token stream suitable for indexing.\n====\n--\n\n`protected_words`::\n(Optional, array of strings)\nArray of tokens the filter won't split.\n\n`protected_words_path`::\n+\n--\n(Optional, string)\nPath to a file that contains a list of tokens the filter won't split.\n\nThis path must be absolute or relative to the `config` location, and the file\nmust be UTF-8 encoded. Each token in the file must be separated by a line\nbreak.\n--\n\n`split_on_case_change`::\n(Optional, Boolean)\nIf `true`, the filter splits tokens at letter case transitions. For example:\n`camelCase` -> [ `camel`, `Case` ]. Defaults to `true`.\n\n`split_on_numerics`::\n(Optional, Boolean)\nIf `true`, the filter splits tokens at letter-number transitions. For example:\n`j2se` -> [ `j`, `2`, `se` ]. Defaults to `true`.\n\n`stem_english_possessive`::\n(Optional, Boolean)\nIf `true`, the filter removes the English possessive (`'s`) from the end of each\ntoken. For example: `O'Neil's` -> [ `O`, `Neil` ]. Defaults to `true`.\n\n`type_table`::\n+\n--\n(Optional, array of strings)\nArray of custom type mappings for characters. This allows you to map\nnon-alphanumeric characters as numeric or alphanumeric to avoid splitting on\nthose characters.\n\nFor example, the following array maps the plus (`+`) and hyphen (`-`) characters\nas alphanumeric, which means they won't be treated as delimiters:\n\n`[ \"+ => ALPHA\", \"- => ALPHA\" ]`\n\nSupported types include:\n\n* `ALPHA` (Alphabetical)\n* `ALPHANUM` (Alphanumeric)\n* `DIGIT` (Numeric)\n* `LOWER` (Lowercase alphabetical)\n* `SUBWORD_DELIM` (Non-alphanumeric delimiter)\n* `UPPER` (Uppercase alphabetical)\n--\n\n`type_table_path`::\n+\n--\n(Optional, string)\nPath to a file that contains custom type mappings for characters. This allows\nyou to map non-alphanumeric characters as numeric or alphanumeric to avoid\nsplitting on those characters.\n\nFor example, the contents of this file may contain the following:\n\n[source,txt]\n----\n# Map the $, %, '.', and ',' characters to DIGIT\n# This might be useful for financial data.\n$ => DIGIT\n% => DIGIT\n. => DIGIT\n\\\\u002C => DIGIT\n\n# in some cases you might not want to split on ZWJ\n# this also tests the case where we need a bigger byte[]\n# see https://en.wikipedia.org/wiki/Zero-width_joiner\n\\\\u200D => ALPHANUM\n----\n\nSupported types include:\n\n* `ALPHA` (Alphabetical)\n* `ALPHANUM` (Alphanumeric)\n* `DIGIT` (Numeric)\n* `LOWER` (Lowercase alphabetical)\n* `SUBWORD_DELIM` (Non-alphanumeric delimiter)\n* `UPPER` (Uppercase alphabetical)\n\nThis file path must be absolute or relative to the `config` location, and the\nfile must be UTF-8 encoded. Each mapping in the file must be separated by a line\nbreak.\n--\n\n[[analysis-word-delimiter-graph-tokenfilter-customize]]\n==== Customize\n\nTo customize the `word_delimiter_graph` filter, duplicate it to create the basis\nfor a new custom token filter. You can modify the filter using its configurable\nparameters.\n\nFor example, the following request creates a `word_delimiter_graph`\nfilter that uses the following rules:\n\n* Split tokens at non-alphanumeric characters, _except_ the hyphen (`-`)\n  character.\n* Remove leading or trailing delimiters from each token.\n* Do _not_ split tokens at letter case transitions.\n* Do _not_ split tokens at letter-number transitions.\n* Remove the English possessive (`'s`) from the end of each token.\n\n[source,console]\n----\nPUT /my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"keyword\",\n          \"filter\": [ \"my_custom_word_delimiter_graph_filter\" ]\n        }\n      },\n      \"filter\": {\n        \"my_custom_word_delimiter_graph_filter\": {\n          \"type\": \"word_delimiter_graph\",\n          \"type_table\": [ \"- => ALPHA\" ],\n          \"split_on_case_change\": false,\n          \"split_on_numerics\": false,\n          \"stem_english_possessive\": true\n        }\n      }\n    }\n  }\n}\n----\n\n[[analysis-word-delimiter-graph-differences]]\n==== Differences between `word_delimiter_graph` and `word_delimiter` \n\nBoth the `word_delimiter_graph` and\n<<analysis-word-delimiter-tokenfilter,`word_delimiter`>> filters produce tokens\nthat span multiple positions when any of the following parameters are `true`:\n\n * <<word-delimiter-graph-tokenfilter-catenate-all,`catenate_all`>>\n * <<word-delimiter-graph-tokenfilter-catenate-numbers,`catenate_numbers`>>\n * <<word-delimiter-graph-tokenfilter-catenate-words,`catenate_words`>>\n * <<word-delimiter-graph-tokenfilter-preserve-original,`preserve_original`>>\n\nHowever, only the `word_delimiter_graph` filter assigns multi-position tokens a\n`positionLength` attribute, which indicates the number of positions a token\nspans. This ensures the `word_delimiter_graph` filter always produces valid\n<<token-graphs,token graphs>>.\n\nThe `word_delimiter` filter does not assign multi-position tokens a\n`positionLength` attribute. This means it produces invalid graphs for streams\nincluding these tokens.\n\nWhile indexing does not support token graphs containing multi-position tokens,\nqueries, such as the <<query-dsl-match-query-phrase,`match_phrase`>> query, can\nuse these graphs to generate multiple sub-queries from a single query string.\n\nTo see how token graphs produced by the `word_delimiter` and\n`word_delimiter_graph` filters differ, check out the following example.\n\n.*Example*\n[%collapsible]\n====\n\n[[analysis-word-delimiter-graph-basic-token-graph]]\n*Basic token graph*\n\nBoth the `word_delimiter` and `word_delimiter_graph` produce the following token\ngraph for `PowerShot2000` when the following parameters are `false`:\n\n * <<word-delimiter-graph-tokenfilter-catenate-all,`catenate_all`>>\n * <<word-delimiter-graph-tokenfilter-catenate-numbers,`catenate_numbers`>>\n * <<word-delimiter-graph-tokenfilter-catenate-words,`catenate_words`>>\n * <<word-delimiter-graph-tokenfilter-preserve-original,`preserve_original`>>\n\nThis graph does not contain multi-position tokens. All tokens span only one\nposition.\n\nimage::images/analysis/token-graph-basic.svg[align=\"center\"]\n\n[[analysis-word-delimiter-graph-wdg-token-graph]]\n*`word_delimiter_graph` graph with a multi-position token*\n\nThe `word_delimiter_graph` filter produces the following token graph for\n`PowerShot2000` when `catenate_words` is `true`.\n\nThis graph correctly indicates the catenated `PowerShot` token spans two\npositions.\n\nimage::images/analysis/token-graph-wdg.svg[align=\"center\"]\n\n[[analysis-word-delimiter-graph-wd-token-graph]]\n*`word_delimiter` graph with a multi-position token*\n\nWhen `catenate_words` is `true`, the `word_delimiter` filter produces\nthe following token graph for `PowerShot2000`.\n\nNote that the catenated `PowerShot` token should span two positions but only\nspans one in the token graph, making it invalid.\n\nimage::images/analysis/token-graph-wd.svg[align=\"center\"]\n\n====\n"
}