{
    "meta": {
        "timestamp": "2024-11-01T02:49:26.372066",
        "size": 6360,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/semantic-search-inference.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "semantic-search-inference",
        "version": "8.15"
    },
    "doc": "[[semantic-search-inference]]\n=== Tutorial: semantic search with the {infer} API\n\n++++\n<titleabbrev>Semantic search with the {infer} API</titleabbrev>\n++++\n\nThe instructions in this tutorial shows you how to use the {infer} API workflow with various services to perform semantic search on your data.\n\nIMPORTANT: For the easiest way to perform semantic search in the {stack}, refer to the <<semantic-search-semantic-text, `semantic_text`>> end-to-end tutorial.\n\nThe following examples use the:\n\n* `embed-english-v3.0` model for https://docs.cohere.com/docs/cohere-embed[Cohere]\n* `all-mpnet-base-v2` model from https://huggingface.co/sentence-transformers/all-mpnet-base-v2[HuggingFace]\n* `text-embedding-ada-002` second generation embedding model for OpenAI\n* models available through https://ai.azure.com/explore/models?selectedTask=embeddings[Azure AI Studio] or https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models[Azure OpenAI]\n* `text-embedding-004` model for https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api[Google Vertex AI]\n* `mistral-embed` model for https://docs.mistral.ai/getting-started/models/[Mistral]\n* `amazon.titan-embed-text-v1` model for https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html[Amazon Bedrock]\n* `ops-text-embedding-zh-001` model for https://help.aliyun.com/zh/open-search/search-platform/developer-reference/text-embedding-api-details[AlibabaCloud AI]\n\nYou can use any Cohere and OpenAI models, they are all supported by the {infer} API.\nFor a list of recommended models available on HuggingFace, refer to <<inference-example-hugging-face-supported-models, the supported model list>>.\n\nClick the name of the service you want to use on any of the widgets below to review the corresponding instructions.\n\n[discrete]\n[[infer-service-requirements]]\n==== Requirements\n\ninclude::{es-ref-dir}/tab-widgets/inference-api/infer-api-requirements-widget.asciidoc[]\n\n[discrete]\n[[infer-text-embedding-task]]\n==== Create an inference endpoint\n\nCreate an {infer} endpoint by using the <<put-inference-api>>:\n\ninclude::{es-ref-dir}/tab-widgets/inference-api/infer-api-task-widget.asciidoc[]\n\n\n[discrete]\n[[infer-service-mappings]]\n==== Create the index mapping\n\nThe mapping of the destination index - the index that contains the embeddings that the model will create based on your input text - must be created.\nThe destination index must have a field with the <<dense-vector, `dense_vector`>> field type for most models and the <<sparse-vector, `sparse_vector`>> field type for the sparse vector models like in the case of the `elser` service to index the output of the used model.\n\ninclude::{es-ref-dir}/tab-widgets/inference-api/infer-api-mapping-widget.asciidoc[]\n\n[discrete]\n[[infer-service-inference-ingest-pipeline]]\n==== Create an ingest pipeline with an inference processor\n\nCreate an <<ingest,ingest pipeline>> with an <<inference-processor,{infer} processor>> and use the model you created above to infer against the data that is being ingested in the pipeline.\n\ninclude::{es-ref-dir}/tab-widgets/inference-api/infer-api-ingest-pipeline-widget.asciidoc[]\n\n[discrete]\n[[infer-load-data]]\n==== Load data\n\nIn this step, you load the data that you later use in the {infer} ingest pipeline to create embeddings from it.\n\nUse the `msmarco-passagetest2019-top1000` data set, which is a subset of the MS MARCO Passage Ranking data set.\nIt consists of 200 queries, each accompanied by a list of relevant text passages.\nAll unique passages, along with their IDs, have been extracted from that data set and compiled into a\nhttps://github.com/elastic/stack-docs/blob/main/docs/en/stack/ml/nlp/data/msmarco-passagetest2019-unique.tsv[tsv file].\n\nDownload the file and upload it to your cluster using the {kibana-ref}/connect-to-elasticsearch.html#upload-data-kibana[Data Visualizer] in the {ml-app} UI.\nAfter your data is analyzed, click **Override settings**.\nUnder **Edit field names**, assign `id` to the first column and `content` to the second.\nClick **Apply**, then **Import**.\nName the index `test-data`, and click **Import**.\nAfter the upload is complete, you will see an index named `test-data` with 182,469 documents.\n\n[discrete]\n[[reindexing-data-infer]]\n==== Ingest the data through the {infer} ingest pipeline\n\nCreate embeddings from the text by reindexing the data through the {infer} pipeline that uses your chosen model.\nThis step uses the {ref}/docs-reindex.html[reindex API] to simulate data ingestion through a pipeline.\n\ninclude::{es-ref-dir}/tab-widgets/inference-api/infer-api-reindex-widget.asciidoc[]\n\nThe call returns a task ID to monitor the progress:\n\n[source,console]\n----\nGET _tasks/<task_id>\n----\n// TEST[skip:TBD]\n\nReindexing large datasets can take a long time.\nYou can test this workflow using only a subset of the dataset.\nDo this by cancelling the reindexing process, and only generating embeddings for the subset that was reindexed.\nThe following API request will cancel the reindexing task:\n\n[source,console]\n----\nPOST _tasks/<task_id>/_cancel\n----\n// TEST[skip:TBD]\n\n\n[discrete]\n[[infer-semantic-search]]\n==== Semantic search\n\nAfter the data set has been enriched with the embeddings, you can query the data using {ref}/knn-search.html#knn-semantic-search[semantic search].\nIn case of dense vector models, pass a `query_vector_builder` to the k-nearest neighbor (kNN) vector search API, and provide the query text and the model you have used to create the embeddings.\nIn case of a sparse vector model like ELSER, use a `sparse_vector` query, and provide the query text with the model you have used to create the embeddings.\n\nNOTE: If you cancelled the reindexing process, you run the query only a part of the data which affects the quality of your results.\n\ninclude::{es-ref-dir}/tab-widgets/inference-api/infer-api-search-widget.asciidoc[]\n\n[discrete]\n[[infer-interactive-tutorials]]\n==== Interactive tutorials\n\nYou can also find tutorials in an interactive Colab notebook format using the\n{es} Python client:\n\n* https://colab.research.google.com/github/elastic/elasticsearch-labs/blob/main/notebooks/integrations/cohere/inference-cohere.ipynb[Cohere {infer} tutorial notebook]\n* https://colab.research.google.com/github/elastic/elasticsearch-labs/blob/main/notebooks/search/07-inference.ipynb[OpenAI {infer} tutorial notebook]\n"
}