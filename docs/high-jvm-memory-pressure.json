{
    "meta": {
        "size": 3593,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/high-jvm-memory-pressure.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "high-jvm-memory-pressure",
        "version": "8.15"
    },
    "doc": "[[high-jvm-memory-pressure]]\n=== High JVM memory pressure\n\nHigh JVM memory usage can degrade cluster performance and trigger\n<<circuit-breaker-errors,circuit breaker errors>>. To prevent this, we recommend\ntaking steps to reduce memory pressure if a node's JVM memory usage consistently\nexceeds 85%.\n\n[discrete]\n[[diagnose-high-jvm-memory-pressure]]\n==== Diagnose high JVM memory pressure\n\n**Check JVM memory pressure**\n\ninclude::{es-ref-dir}/tab-widgets/jvm-memory-pressure-widget.asciidoc[]\n\n**Check garbage collection logs**\n\nAs memory usage increases, garbage collection becomes more frequent and takes\nlonger. You can track the frequency and length of garbage collection events in\n<<logging,`elasticsearch.log`>>. For example, the following event states {es}\nspent more than 50% (21 seconds) of the last 40 seconds performing garbage\ncollection.\n\n[source,log]\n----\n[timestamp_short_interval_from_last][INFO ][o.e.m.j.JvmGcMonitorService] [node_id] [gc][number] overhead, spent [21s] collecting in the last [40s]\n----\n\n**Capture a JVM heap dump**\n\nTo determine the exact reason for the high JVM memory pressure, capture a heap\ndump of the JVM while its memory usage is high, and also capture the\n<<gc-logging,garbage collector logs>> covering the same time period.\n\n[discrete]\n[[reduce-jvm-memory-pressure]]\n==== Reduce JVM memory pressure\n\nThis section contains some common suggestions for reducing JVM memory pressure.\n\n**Reduce your shard count**\n\nEvery shard uses memory. In most cases, a small set of large shards uses fewer\nresources than many small shards. For tips on reducing your shard count, see\n<<size-your-shards>>.\n\n[[avoid-expensive-searches]]\n**Avoid expensive searches**\n\nExpensive searches can use large amounts of memory. To better track expensive\nsearches on your cluster, enable <<index-modules-slowlog,slow logs>>.\n\nExpensive searches may have a large <<paginate-search-results,`size` argument>>,\nuse aggregations with a large number of buckets, or include\n<<query-dsl-allow-expensive-queries,expensive queries>>. To prevent expensive\nsearches, consider the following setting changes:\n\n* Lower the `size` limit using the\n<<index-max-result-window,`index.max_result_window`>> index setting.\n\n* Decrease the maximum number of allowed aggregation buckets using the\n<<search-settings-max-buckets,search.max_buckets>> cluster setting.\n\n* Disable expensive queries using the\n<<query-dsl-allow-expensive-queries,`search.allow_expensive_queries`>> cluster\nsetting.\n\n* Set a default search timeout using the <<search-timeout,`search.default_search_timeout`>> cluster setting. \n\n[source,console]\n----\nPUT _settings\n{\n  \"index.max_result_window\": 5000\n}\n\nPUT _cluster/settings\n{\n  \"persistent\": {\n    \"search.max_buckets\": 20000,\n    \"search.allow_expensive_queries\": false\n  }\n}\n----\n// TEST[s/^/PUT my-index\\n/]\n\n**Prevent mapping explosions**\n\nDefining too many fields or nesting fields too deeply can lead to\n<<mapping-limit-settings,mapping explosions>> that use large amounts of memory.\nTo prevent mapping explosions, use the <<mapping-settings-limit,mapping limit\nsettings>> to limit the number of field mappings.\n\n**Spread out bulk requests**\n\nWhile more efficient than individual requests, large <<docs-bulk,bulk indexing>>\nor <<search-multi-search,multi-search>> requests can still create high JVM\nmemory pressure. If possible, submit smaller requests and allow more time\nbetween them.\n\n**Upgrade node memory**\n\nHeavy indexing and search loads can cause high JVM memory pressure. To better\nhandle heavy workloads, upgrade your nodes to increase their memory capacity.\n"
}