{
    "meta": {
        "timestamp": "2024-11-01T03:07:09.900275",
        "size": 13413,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-percentile-aggregation.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "search-aggregations-metrics-percentile-aggregation",
        "version": "8.15"
    },
    "doc": "[[search-aggregations-metrics-percentile-aggregation]]\n=== Percentiles aggregation\n++++\n<titleabbrev>Percentiles</titleabbrev>\n++++\n\nA `multi-value` metrics aggregation that calculates one or more percentiles\nover numeric values extracted from the aggregated documents. These values can be\nextracted from specific numeric or <<histogram,histogram fields>> in the documents.\n\nPercentiles show the point at which a certain percentage of observed values\noccur. For example, the 95th percentile is the value which is greater than 95%\nof the observed values.\n\nPercentiles are often used to find outliers. In normal distributions, the\n0.13th and 99.87th percentiles represents three standard deviations from the\nmean. Any data which falls outside three standard deviations is often considered\nan anomaly.\n\nWhen a range of percentiles are retrieved, they can be used to estimate the\ndata distribution and determine if the data is skewed, bimodal, etc.\n\nAssume your data consists of website load times. The average and median\nload times are not overly useful to an administrator. The max may be interesting,\nbut it can be easily skewed by a single slow response.\n\nLet's look at a range of percentiles representing load time:\n\n[source,console]\n--------------------------------------------------\nGET latency/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"load_time_outlier\": {\n      \"percentiles\": {\n        \"field\": \"load_time\" <1>\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:latency]\n<1> The field `load_time` must be a numeric field\n\nBy default, the `percentile` metric will generate a range of\npercentiles: `[ 1, 5, 25, 50, 75, 95, 99 ]`. The response will look like this:\n\n[source,console-result]\n--------------------------------------------------\n{\n  ...\n\n \"aggregations\": {\n    \"load_time_outlier\": {\n      \"values\": {\n        \"1.0\": 10.0,\n        \"5.0\": 30.0,\n        \"25.0\": 170.0,\n        \"50.0\": 445.0,\n        \"75.0\": 720.0,\n        \"95.0\": 940.0,\n        \"99.0\": 980.0\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TESTRESPONSE[s/\\.\\.\\./\"took\": $body.took,\"timed_out\": false,\"_shards\": $body._shards,\"hits\": $body.hits,/]\n// TESTRESPONSE[s/\"1.0\": 10.0/\"1.0\": 9.9/]\n// TESTRESPONSE[s/\"5.0\": 30.0/\"5.0\": 29.5/]\n// TESTRESPONSE[s/\"25.0\": 170.0/\"25.0\": 167.5/]\n// TESTRESPONSE[s/\"50.0\": 445.0/\"50.0\": 445.0/]\n// TESTRESPONSE[s/\"75.0\": 720.0/\"75.0\": 722.5/]\n// TESTRESPONSE[s/\"95.0\": 940.0/\"95.0\": 940.5/]\n// TESTRESPONSE[s/\"99.0\": 980.0/\"99.0\": 980.1/]\n\nAs you can see, the aggregation will return a calculated value for each percentile\nin the default range. If we assume response times are in milliseconds, it is\nimmediately obvious that the webpage normally loads in 10-725ms, but occasionally\nspikes to 945-985ms.\n\nOften, administrators are only interested in outliers -- the extreme percentiles.\nWe can specify just the percents we are interested in (requested percentiles\nmust be a value between 0-100 inclusive):\n\n[source,console]\n--------------------------------------------------\nGET latency/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"load_time_outlier\": {\n      \"percentiles\": {\n        \"field\": \"load_time\",\n        \"percents\": [ 95, 99, 99.9 ] <1>\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:latency]\n<1> Use the `percents` parameter to specify particular percentiles to calculate\n\n==== Keyed Response\n\nBy default the `keyed` flag is set to `true` which associates a unique string key with each bucket and returns the ranges as a hash rather than an array. Setting the `keyed` flag to `false` will disable this behavior:\n\n[source,console]\n--------------------------------------------------\nGET latency/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"load_time_outlier\": {\n      \"percentiles\": {\n        \"field\": \"load_time\",\n        \"keyed\": false\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:latency]\n\nResponse:\n\n[source,console-result]\n--------------------------------------------------\n{\n  ...\n\n  \"aggregations\": {\n    \"load_time_outlier\": {\n      \"values\": [\n        {\n          \"key\": 1.0,\n          \"value\": 10.0\n        },\n        {\n          \"key\": 5.0,\n          \"value\": 30.0\n        },\n        {\n          \"key\": 25.0,\n          \"value\": 170.0\n        },\n        {\n          \"key\": 50.0,\n          \"value\": 445.0\n        },\n        {\n          \"key\": 75.0,\n          \"value\": 720.0\n        },\n        {\n          \"key\": 95.0,\n          \"value\": 940.0\n        },\n        {\n          \"key\": 99.0,\n          \"value\": 980.0\n        }\n      ]\n    }\n  }\n}\n--------------------------------------------------\n// TESTRESPONSE[s/\\.\\.\\./\"took\": $body.took,\"timed_out\": false,\"_shards\": $body._shards,\"hits\": $body.hits,/]\n// TESTRESPONSE[s/\"value\": 10.0/\"value\": 9.9/]\n// TESTRESPONSE[s/\"value\": 30.0/\"value\": 29.5/]\n// TESTRESPONSE[s/\"value\": 170.0/\"value\": 167.5/]\n// TESTRESPONSE[s/\"value\": 445.0/\"value\": 445.0/]\n// TESTRESPONSE[s/\"value\": 720.0/\"value\": 722.5/]\n// TESTRESPONSE[s/\"value\": 940.0/\"value\": 940.5/]\n// TESTRESPONSE[s/\"value\": 980.0/\"value\": 980.1/]\n\n==== Script\n\nIf you need to run the aggregation against values that aren't indexed, use\na <<runtime,runtime field>>. For example, if our load times\nare in milliseconds but you want percentiles calculated in seconds:\n\n[source,console]\n----\nGET latency/_search\n{\n  \"size\": 0,\n  \"runtime_mappings\": {\n    \"load_time.seconds\": {\n      \"type\": \"long\",\n      \"script\": {\n        \"source\": \"emit(doc['load_time'].value / params.timeUnit)\",\n        \"params\": {\n          \"timeUnit\": 1000\n        }\n      }\n    }\n  },\n  \"aggs\": {\n    \"load_time_outlier\": {\n      \"percentiles\": {\n        \"field\": \"load_time.seconds\"\n      }\n    }\n  }\n}\n----\n// TEST[setup:latency]\n// TEST[s/_search/_search?filter_path=aggregations/]\n// TEST[s/\"timeUnit\": 1000/\"timeUnit\": 10/]\n\n////\n[source,console-result]\n----\n{\n \"aggregations\": {\n    \"load_time_outlier\": {\n      \"values\": {\n        \"1.0\": 0.99,\n        \"5.0\": 2.95,\n        \"25.0\": 16.75,\n        \"50.0\": 44.5,\n        \"75.0\": 72.25,\n        \"95.0\": 94.05,\n        \"99.0\": 98.01\n      }\n    }\n  }\n}\n----\n////\n\n[[search-aggregations-metrics-percentile-aggregation-approximation]]\n==== Percentiles are (usually) approximate\n\n// tag::approximate[]\nThere are many different algorithms to calculate percentiles. The naive\nimplementation simply stores all the values in a sorted array. To find the 50th\npercentile, you simply find the value that is at `my_array[count(my_array) * 0.5]`.\n\nClearly, the naive implementation does not scale -- the sorted array grows\nlinearly with the number of values in your dataset. To calculate percentiles\nacross potentially billions of values in an Elasticsearch cluster, _approximate_\npercentiles are calculated.\n\nThe algorithm used by the `percentile` metric is called TDigest (introduced by\nTed Dunning in\nhttps://github.com/tdunning/t-digest/blob/master/docs/t-digest-paper/histo.pdf[Computing Accurate Quantiles using T-Digests]).\n\nWhen using this metric, there are a few guidelines to keep in mind:\n\n- Accuracy is proportional to `q(1-q)`. This means that extreme percentiles (e.g. 99%)\nare more accurate than less extreme percentiles, such as the median\n- For small sets of values, percentiles are highly accurate (and potentially\n100% accurate if the data is small enough).\n- As the quantity of values in a bucket grows, the algorithm begins to approximate\nthe percentiles. It is effectively trading accuracy for memory savings. The\nexact level of inaccuracy is difficult to generalize, since it depends on your\ndata distribution and volume of data being aggregated\n\nThe following chart shows the relative error on a uniform distribution depending\non the number of collected values and the requested percentile:\n\nimage:images/percentiles_error.png[]\n\nIt shows how precision is better for extreme percentiles. The reason why error diminishes\nfor large number of values is that the law of large numbers makes the distribution of\nvalues more and more uniform and the t-digest tree can do a better job at summarizing\nit. It would not be the case on more skewed distributions.\n\n// end::approximate[]\n\n[WARNING]\n====\nPercentile aggregations are also\n{wikipedia}/Nondeterministic_algorithm[non-deterministic].\nThis means you can get slightly different results using the same data.\n====\n\n[[search-aggregations-metrics-percentile-aggregation-compression]]\n==== Compression\n\nApproximate algorithms must balance memory utilization with estimation accuracy.\nThis balance can be controlled using a `compression` parameter:\n\n[source,console]\n--------------------------------------------------\nGET latency/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"load_time_outlier\": {\n      \"percentiles\": {\n        \"field\": \"load_time\",\n        \"tdigest\": {\n          \"compression\": 200    <1>\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:latency]\n\n<1> Compression controls memory usage and approximation error\n\n// tag::t-digest[]\nThe TDigest algorithm uses a number of \"nodes\" to approximate percentiles -- the\nmore nodes available, the higher the accuracy (and large memory footprint) proportional\nto the volume of data. The `compression` parameter limits the maximum number of\nnodes to `20 * compression`.\n\nTherefore, by increasing the compression value, you can increase the accuracy of\nyour percentiles at the cost of more memory. Larger compression values also\nmake the algorithm slower since the underlying tree data structure grows in size,\nresulting in more expensive operations. The default compression value is\n`100`.\n\nA \"node\" uses roughly 32 bytes of memory, so under worst-case scenarios (large amount\nof data which arrives sorted and in-order) the default settings will produce a\nTDigest roughly 64KB in size. In practice data tends to be more random and\nthe TDigest will use less memory.\n// end::t-digest[]\n\n[[search-aggregations-metrics-percentile-aggregation-execution-hint]]\n==== Execution hint\n\nThe default implementation of TDigest is optimized for performance, scaling to millions or even\nbillions of sample values while maintaining acceptable accuracy levels (close to 1% relative error\nfor millions of samples in some cases). There's an option to use an implementation optimized\nfor accuracy by setting parameter `execution_hint` to value `high_accuracy`:\n\n[source,console]\n--------------------------------------------------\nGET latency/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"load_time_outlier\": {\n      \"percentiles\": {\n        \"field\": \"load_time\",\n        \"tdigest\": {\n          \"execution_hint\": \"high_accuracy\"    <1>\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:latency]\n\n<1> Optimize TDigest for accuracy, at the expense of performance\n\nThis option can lead to improved accuracy (relative error close to 0.01% for millions of samples in some\ncases) but then percentile queries take 2x-10x longer to complete.\n\n==== HDR histogram\n\nhttps://github.com/HdrHistogram/HdrHistogram[HDR Histogram] (High Dynamic Range Histogram) is an alternative implementation\nthat can be useful when calculating percentiles for latency measurements as it can be faster than the t-digest implementation\nwith the trade-off of a larger memory footprint. This implementation maintains a fixed worse-case percentage error (specified\nas a number of significant digits). This means that if data is recorded with values from 1 microsecond up to 1 hour\n(3,600,000,000 microseconds) in a histogram set to 3 significant digits, it will maintain a value resolution of 1 microsecond\nfor values up to 1 millisecond and 3.6 seconds (or better) for the maximum tracked value (1 hour).\n\nThe HDR Histogram can be used by specifying the `hdr` parameter in the request:\n\n[source,console]\n--------------------------------------------------\nGET latency/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"load_time_outlier\": {\n      \"percentiles\": {\n        \"field\": \"load_time\",\n        \"percents\": [ 95, 99, 99.9 ],\n        \"hdr\": {                                  <1>\n          \"number_of_significant_value_digits\": 3 <2>\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:latency]\n\n<1> `hdr` object indicates that HDR Histogram should be used to calculate the percentiles and specific settings for this algorithm can be specified inside the object\n<2> `number_of_significant_value_digits` specifies the resolution of values for the histogram in number of significant digits\n\nThe HDRHistogram only supports positive values and will error if it is passed a negative value. It is also not a good idea to use\nthe HDRHistogram if the range of values is unknown as this could lead to high memory usage.\n\n==== Missing value\n\nThe `missing` parameter defines how documents that are missing a value should be treated.\nBy default they will be ignored but it is also possible to treat them as if they\nhad a value.\n\n[source,console]\n--------------------------------------------------\nGET latency/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"grade_percentiles\": {\n      \"percentiles\": {\n        \"field\": \"grade\",\n        \"missing\": 10       <1>\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[setup:latency]\n\n<1> Documents without a value in the `grade` field will fall into the same bucket as documents that have the value `10`.\n"
}