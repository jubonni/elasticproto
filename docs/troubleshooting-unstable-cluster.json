{
    "meta": {
        "timestamp": "2024-11-01T03:07:10.467271",
        "size": 15667,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/troubleshooting-unstable-cluster.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "troubleshooting-unstable-cluster",
        "version": "8.15"
    },
    "doc": "[[troubleshooting-unstable-cluster]]\n== Troubleshooting an unstable cluster\n\nNormally, a node will only leave a cluster if deliberately shut down. If a node\nleaves the cluster unexpectedly, it's important to address the cause. A cluster\nin which nodes leave unexpectedly is unstable and can create several issues.\nFor instance:\n\n* The cluster health may be yellow or red.\n\n* Some shards will be initializing and other shards may be failing.\n\n* Search, indexing, and monitoring operations may fail and report exceptions in\nlogs.\n\n* The `.security` index may be unavailable, blocking access to the cluster.\n\n* The master may appear busy due to frequent cluster state updates.\n\nTo troubleshoot a cluster in this state, first ensure the cluster has a\n<<discovery-troubleshooting,stable master>>. Next, focus on the nodes\nunexpectedly leaving the cluster ahead of all other issues. It will not be\npossible to solve other issues until the cluster has a stable master node and\nstable node membership.\n\nDiagnostics and statistics are usually not useful in an unstable cluster. These\ntools only offer a view of the state of the cluster at a single point in time.\nInstead, look at the cluster logs to see the pattern of behaviour over time.\nFocus particularly on logs from the elected master. When a node leaves the\ncluster, logs for the elected master include a message like this (with line\nbreaks added to make it easier to read):\n\n[source,text]\n----\n[2022-03-21T11:02:35,513][INFO ][o.e.c.c.NodeLeftExecutor] [instance-0000000000]\n    node-left: [{instance-0000000004}{bfcMDTiDRkietFb9v_di7w}{aNlyORLASam1ammv2DzYXA}{172.27.47.21}{172.27.47.21:19054}{m}]\n    with reason [disconnected]\n----\n\nThis message says that the `NodeLeftExecutor` on the elected master\n(`instance-0000000000`) processed a `node-left` task, identifying the node that\nwas removed and the reason for its removal. When the node joins the cluster\nagain, logs for the elected master will include a message like this (with line\nbreaks added to make it easier to read):\n\n[source,text]\n----\n[2022-03-21T11:02:59,892][INFO ][o.e.c.c.NodeJoinExecutor] [instance-0000000000]\n    node-join: [{instance-0000000004}{bfcMDTiDRkietFb9v_di7w}{UNw_RuazQCSBskWZV8ID_w}{172.27.47.21}{172.27.47.21:19054}{m}]\n    with reason [joining after restart, removed [24s] ago with reason [disconnected]]\n----\n\nThis message says that the `NodeJoinExecutor` on the elected master\n(`instance-0000000000`) processed a `node-join` task, identifying the node that\nwas added to the cluster and the reason for the task.\n\nOther nodes may log similar messages, but report fewer details:\n\n[source,text]\n----\n[2020-01-29T11:02:36,985][INFO ][o.e.c.s.ClusterApplierService]\n    [instance-0000000001] removed {\n        {instance-0000000004}{bfcMDTiDRkietFb9v_di7w}{aNlyORLASam1ammv2DzYXA}{172.27.47.21}{172.27.47.21:19054}{m}\n        {tiebreaker-0000000003}{UNw_RuazQCSBskWZV8ID_w}{bltyVOQ-RNu20OQfTHSLtA}{172.27.161.154}{172.27.161.154:19251}{mv}\n    }, term: 14, version: 1653415, reason: Publication{term=14, version=1653415}\n----\n\nThese messages are not especially useful for troubleshooting, so focus on the\nones from the `NodeLeftExecutor` and `NodeJoinExecutor` which are only emitted\non the elected master and which contain more details. If you don't see the\nmessages from the `NodeLeftExecutor` and `NodeJoinExecutor`, check that:\n\n* You're looking at the logs for the elected master node.\n\n* The logs cover the correct time period.\n\n* Logging is enabled at `INFO` level.\n\nNodes will also log a message containing `master node changed` whenever they\nstart or stop following the elected master. You can use these messages to\ndetermine each node's view of the state of the master over time.\n\nIf a node restarts, it will leave the cluster and then join the cluster again.\nWhen it rejoins, the `NodeJoinExecutor` will log that it processed a\n`node-join` task indicating that the node is `joining after restart`. If a node\nis unexpectedly restarting, look at the node's logs to see why it is shutting\ndown.\n\nThe <<health-api>> API on the affected node will also provide some useful\ninformation about the situation.\n\nIf the node did not restart then you should look at the reason for its\ndeparture more closely. Each reason has different troubleshooting steps,\ndescribed below. There are three possible reasons:\n\n* `disconnected`: The connection from the master node to the removed node was\nclosed.\n\n* `lagging`: The master published a cluster state update, but the removed node\ndid not apply it within the permitted timeout. By default, this timeout is 2\nminutes. Refer to <<modules-discovery-settings>> for information about the\nsettings which control this mechanism.\n\n* `followers check retry count exceeded`: The master sent a number of\nconsecutive health checks to the removed node. These checks were rejected or\ntimed out. By default, each health check times out after 10 seconds and {es}\nremoves the node removed after three consecutively failed health checks. Refer\nto <<modules-discovery-settings>> for information about the settings which\ncontrol this mechanism.\n\n[discrete]\n[[troubleshooting-unstable-cluster-disconnected]]\n=== Diagnosing `disconnected` nodes\n\nNodes typically leave the cluster with reason `disconnected` when they shut\ndown, but if they rejoin the cluster without restarting then there is some\nother problem.\n\n{es} is designed to run on a fairly reliable network. It opens a number of TCP\nconnections between nodes and expects these connections to remain open\n<<long-lived-connections,forever>>. If a connection is closed then {es} will\ntry and reconnect, so the occasional blip may fail some in-flight operations\nbut should otherwise have limited impact on the cluster. In contrast,\nrepeatedly-dropped connections will severely affect its operation.\n\nThe connections from the elected master node to every other node in the cluster\nare particularly important. The elected master never spontaneously closes its\noutbound connections to other nodes. Similarly, once an inbound connection is\nfully established, a node never spontaneously it unless the node is shutting\ndown.\n\nIf you see a node unexpectedly leave the cluster with the `disconnected`\nreason, something other than {es} likely caused the connection to close. A\ncommon cause is a misconfigured firewall with an improper timeout or another\npolicy that's <<long-lived-connections,incompatible with {es}>>. It could also\nbe caused by general connectivity issues, such as packet loss due to faulty\nhardware or network congestion. If you're an advanced user, configure the\nfollowing loggers to get more detailed information about network exceptions:\n\n[source,yaml]\n----\nlogger.org.elasticsearch.transport.TcpTransport: DEBUG\nlogger.org.elasticsearch.xpack.core.security.transport.netty4.SecurityNetty4Transport: DEBUG\n----\n\nIf these logs do not show enough information to diagnose the problem, obtain a\npacket capture simultaneously from the nodes at both ends of an unstable\nconnection and analyse it alongside the {es} logs from those nodes to determine\nif traffic between the nodes is being disrupted by another device on the\nnetwork.\n\n[discrete]\n[[troubleshooting-unstable-cluster-lagging]]\n=== Diagnosing `lagging` nodes\n\n{es} needs every node to process cluster state updates reasonably quickly. If a\nnode takes too long to process a cluster state update, it can be harmful to the\ncluster. The master will remove these nodes with the `lagging` reason. Refer to\n<<modules-discovery-settings>> for information about the settings which control\nthis mechanism.\n\nLagging is typically caused by performance issues on the removed node. However,\na node may also lag due to severe network delays. To rule out network delays,\nensure that `net.ipv4.tcp_retries2` is <<system-config-tcpretries,configured\nproperly>>. Log messages that contain `warn threshold` may provide more\ninformation about the root cause.\n\nIf you're an advanced user, you can get more detailed information about what\nthe node was doing when it was removed by configuring the following logger:\n\n[source,yaml]\n----\nlogger.org.elasticsearch.cluster.coordination.LagDetector: DEBUG\n----\n\nWhen this logger is enabled, {es} will attempt to run the\n<<cluster-nodes-hot-threads>> API on the faulty node and report the results in\nthe logs on the elected master. The results are compressed, encoded, and split\ninto chunks to avoid truncation:\n\n[source,text]\n----\n[DEBUG][o.e.c.c.LagDetector      ] [master] hot threads from node [{node}{g3cCUaMDQJmQ2ZLtjr-3dg}{10.0.0.1:9300}] lagging at version [183619] despite commit of cluster state version [183620] [part 1]: H4sIAAAAAAAA/x...\n[DEBUG][o.e.c.c.LagDetector      ] [master] hot threads from node [{node}{g3cCUaMDQJmQ2ZLtjr-3dg}{10.0.0.1:9300}] lagging at version [183619] despite commit of cluster state version [183620] [part 2]: p7x3w1hmOQVtuV...\n[DEBUG][o.e.c.c.LagDetector      ] [master] hot threads from node [{node}{g3cCUaMDQJmQ2ZLtjr-3dg}{10.0.0.1:9300}] lagging at version [183619] despite commit of cluster state version [183620] [part 3]: v7uTboMGDbyOy+...\n[DEBUG][o.e.c.c.LagDetector      ] [master] hot threads from node [{node}{g3cCUaMDQJmQ2ZLtjr-3dg}{10.0.0.1:9300}] lagging at version [183619] despite commit of cluster state version [183620] [part 4]: 4tse0RnPnLeDNN...\n[DEBUG][o.e.c.c.LagDetector      ] [master] hot threads from node [{node}{g3cCUaMDQJmQ2ZLtjr-3dg}{10.0.0.1:9300}] lagging at version [183619] despite commit of cluster state version [183620] (gzip compressed, base64-encoded, and split into 4 parts on preceding log lines)\n----\n\nTo reconstruct the output, base64-decode the data and decompress it using\n`gzip`. For instance, on Unix-like systems:\n\n[source,sh]\n----\ncat lagdetector.log | sed -e 's/.*://' | base64 --decode | gzip --decompress\n----\n\n[discrete]\n[[troubleshooting-unstable-cluster-follower-check]]\n=== Diagnosing `follower check retry count exceeded` nodes\n\nNodes sometimes leave the cluster with reason `follower check retry count\nexceeded` when they shut down, but if they rejoin the cluster without\nrestarting then there is some other problem.\n\n{es} needs every node to respond to network messages successfully and\nreasonably quickly. If a node rejects requests or does not respond at all then\nit can be harmful to the cluster. If enough consecutive checks fail then the\nmaster will remove the node with reason `follower check retry count exceeded`\nand will indicate in the `node-left` message how many of the consecutive\nunsuccessful checks failed and how many of them timed out. Refer to\n<<modules-discovery-settings>> for information about the settings which control\nthis mechanism.\n\nTimeouts and failures may be due to network delays or performance problems on\nthe affected nodes. Ensure that `net.ipv4.tcp_retries2` is\n<<system-config-tcpretries,configured properly>> to eliminate network delays as\na possible cause for this kind of instability. Log messages containing\n`warn threshold` may give further clues about the cause of the instability.\n\nIf the last check failed with an exception then the exception is reported, and\ntypically indicates the problem that needs to be addressed. If any of the\nchecks timed out then narrow down the problem as follows.\n\ninclude::network-timeouts.asciidoc[tag=troubleshooting-network-timeouts-gc-vm]\n\ninclude::network-timeouts.asciidoc[tag=troubleshooting-network-timeouts-packet-capture-fault-detection]\n\ninclude::network-timeouts.asciidoc[tag=troubleshooting-network-timeouts-threads]\n\nBy default the follower checks will time out after 30s, so if node departures\nare unpredictable then capture stack dumps every 15s to be sure that at least\none stack dump was taken at the right time.\n\n[discrete]\n[[troubleshooting-unstable-cluster-shardlockobtainfailedexception]]\n=== Diagnosing `ShardLockObtainFailedException` failures\n\nIf a node leaves and rejoins the cluster then {es} will usually shut down and\nre-initialize its shards. If the shards do not shut down quickly enough then\n{es} may fail to re-initialize them due to a `ShardLockObtainFailedException`.\n\nTo gather more information about the reason for shards shutting down slowly,\nconfigure the following logger:\n\n[source,yaml]\n----\nlogger.org.elasticsearch.env.NodeEnvironment: DEBUG\n----\n\nWhen this logger is enabled, {es} will attempt to run the\n<<cluster-nodes-hot-threads>> API whenever it encounters a\n`ShardLockObtainFailedException`. The results are compressed, encoded, and\nsplit into chunks to avoid truncation:\n\n[source,text]\n----\n[DEBUG][o.e.e.NodeEnvironment    ] [master] hot threads while failing to obtain shard lock for [index][0] [part 1]: H4sIAAAAAAAA/x...\n[DEBUG][o.e.e.NodeEnvironment    ] [master] hot threads while failing to obtain shard lock for [index][0] [part 2]: p7x3w1hmOQVtuV...\n[DEBUG][o.e.e.NodeEnvironment    ] [master] hot threads while failing to obtain shard lock for [index][0] [part 3]: v7uTboMGDbyOy+...\n[DEBUG][o.e.e.NodeEnvironment    ] [master] hot threads while failing to obtain shard lock for [index][0] [part 4]: 4tse0RnPnLeDNN...\n[DEBUG][o.e.e.NodeEnvironment    ] [master] hot threads while failing to obtain shard lock for [index][0] (gzip compressed, base64-encoded, and split into 4 parts on preceding log lines)\n----\n\nTo reconstruct the output, base64-decode the data and decompress it using\n`gzip`. For instance, on Unix-like systems:\n\n[source,sh]\n----\ncat shardlock.log | sed -e 's/.*://' | base64 --decode | gzip --decompress\n----\n\n[discrete]\n[[troubleshooting-unstable-cluster-network]]\n=== Diagnosing other network disconnections\n\n{es} is designed to run on a fairly reliable network. It opens a number of TCP\nconnections between nodes and expects these connections to remain open\n<<long-lived-connections,forever>>. If a connection is closed then {es} will\ntry and reconnect, so the occasional blip may fail some in-flight operations\nbut should otherwise have limited impact on the cluster. In contrast,\nrepeatedly-dropped connections will severely affect its operation.\n\n{es} nodes will only actively close an outbound connection to another node if\nthe other node leaves the cluster. See\n<<cluster-fault-detection-troubleshooting>> for further information about\nidentifying and troubleshooting this situation. If an outbound connection\ncloses for some other reason, nodes will log a message such as the following:\n\n[source,text]\n----\n[INFO ][o.e.t.ClusterConnectionManager] [node-1] transport connection to [{node-2}{g3cCUaMDQJmQ2ZLtjr-3dg}{10.0.0.1:9300}] closed by remote\n----\n\nSimilarly, once an inbound connection is fully established, a node never\nspontaneously closes it unless the node is shutting down.\n\nTherefore if you see a node report that a connection to another node closed\nunexpectedly, something other than {es} likely caused the connection to close.\nA common cause is a misconfigured firewall with an improper timeout or another\npolicy that's <<long-lived-connections,incompatible with {es}>>. It could also\nbe caused by general connectivity issues, such as packet loss due to faulty\nhardware or network congestion. If you're an advanced user, configure the\nfollowing loggers to get more detailed information about network exceptions:\n\n[source,yaml]\n----\nlogger.org.elasticsearch.transport.TcpTransport: DEBUG\nlogger.org.elasticsearch.xpack.core.security.transport.netty4.SecurityNetty4Transport: DEBUG\n----\n\nIf these logs do not show enough information to diagnose the problem, obtain a\npacket capture simultaneously from the nodes at both ends of an unstable\nconnection and analyse it alongside the {es} logs from those nodes to determine\nif traffic between the nodes is being disrupted by another device on the\nnetwork.\n"
}