{
    "meta": {
        "timestamp": "2024-11-01T03:02:52.500580",
        "size": 16208,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/evaluate-dfanalytics.html",
        "type": "documentation",
        "role": [
            "xpack"
        ],
        "has_code": true,
        "title": "evaluate-dfanalytics",
        "version": "8.15"
    },
    "doc": "[role=\"xpack\"]\n[[evaluate-dfanalytics]]\n= Evaluate {dfanalytics} API\n\n[subs=\"attributes\"]\n++++\n<titleabbrev>Evaluate {dfanalytics}</titleabbrev>\n++++\n\nEvaluates the {dfanalytics} for an annotated index.\n\n\n[[ml-evaluate-dfanalytics-request]]\n== {api-request-title}\n\n`POST _ml/data_frame/_evaluate`\n\n\n[[ml-evaluate-dfanalytics-prereq]]\n== {api-prereq-title}\n\nRequires the following privileges:\n\n* cluster: `monitor_ml` (the `machine_learning_user` built-in role grants this \n  privilege)\n* destination index: `read`\n\n\n[[ml-evaluate-dfanalytics-desc]]\n== {api-description-title}\n\nThe API packages together commonly used evaluation metrics for various types of \nmachine learning features. This has been designed for use on indexes created by \n{dfanalytics}. Evaluation requires both a ground truth field and an analytics \nresult field to be present.\n\n\n[[ml-evaluate-dfanalytics-request-body]]\n== {api-request-body-title}\n\n`evaluation`::\n(Required, object) Defines the type of evaluation you want to perform.\nSee <<ml-evaluate-dfanalytics-resources>>.\n+\n--\nAvailable evaluation types:\n\n* `outlier_detection`\n* `regression`\n* `classification`\n\n--\n\n`index`::\n(Required, object) Defines the `index` in which the evaluation will be\nperformed.\n\n`query`::\n(Optional, object) A query clause that retrieves a subset of data from the\nsource index. See <<query-dsl>>.\n\n[[ml-evaluate-dfanalytics-resources]]\n== {dfanalytics-cap} evaluation resources\n\n[[oldetection-resources]]\n=== {oldetection-cap} evaluation objects\n\n{oldetection-cap} evaluates the results of an {oldetection} analysis which \noutputs the probability that each document is an outlier.\n\n`actual_field`::\n  (Required, string) The field of the `index` which contains the `ground truth`. \n  The data type of this field can be boolean or integer. If the data type is \n  integer, the value has to be either `0` (false) or `1` (true).\n\n`predicted_probability_field`::\n  (Required, string) The field of the `index` that defines the probability of \n  whether the item belongs to the class in question or not. It's the field that \n  contains the results of the analysis.\n\n`metrics`::\n  (Optional, object) Specifies the metrics that are used for the evaluation. If \n  no metrics are specified, the following are returned by default: \n  \n  * `auc_roc` (`include_curve`: false), \n  * `precision` (`at`: [0.25, 0.5, 0.75]), \n  * `recall` (`at`: [0.25, 0.5, 0.75]), \n  * `confusion_matrix` (`at`: [0.25, 0.5, 0.75]).\n  \n  `auc_roc`:::\n    (Optional, object) The AUC ROC (area under the curve of the receiver \n    operating characteristic) score and optionally the curve. Default value is \n    {\"include_curve\": false}.\n    \n  `confusion_matrix`:::\n    (Optional, object) Set the different thresholds of the {olscore} at where\n    the metrics (`tp` - true positive, `fp` - false positive, `tn` - true\n    negative, `fn` - false negative) are calculated. Default value is\n    {\"at\": [0.25, 0.50, 0.75]}.\n\n  `precision`:::\n    (Optional, object) Set the different thresholds of the {olscore} at where \n    the metric is calculated. Default value is {\"at\": [0.25, 0.50, 0.75]}.\n  \n  `recall`:::\n    (Optional, object) Set the different thresholds of the {olscore} at where \n    the metric is calculated. Default value is {\"at\": [0.25, 0.50, 0.75]}.\n\n    \n[[regression-evaluation-resources]]\n=== {regression-cap} evaluation objects\n\n{regression-cap} evaluation evaluates the results of a {regression} analysis \nwhich outputs a prediction of values.\n\n`actual_field`::\n  (Required, string) The field of the `index` which contains the `ground truth`. \n  The data type of this field must be numerical.\n  \n`predicted_field`::\n  (Required, string) The field in the `index` that contains the predicted value, \n  in other words the results of the {regression} analysis.\n  \n`metrics`::\n  (Optional, object) Specifies the metrics that are used for the evaluation. For \n  more information on `mse`, `msle`, and `huber`, consult \n  https://github.com/elastic/examples/tree/master/Machine%20Learning/Regression%20Loss%20Functions[the Jupyter notebook on regression loss functions].\n  If no metrics are specified, the following are returned by default: \n  \n  * `mse`, \n  * `r_squared`,\n  * `huber` (`delta`: 1.0). \n\n  `mse`:::\n    (Optional, object) Average squared difference between the predicted values \n    and the actual (`ground truth`) value. For more information, read \n    {wikipedia}/Mean_squared_error[this wiki article].\n\n  `msle`:::\n    (Optional, object) Average squared difference between the logarithm of the \n    predicted values and the logarithm of the actual (`ground truth`) value.\n    \n    `offset`::::\n      (Optional, double) Defines the transition point at which you switch from \n      minimizing quadratic error to minimizing quadratic log error. Defaults to \n      `1`.\n\n  `huber`:::\n    (Optional, object) Pseudo Huber loss function. For more information, read \n    {wikipedia}/Huber_loss#Pseudo-Huber_loss_function[this wiki article].\n    \n    `delta`::::\n      (Optional, double) Approximates 1/2 (prediction - actual)^2^ for values \n      much less than delta and approximates a straight line with slope delta for \n      values much larger than delta. Defaults to `1`. Delta needs to be greater \n      than `0`.\n\n  `r_squared`:::\n    (Optional, object) Proportion of the variance in the dependent variable that \n    is predictable from the independent variables. For more information, read \n    {wikipedia}/Coefficient_of_determination[this wiki article].\n\n\n  \n[[classification-evaluation-resources]]\n== {classification-cap} evaluation objects\n\n{classification-cap} evaluation evaluates the results of a {classanalysis} which \noutputs a prediction that identifies to which of the classes each document \nbelongs.\n\n`actual_field`::\n  (Required, string) The field of the `index` which contains the `ground truth`.\n  The data type of this field must be categorical.\n  \n`predicted_field`::\n  (Optional, string) The field in the `index` which contains the predicted value,\n  in other words the results of the {classanalysis}.\n\n`top_classes_field`::\n  (Optional, string) The field of the `index` which is an array of documents\n  of the form `{ \"class_name\": XXX, \"class_probability\": YYY }`.\n  This field must be defined as `nested` in the mappings.\n\n`metrics`::\n  (Optional, object) Specifies the metrics that are used for the evaluation. If \n  no metrics are specificed, the following are returned by default: \n  \n  * `accuracy`, \n  * `multiclass_confusion_matrix`, \n  * `precision`, \n  * `recall`.\n\n  `accuracy`:::\n    (Optional, object) Accuracy of predictions (per-class and overall).\n\n  `auc_roc`:::\n    (Optional, object) The AUC ROC (area under the curve of the receiver\n    operating characteristic) score and optionally the curve.\n    It is calculated for a specific class (provided as \"class_name\") treated as \n    positive.\n\n    `class_name`::::\n      (Required, string) Name of the only class that is treated as positive \n      during AUC ROC calculation. Other classes are treated as negative \n      (\"one-vs-all\" strategy). All the evaluated documents must have \n      `class_name` in the list of their top classes.\n\n    `include_curve`::::\n      (Optional, Boolean) Whether or not the curve should be returned in\n      addition to the score. Default value is false.\n\n  `multiclass_confusion_matrix`:::\n    (Optional, object) Multiclass confusion matrix.\n\n    `size`::::\n      (Optional, double) Specifies the size of the multiclass confusion matrix. \n      Defaults to `10` which results in a matrix of size 10x10.\n\n  `precision`:::\n    (Optional, object) Precision of predictions (per-class and average).\n\n  `recall`:::\n    (Optional, object) Recall of predictions (per-class and average).\n\n\n////\n[[ml-evaluate-dfanalytics-results]]\n== {api-response-body-title}\n\n`outlier_detection`::\n  (object) If you chose to do outlier detection, the API returns the\n  following evaluation metrics:\n  \n`auc_roc`::: TBD\n\n`confusion_matrix`::: TBD\n  \n`precision`::: TBD\n\n`recall`::: TBD\n////\n\n\n[[ml-evaluate-dfanalytics-example]]\n== {api-examples-title}\n\n\n[[ml-evaluate-oldetection-example]]\n=== {oldetection-cap}\n\n[source,console]\n--------------------------------------------------\nPOST _ml/data_frame/_evaluate\n{\n  \"index\": \"my_analytics_dest_index\",\n  \"evaluation\": {\n    \"outlier_detection\": {\n      \"actual_field\": \"is_outlier\",\n      \"predicted_probability_field\": \"ml.outlier_score\"\n    }\n  }\n}\n--------------------------------------------------\n// TEST[skip:TBD]\n\nThe API returns the following results:\n\n[source,console-result]\n----\n{\n  \"outlier_detection\": {\n    \"auc_roc\": {\n      \"value\": 0.92584757746414444\n    },\n    \"confusion_matrix\": {\n      \"0.25\": {\n          \"tp\": 5,\n          \"fp\": 9,\n          \"tn\": 204,\n          \"fn\": 5\n      },\n      \"0.5\": {\n          \"tp\": 1,\n          \"fp\": 5,\n          \"tn\": 208,\n          \"fn\": 9\n      },\n      \"0.75\": {\n          \"tp\": 0,\n          \"fp\": 4,\n          \"tn\": 209,\n          \"fn\": 10\n      }\n    },\n    \"precision\": {\n        \"0.25\": 0.35714285714285715,\n        \"0.5\": 0.16666666666666666,\n        \"0.75\": 0\n    },\n    \"recall\": {\n        \"0.25\": 0.5,\n        \"0.5\": 0.1,\n        \"0.75\": 0\n    }\n  }\n}\n----\n\n\n[[ml-evaluate-regression-example]]\n=== {regression-cap}\n\n[source,console]\n--------------------------------------------------\nPOST _ml/data_frame/_evaluate\n{\n  \"index\": \"house_price_predictions\", <1>\n  \"query\": {\n      \"bool\": {\n        \"filter\": [\n          { \"term\":  { \"ml.is_training\": false } } <2>\n        ]\n      }\n  },\n  \"evaluation\": {\n    \"regression\": { \n      \"actual_field\": \"price\", <3>\n      \"predicted_field\": \"ml.price_prediction\", <4>\n      \"metrics\": {  \n        \"r_squared\": {},\n        \"mse\": {},\n        \"msle\": {\"offset\": 10},\n        \"huber\": {\"delta\": 1.5}\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[skip:TBD]\n\n<1> The output destination index from a {dfanalytics} {reganalysis}.\n<2> In this example, a test/train split (`training_percent`) was defined for the \n{reganalysis}. This query limits evaluation to be performed on the test split \nonly. \n<3> The ground truth value for the actual house price. This is required in order \nto evaluate results.\n<4> The predicted value for house price calculated by the {reganalysis}.\n\n\nThe following example calculates the training error:\n\n[source,console]\n--------------------------------------------------\nPOST _ml/data_frame/_evaluate\n{\n  \"index\": \"student_performance_mathematics_reg\",\n  \"query\": {\n    \"term\": {\n      \"ml.is_training\": {\n        \"value\": true <1>\n      }\n    }\n  },\n  \"evaluation\": {\n    \"regression\": { \n      \"actual_field\": \"G3\", <2>\n      \"predicted_field\": \"ml.G3_prediction\", <3>\n      \"metrics\": {  \n        \"r_squared\": {},\n        \"mse\": {},\n        \"msle\": {},\n        \"huber\": {}\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[skip:TBD]\n\n<1> In this example, a test/train split (`training_percent`) was defined for the \n{reganalysis}. This query limits evaluation to be performed on the train split \nonly. It means that a training error will be calculated.\n<2> The field that contains the ground truth value for the actual student \nperformance. This is required in order to evaluate results.\n<3> The field that contains the predicted value for student performance \ncalculated by the {reganalysis}.\n\n\nThe next example calculates the testing error. The only difference compared with \nthe previous example is that `ml.is_training` is set to `false` this time, so \nthe query excludes the train split from the evaluation.\n\n[source,console]\n--------------------------------------------------\nPOST _ml/data_frame/_evaluate\n{\n  \"index\": \"student_performance_mathematics_reg\",\n  \"query\": {\n    \"term\": {\n      \"ml.is_training\": {\n        \"value\": false <1>\n      }\n    }\n  },\n  \"evaluation\": {\n    \"regression\": { \n      \"actual_field\": \"G3\", <2>\n      \"predicted_field\": \"ml.G3_prediction\", <3>\n      \"metrics\": {  \n        \"r_squared\": {},\n        \"mse\": {},\n        \"msle\": {},\n        \"huber\": {}\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[skip:TBD]\n\n<1> In this example, a test/train split (`training_percent`) was defined for the \n{reganalysis}. This query limits evaluation to be performed on the test split \nonly. It means that a testing error will be calculated.\n<2> The field that contains the ground truth value for the actual student \nperformance. This is required in order to evaluate results.\n<3> The field that contains the predicted value for student performance \ncalculated by the {reganalysis}.\n\n\n[[ml-evaluate-classification-example]]\n=== {classification-cap}\n\n\n[source,console]\n--------------------------------------------------\nPOST _ml/data_frame/_evaluate\n{ \n   \"index\": \"animal_classification\",\n   \"evaluation\": {\n      \"classification\": { <1>\n         \"actual_field\": \"animal_class\", <2>\n         \"predicted_field\": \"ml.animal_class_prediction\", <3>\n         \"metrics\": {  \n           \"multiclass_confusion_matrix\" : {} <4>\n         }\n      }\n   }\n}\n--------------------------------------------------\n// TEST[skip:TBD]\n\n<1> The evaluation type.\n<2> The field that contains the ground truth value for the actual animal \nclassification. This is required in order to evaluate results.\n<3> The field that contains the predicted value for animal classification by \nthe {classanalysis}.\n<4> Specifies the metric for the evaluation.\n\n\nThe API returns the following result:\n\n[source,console-result]\n--------------------------------------------------\n{\n   \"classification\" : {\n      \"multiclass_confusion_matrix\" : {\n         \"confusion_matrix\" : [\n         {\n            \"actual_class\" : \"cat\", <1>\n            \"actual_class_doc_count\" : 12, <2>\n            \"predicted_classes\" : [ <3>\n              {\n                \"predicted_class\" : \"cat\",\n                \"count\" : 12 <4>\n              },\n              {\n                \"predicted_class\" : \"dog\",\n                \"count\" : 0 <5>\n              }\n            ],\n            \"other_predicted_class_doc_count\" : 0 <6>\n          },\n          {\n            \"actual_class\" : \"dog\",\n            \"actual_class_doc_count\" : 11,\n            \"predicted_classes\" : [\n              {\n                \"predicted_class\" : \"dog\",\n                \"count\" : 7\n              },\n              {\n                \"predicted_class\" : \"cat\",\n                \"count\" : 4\n              }\n            ],\n            \"other_predicted_class_doc_count\" : 0\n          }\n        ],\n        \"other_actual_class_count\" : 0\n      }\n    }\n  }\n--------------------------------------------------\n<1> The name of the actual class that the analysis tried to predict.\n<2> The number of documents in the index that belong to the `actual_class`.\n<3> This object contains the list of the predicted classes and the number of \npredictions associated with the class.\n<4> The number of cats in the dataset that are correctly identified as cats.\n<5> The number of cats in the dataset that are incorrectly classified as dogs.\n<6> The number of documents that are classified as a class that is not listed as \na `predicted_class`.\n\n\n\n[source,console]\n--------------------------------------------------\nPOST _ml/data_frame/_evaluate\n{\n   \"index\": \"animal_classification\",\n   \"evaluation\": {\n      \"classification\": { <1>\n         \"actual_field\": \"animal_class\", <2>\n         \"metrics\": {\n            \"auc_roc\" : { <3>\n              \"class_name\": \"dog\" <4>\n            }\n         }\n      }\n   }\n}\n--------------------------------------------------\n// TEST[skip:TBD]\n\n<1> The evaluation type.\n<2> The field that contains the ground truth value for the actual animal \nclassification. This is required in order to evaluate results.\n<3> Specifies the metric for the evaluation.\n<4> Specifies the class name that is treated as positive during the evaluation, \nall the other classes are treated as negative.\n\n\nThe API returns the following result:\n\n[source,console-result]\n--------------------------------------------------\n{\n  \"classification\" : {\n    \"auc_roc\" : {\n      \"value\" : 0.8941788639536681\n    }\n  }\n}\n--------------------------------------------------\n// TEST[skip:TBD]\n"
}