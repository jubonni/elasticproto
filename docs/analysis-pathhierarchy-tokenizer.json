{
    "meta": {
        "timestamp": "2024-11-01T03:02:53.152583",
        "size": 8717,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pathhierarchy-tokenizer.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "analysis-pathhierarchy-tokenizer",
        "version": "8.15"
    },
    "doc": "[[analysis-pathhierarchy-tokenizer]]\n=== Path hierarchy tokenizer\n++++\n<titleabbrev>Path hierarchy</titleabbrev>\n++++\n\nThe `path_hierarchy` tokenizer takes a hierarchical value like a filesystem\npath, splits on the path separator, and emits a term for each component in the\ntree. The `path_hierarcy` tokenizer uses Lucene's\nhttps://lucene.apache.org/core/{lucene_version_path}/analysis/common/org/apache/lucene/analysis/path/PathHierarchyTokenizer.html[PathHierarchyTokenizer]\nunderneath.\n\n[discrete]\n=== Example output\n\n[source,console]\n---------------------------\nPOST _analyze\n{\n  \"tokenizer\": \"path_hierarchy\",\n  \"text\": \"/one/two/three\"\n}\n---------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"/one\",\n      \"start_offset\": 0,\n      \"end_offset\": 4,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"/one/two\",\n      \"start_offset\": 0,\n      \"end_offset\": 8,\n      \"type\": \"word\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"/one/two/three\",\n      \"start_offset\": 0,\n      \"end_offset\": 14,\n      \"type\": \"word\",\n      \"position\": 2\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\n\nThe above text would produce the following terms:\n\n[source,text]\n---------------------------\n[ /one, /one/two, /one/two/three ]\n---------------------------\n\n[discrete]\n=== Configuration\n\nThe `path_hierarchy` tokenizer accepts the following parameters:\n\n[horizontal]\n`delimiter`::\n    The character to use as the path separator. Defaults to `/`.\n\n`replacement`::\n    An optional replacement character to use for the delimiter.\n    Defaults to the `delimiter`.\n\n`buffer_size`::\n    The number of characters read into the term buffer in a single pass.\n    Defaults to `1024`. The term buffer will grow by this size until all the\n    text has been consumed. It is advisable not to change this setting.\n\n`reverse`::\n    If `true`, uses Lucene's\n    http://lucene.apache.org/core/{lucene_version_path}/analysis/common/org/apache/lucene/analysis/path/ReversePathHierarchyTokenizer.html[ReversePathHierarchyTokenizer],\n    which is suitable for domain\u2013like hierarchies. Defaults to `false`.\n\n`skip`::\n    The number of initial tokens to skip. Defaults to `0`.\n\n[discrete]\n=== Example configuration\n\nIn this example, we configure the `path_hierarchy` tokenizer to split on `-`\ncharacters, and to replace them with `/`. The first two tokens are skipped:\n\n[source,console]\n----------------------------\nPUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"my_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"path_hierarchy\",\n          \"delimiter\": \"-\",\n          \"replacement\": \"/\",\n          \"skip\": 2\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"one-two-three-four-five\"\n}\n----------------------------\n\n/////////////////////\n\n[source,console-result]\n----------------------------\n{\n  \"tokens\": [\n    {\n      \"token\": \"/three\",\n      \"start_offset\": 7,\n      \"end_offset\": 13,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"/three/four\",\n      \"start_offset\": 7,\n      \"end_offset\": 18,\n      \"type\": \"word\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"/three/four/five\",\n      \"start_offset\": 7,\n      \"end_offset\": 23,\n      \"type\": \"word\",\n      \"position\": 2\n    }\n  ]\n}\n----------------------------\n\n/////////////////////\n\n\nThe above example produces the following terms:\n\n[source,text]\n---------------------------\n[ /three, /three/four, /three/four/five ]\n---------------------------\n\nIf we were to set `reverse` to `true`, it would produce the following:\n\n[source,text]\n---------------------------\n[ one/two/three/, two/three/, three/ ]\n---------------------------\n\n[discrete]\n[[analysis-pathhierarchy-tokenizer-detailed-examples]]\n=== Detailed examples\n\nA common use-case for the `path_hierarchy` tokenizer is filtering results by\nfile paths. If indexing a file path along with the data, the use of the\n`path_hierarchy` tokenizer to analyze the path allows filtering the results\nby different parts of the file path string.\n\n\nThis example configures an index to have two custom analyzers and applies\nthose analyzers to multifields of the `file_path` text field that will\nstore filenames. One of the two analyzers uses reverse tokenization.\nSome sample documents are then indexed to represent some file paths\nfor photos inside photo folders of two different users.\n\n\n[source,console]\n--------------------------------------------------\nPUT file-path-test\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"custom_path_tree\": {\n          \"tokenizer\": \"custom_hierarchy\"\n        },\n        \"custom_path_tree_reversed\": {\n          \"tokenizer\": \"custom_hierarchy_reversed\"\n        }\n      },\n      \"tokenizer\": {\n        \"custom_hierarchy\": {\n          \"type\": \"path_hierarchy\",\n          \"delimiter\": \"/\"\n        },\n        \"custom_hierarchy_reversed\": {\n          \"type\": \"path_hierarchy\",\n          \"delimiter\": \"/\",\n          \"reverse\": \"true\"\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"file_path\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"tree\": {\n            \"type\": \"text\",\n            \"analyzer\": \"custom_path_tree\"\n          },\n          \"tree_reversed\": {\n            \"type\": \"text\",\n            \"analyzer\": \"custom_path_tree_reversed\"\n          }\n        }\n      }\n    }\n  }\n}\n\nPOST file-path-test/_doc/1\n{\n  \"file_path\": \"/User/alice/photos/2017/05/16/my_photo1.jpg\"\n}\n\nPOST file-path-test/_doc/2\n{\n  \"file_path\": \"/User/alice/photos/2017/05/16/my_photo2.jpg\"\n}\n\nPOST file-path-test/_doc/3\n{\n  \"file_path\": \"/User/alice/photos/2017/05/16/my_photo3.jpg\"\n}\n\nPOST file-path-test/_doc/4\n{\n  \"file_path\": \"/User/alice/photos/2017/05/15/my_photo1.jpg\"\n}\n\nPOST file-path-test/_doc/5\n{\n  \"file_path\": \"/User/bob/photos/2017/05/16/my_photo1.jpg\"\n}\n--------------------------------------------------\n\n\nA search for a particular file path string against the text field matches all\nthe example documents, with Bob's documents ranking highest due to `bob` also\nbeing one of the terms created by the standard analyzer boosting relevance for\nBob's documents.\n\n[source,console]\n--------------------------------------------------\nGET file-path-test/_search\n{\n  \"query\": {\n    \"match\": {\n      \"file_path\": \"/User/bob/photos/2017/05\"\n    }\n  }\n}\n--------------------------------------------------\n// TEST[continued]\n\nIt's simple to match or filter documents with file paths that exist within a\nparticular directory using the `file_path.tree` field.\n\n[source,console]\n--------------------------------------------------\nGET file-path-test/_search\n{\n  \"query\": {\n    \"term\": {\n      \"file_path.tree\": \"/User/alice/photos/2017/05/16\"\n    }\n  }\n}\n--------------------------------------------------\n// TEST[continued]\n\nWith the reverse parameter for this tokenizer, it's also possible to match\nfrom the other end of the file path, such as individual file names or a deep\nlevel subdirectory. The following example shows a search for all files named\n`my_photo1.jpg` within any directory via the `file_path.tree_reversed` field\nconfigured to use the reverse parameter in the mapping.\n\n\n[source,console]\n--------------------------------------------------\nGET file-path-test/_search\n{\n  \"query\": {\n    \"term\": {\n      \"file_path.tree_reversed\": {\n        \"value\": \"my_photo1.jpg\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[continued]\n\nViewing the tokens generated with both forward and reverse is instructive\nin showing the tokens created for the same file path value.\n\n\n[source,console]\n--------------------------------------------------\nPOST file-path-test/_analyze\n{\n  \"analyzer\": \"custom_path_tree\",\n  \"text\": \"/User/alice/photos/2017/05/16/my_photo1.jpg\"\n}\n\nPOST file-path-test/_analyze\n{\n  \"analyzer\": \"custom_path_tree_reversed\",\n  \"text\": \"/User/alice/photos/2017/05/16/my_photo1.jpg\"\n}\n--------------------------------------------------\n// TEST[continued]\n\n\nIt's also useful to be able to filter with file paths when combined with other\ntypes of searches, such as this example looking for any files paths with `16`\nthat also must be in Alice's photo directory.\n\n[source,console]\n--------------------------------------------------\nGET file-path-test/_search\n{\n  \"query\": {\n    \"bool\" : {\n      \"must\" : {\n        \"match\" : { \"file_path\" : \"16\" }\n      },\n      \"filter\": {\n        \"term\" : { \"file_path.tree\" : \"/User/alice\" }\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST[continued]\n"
}