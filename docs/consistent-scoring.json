{
    "meta": {
        "timestamp": "2024-11-01T03:07:10.140270",
        "size": 7423,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/consistent-scoring.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "consistent-scoring",
        "version": "8.15"
    },
    "doc": "[[consistent-scoring]]\n=== Getting consistent scoring\n\nThe fact that Elasticsearch operates with shards and replicas adds challenges\nwhen it comes to having good scoring.\n\n[discrete]\n==== Scores are not reproducible\n\nSay the same user runs the same request twice in a row and documents do not come\nback in the same order both times, this is a pretty bad experience isn't it?\nUnfortunately this is something that can happen if you have replicas\n(`index.number_of_replicas` is greater than 0). The reason is that Elasticsearch\nselects the shards that the query should go to in a round-robin fashion, so it\nis quite likely if you run the same query twice in a row that it will go to\ndifferent copies of the same shard.\n\nNow why is it a problem? Index statistics are an important part of the score.\nAnd these index statistics may be different across copies of the same shard\ndue to deleted documents. As you may know when documents are deleted or updated,\nthe old document is not immediately removed from the index, it is just marked\nas deleted and it will only be removed from disk on the next time that the\nsegment this old document belongs to is merged. However for practical reasons,\nthose deleted documents are taken into account for index statistics. So imagine\nthat the primary shard just finished a large merge that removed lots of deleted\ndocuments, then it might have index statistics that are sufficiently different\nfrom the replica (which still have plenty of deleted documents) so that scores\nare different too.\n\nThe recommended way to work around this issue is to use a string that identifies\nthe user that is logged in (a user id or session id for instance) as a\n<<search-preference,preference>>. This ensures that all queries of a\ngiven user are always going to hit the same shards, so scores remain more\nconsistent across queries.\n\nThis work around has another benefit: when two documents have the same score,\nthey will be sorted by their internal Lucene doc id (which is unrelated to the\n`_id`) by default. However these doc ids could be different across copies of\nthe same shard. So by always hitting the same shard, we would get more\nconsistent ordering of documents that have the same scores.\n\n[discrete]\n==== Relevancy looks wrong\n\nIf you notice that two documents with the same content get different scores or\nthat an exact match is not ranked first, then the issue might be related to\nsharding. By default, Elasticsearch makes each shard responsible for producing\nits own scores. However since index statistics are an important contributor to\nthe scores, this only works well if shards have similar index statistics. The\nassumption is that since documents are routed evenly to shards by default, then\nindex statistics should be very similar and scoring would work as expected.\nHowever in the event that you either:\n\n - use routing at index time,\n - query multiple _indices_,\n - or have too little data in your index\n\nthen there are good chances that all shards that are involved in the search\nrequest do not have similar index statistics and relevancy could be bad.\n\nIf you have a small dataset, the easiest way to work around this issue is to\nindex everything into an index that has a single shard\n(`index.number_of_shards: 1`), which is the default. Then index statistics\nwill be the same for all documents and scores will be consistent.\n\nOtherwise the recommended way to work around this issue is to use the\n<<dfs-query-then-fetch,`dfs_query_then_fetch`>> search type. This will make\nElasticsearch perform an initial round trip to all involved shards, asking\nthem for their index statistics relatively to the query, then the coordinating\nnode will merge those statistics and send the merged statistics alongside the\nrequest when asking shards to perform the `query` phase, so that shards can\nuse these global statistics rather than their own statistics in order to do the\nscoring.\n\nIn most cases, this additional round trip should be very cheap. However in the\nevent that your query contains a very large number of fields/terms or fuzzy\nqueries, beware that gathering statistics alone might not be cheap since all\nterms have to be looked up in the terms dictionaries in order to look up\nstatistics.\n\n[[static-scoring-signals]]\n=== Incorporating static relevance signals into the score\n\nMany domains have static signals that are known to be correlated with relevance.\nFor instance {wikipedia}/PageRank[PageRank] and url length are\ntwo commonly used features for web search in order to tune the score of web\npages independently of the query.\n\nThere are two main queries that allow combining static score contributions with\ntextual relevance, eg. as computed with BM25:\n - <<query-dsl-script-score-query,`script_score` query>>\n - <<query-dsl-rank-feature-query,`rank_feature` query>>\n\nFor instance imagine that you have a `pagerank` field that you wish to\ncombine with the BM25 score so that the final score is equal to\n`score = bm25_score + pagerank / (10 + pagerank)`.\n\nWith the <<query-dsl-script-score-query,`script_score` query>> the query would\nlook like this:\n\n//////////////////////////\n\n[source,console]\n--------------------------------------------------\nPUT index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"body\": {\n        \"type\": \"text\"\n      },\n      \"pagerank\": {\n        \"type\": \"long\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n//////////////////////////\n\n[source,console]\n--------------------------------------------------\nGET index/_search\n{\n  \"query\": {\n    \"script_score\": {\n      \"query\": {\n        \"match\": { \"body\": \"elasticsearch\" }\n      },\n      \"script\": {\n        \"source\": \"_score * saturation(doc['pagerank'].value, 10)\" <1>\n      }\n    }\n  }\n}\n--------------------------------------------------\n//TEST[continued]\n\n<1> `pagerank` must be mapped as a <<number>>\n\nwhile with the <<query-dsl-rank-feature-query,`rank_feature` query>> it would\nlook like below:\n\n//////////////////////////\n\n[source,console]\n--------------------------------------------------\nPUT index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"body\": {\n        \"type\": \"text\"\n      },\n      \"pagerank\": {\n        \"type\": \"rank_feature\"\n      }\n    }\n  }\n}\n--------------------------------------------------\n// TEST\n\n//////////////////////////\n\n[source,console]\n--------------------------------------------------\nGET _search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": {\n        \"match\": { \"body\": \"elasticsearch\" }\n      },\n      \"should\": {\n        \"rank_feature\": {\n          \"field\": \"pagerank\", <1>\n          \"saturation\": {\n            \"pivot\": 10\n          }\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n<1> `pagerank` must be mapped as a <<rank-feature,`rank_feature`>> field\n\nWhile both options would return similar scores, there are trade-offs:\n<<query-dsl-script-score-query,script_score>> provides a lot of flexibility,\nenabling you to combine the text relevance score with static signals as you\nprefer. On the other hand, the <<rank-feature,`rank_feature` query>> only\nexposes a couple ways to incorporate static signals into the score. However,\nit relies on the <<rank-feature,`rank_feature`>> and\n<<rank-features,`rank_features`>> fields, which index values in a special way\nthat allows the <<query-dsl-rank-feature-query,`rank_feature` query>> to skip\nover non-competitive documents and get the top matches of a query faster.\n"
}