{
    "meta": {
        "timestamp": "2024-11-01T03:02:53.433579",
        "size": 17294,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-pipeline-search.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "ingest-pipeline-search",
        "version": "8.15"
    },
    "doc": "[[ingest-pipeline-search]]\n== Ingest pipelines in Search\n\nYou can manage ingest pipelines through Elasticsearch APIs or Kibana UIs.\n\nThe *Content* UI under *Search* has a set of tools for creating and managing indices optimized for search use cases (non time series data).\nYou can also manage your ingest pipelines in this UI.\n\n[discrete]\n[[ingest-pipeline-search-where]]\n=== Find pipelines in Content UI\n\nTo work with ingest pipelines using these UI tools, you'll be using the *Pipelines* tab on your search-optimized Elasticsearch index.\n\nTo find this tab in the Kibana UI:\n\n1. Go to *Search > Content > Elasticsearch indices*.\n2. Select the index you want to work with. For example, `search-my-index`.\n3. On the index's overview page, open the *Pipelines* tab.\n4. From here, you can follow the instructions to create custom pipelines, and set up ML inference pipelines.\n\nThe tab is highlighted in this screenshot:\n\n[.screenshot]\nimage::images/ingest/ingest-pipeline-ent-search-ui.png[align=\"center\"]\n\n[discrete#ingest-pipeline-search-in-enterprise-search]\n=== Overview\n\nThese tools can be particularly helpful by providing a layer of customization and post-processing of documents.\nFor example:\n\n* providing consistent extraction of text from binary data types\n* ensuring consistent formatting\n* providing consistent sanitization steps (removing PII like phone numbers or SSN's)\n\nIt can be a lot of work to set up and manage production-ready pipelines from scratch.\nConsiderations such as error handling, conditional execution, sequencing, versioning, and modularization must all be taken into account.\n\nTo this end, when you create indices for search use cases, (including {enterprise-search-ref}/crawler.html[Elastic web crawler], <<es-connectors,connectors>>.\n, and API indices), each index already has a pipeline set up with several processors that optimize your content for search.\n\nThis pipeline is called `ent-search-generic-ingestion`.\nWhile it is a \"managed\" pipeline (meaning it should not be tampered with), you can view its details via the Kibana UI or the Elasticsearch API.\nYou can also <<ingest-pipeline-search-details-generic-reference,read more about its contents below>>.\n\nYou can control whether you run some of these processors.\nWhile all features are enabled by default, they are eligible for opt-out.\nFor {enterprise-search-ref}/crawler.html[Elastic crawler] and <<es-connectors,connectors>>.\n, you can opt out (or back in) per index, and your choices are saved.\nFor API indices, you can opt out (or back in) by including specific fields in your documents.\n<<ingest-pipeline-search-pipeline-settings-using-the-api,See below for details>>.\n\nAt the deployment level, you can change the default settings for all new indices.\nThis will not effect existing indices.\n\nEach index also provides the capability to easily create index-specific ingest pipelines with customizable processing.\nIf you need that extra flexibility, you can create a custom pipeline by going to your pipeline settings and choosing to \"copy and customize\".\nThis will replace the index's use of `ent-search-generic-ingestion` with 3 newly generated pipelines:\n\n1. `<index-name>`\n2. `<index-name>@custom`\n3. `<index-name>@ml-inference`\n\nLike `ent-search-generic-ingestion`, the first of these is \"managed\", but the other two can and should be modified to fit your needs.\nYou can view these pipelines using the platform tools (Kibana UI, Elasticsearch API), and can also \n<<ingest-pipeline-search-details-specific,read more about their content below>>.\n\n[discrete#ingest-pipeline-search-pipeline-settings]\n=== Pipeline Settings\n\nAside from the pipeline itself, you have a few configuration options which control individual features of the pipelines.\n\n* **Extract Binary Content** - This controls whether or not binary documents should be processed and any textual content should be extracted.\n* **Reduce Whitespace** - This controls whether or not consecutive, leading, and trailing whitespaces should be removed.\n  This can help to display more content in some search experiences.\n* **Run ML Inference** - Only available on index-specific pipelines.\n  This controls whether or not the optional `<index-name>@ml-inference` pipeline will be run.\n  Enabled by default.\n\nFor Elastic web crawler and connectors, you can opt in or out per index.\nThese settings are stored in Elasticsearch in the `.elastic-connectors` index, in the document that corresponds to the specific index.\nThese settings can be changed there directly, or through the Kibana UI at *Search > Content > Indices > <your index> > Pipelines > Settings*.\n\nYou can also change the deployment wide defaults.\nThese settings are stored in the Elasticsearch mapping for `.elastic-connectors` in the `_meta` section.\nThese settings can be changed there directly, or from the Kibana UI at *Search > Content > Settings* tab.\nChanging the deployment wide defaults will not impact any existing indices, but will only impact any newly created indices defaults.\nThose defaults will still be able to be overriden by the index-specific settings.\n\n[discrete#ingest-pipeline-search-pipeline-settings-using-the-api]\n==== Using the API\n\nThese settings are not persisted for indices that \"Use the API\".\nInstead, changing these settings will, in real time, change the example cURL request displayed.\nNotice that the example document in the cURL request contains three underscore-prefixed fields:\n\n[source,js]\n----\n{\n  ...\n  \"_extract_binary_content\": true,\n  \"_reduce_whitespace\": true,\n  \"_run_ml_inference\": true\n}\n----\n// NOTCONSOLE\n\nOmitting one of these special fields is the same as specifying it with the value `false`.\n\n[NOTE]\n=========================\nYou must also specify the pipeline in your indexing request.\nThis is also shown in the example cURL request.\n=========================\n\n[WARNING]\n=========================\nIf the pipeline is not specified, the underscore-prefixed fields will actually be indexed, and will not impact any processing behaviors.\n=========================\n\n[discrete#ingest-pipeline-search-details]\n=== Details\n\n[discrete#ingest-pipeline-search-details-generic-reference]\n==== `ent-search-generic-ingestion` Reference\n\nYou can access this pipeline with the <<get-pipeline-api, Elasticsearch Ingest Pipelines API>> or via Kibana's <<create-manage-ingest-pipelines,Stack Management > Ingest Pipelines>> UI.\n\n[WARNING]\n=========================\nThis pipeline is a \"managed\" pipeline.\nThat means that it is not intended to be edited.\nEditing/updating this pipeline manually could result in unintended behaviors, or difficulty in upgrading in the future.\nIf you want to make customizations, we recommend you utilize index-specific pipelines (see below), specifically <<ingest-pipeline-search-details-specific-custom-reference,the `<index-name>@custom` pipeline>>.\n=========================\n\n[discrete#ingest-pipeline-search-details-generic-reference-processors]\n===== Processors\n\n1. `attachment` - this uses the <<attachment, Attachment>> processor to convert any binary data stored in a document's `_attachment` field to a nested object of plain text and metadata.\n2. `set_body` - this uses the <<set-processor, Set>> processor to copy any plain text extracted from the previous step and persist it on the document in the `body` field.\n3. `remove_replacement_chars` - this uses the <<gsub-processor, Gsub>> processor to remove characters like \"\ufffd\" from the `body` field.\n4. `remove_extra_whitespace` - this uses the <<gsub-processor, Gsub>> processor to replace consecutive whitespace characters with single spaces in the `body` field.\n  While not perfect for every use case (see below for how to disable), this can ensure that search experiences display more content and highlighting and less empty space for your search results.\n5. `trim` - this uses the <<trim-processor, Trim>> processor to remove any remaining leading or trailing whitespace from the `body` field.\n6. `remove_meta_fields` - this final step of the pipeline uses the <<remove-processor, Remove>> processor to remove special fields that may have been used elsewhere in the pipeline, whether as temporary storage or as control flow parameters.\n\n[discrete#ingest-pipeline-search-details-generic-reference-params]\n===== Control flow parameters\n\nThe `ent-search-generic-ingestion` pipeline does not always run all processors.\nIt utilizes a feature of ingest pipelines to <<conditionally-run-processor,conditionally run processors>> based on the contents of each individual document.\n\n* `_extract_binary_content` - if this field is present and has a value of `true` on a source document, the pipeline will attempt to run the `attachment`, `set_body`, and `remove_replacement_chars` processors.\n  Note that the document will also need an `_attachment` field populated with base64-encoded binary data in order for the `attachment` processor to have any output.\n  If the `_extract_binary_content` field is missing or `false` on a source document, these processors will be skipped.\n* `_reduce_whitespace` - if this field is present and has a value of `true` on a source document, the pipeline will attempt to run the `remove_extra_whitespace` and `trim` processors.\n  These processors only apply to the `body` field.\n  If the `_reduce_whitespace` field is missing or `false` on a source document, these processors will be skipped.\n\nCrawler, Native Connectors, and Connector Clients will automatically add these control flow parameters based on the settings in the index's Pipeline tab.\nTo control what settings any new indices will have upon creation, see the deployment wide content settings.\nSee <<ingest-pipeline-search-pipeline-settings>>.\n\n[discrete#ingest-pipeline-search-details-specific]\n==== Index-specific ingest pipelines\n\nIn the Kibana UI for your index, by clicking on the Pipelines tab, then *Settings > Copy and customize*, you can quickly generate 3 pipelines which are specific to your index.\nThese 3 pipelines replace `ent-search-generic-ingestion` for the index.\nThere is nothing lost in this action, as the `<index-name>` pipeline is a superset of functionality over the `ent-search-generic-ingestion` pipeline.\n\n[IMPORTANT]\n====\nThe \"copy and customize\" button is not available at all Elastic subscription levels.\nRefer to the Elastic subscriptions pages for https://www.elastic.co/subscriptions/cloud[Elastic Cloud^] and https://www.elastic.co/subscriptions[self-managed] deployments.\n====\n\n[discrete#ingest-pipeline-search-details-specific-reference]\n===== `<index-name>` Reference\n\nThis pipeline looks and behaves a lot like the <<ingest-pipeline-search-details-generic-reference,`ent-search-generic-ingestion` pipeline>>, but with <<ingest-pipeline-search-details-specific-reference-processors,two additional processors>>.\n\n[WARNING]\n=========================\nYou should not rename this pipeline.\n=========================\n\n[WARNING]\n=========================\nThis pipeline is a \"managed\" pipeline.\nThat means that it is not intended to be edited.\nEditing/updating this pipeline manually could result in unintended behaviors, or difficulty in upgrading in the future.\nIf you want to make customizations, we recommend you utilize <<ingest-pipeline-search-details-specific-custom-reference,the `<index-name>@custom` pipeline>>.\n=========================\n\n[discrete#ingest-pipeline-search-details-specific-reference-processors]\n====== Processors\n\nIn addition to the processors inherited from the <<ingest-pipeline-search-details-generic-reference,`ent-search-generic-ingestion` pipeline>>, the index-specific pipeline also defines:\n\n* `index_ml_inference_pipeline` - this uses the <<pipeline-processor, Pipeline>> processor to run the `<index-name>@ml-inference` pipeline.\n  This processor will only be run if the source document includes a `_run_ml_inference` field with the value `true`.\n* `index_custom_pipeline` - this uses the <<pipeline-processor, Pipeline>> processor to run the `<index-name>@custom` pipeline.\n\n[discrete#ingest-pipeline-search-details-specific-reference-params]\n====== Control flow parameters\n\nLike the `ent-search-generic-ingestion` pipeline, the `<index-name>` pipeline does not always run all processors.\nIn addition to the `_extract_binary_content` and `_reduce_whitespace` control flow parameters, the `<index-name>` pipeline also supports:\n\n* `_run_ml_inference` - if this field is present and has a value of `true` on a source document, the pipeline will attempt to run the `index_ml_inference_pipeline` processor.\n  If the `_run_ml_inference` field is missing or `false` on a source document, this processor will be skipped.\n\nCrawler, Native Connectors, and Connector Clients will automatically add these control flow parameters based on the settings in the index's Pipeline tab.\nTo control what settings any new indices will have upon creation, see the deployment wide content settings.\nSee <<ingest-pipeline-search-pipeline-settings>>.\n\n[discrete#ingest-pipeline-search-details-specific-ml-reference]\n===== `<index-name>@ml-inference` Reference\n\nThis pipeline is empty to start (no processors), but can be added to via the Kibana UI either through the Pipelines tab of your index, or from the *Stack Management > Ingest Pipelines* page.\nUnlike the `ent-search-generic-ingestion` pipeline and the `<index-name>` pipeline, this pipeline is NOT \"managed\".\n\nIt's possible to add one or more ML inference pipelines to an index in the *Content* UI.\nThis pipeline will serve as a container for all of the ML inference pipelines configured for the index.\nEach ML inference pipeline added to the index is referenced within `<index-name>@ml-inference` using a `pipeline` processor.\n\n[WARNING]\n=========================\nYou should not rename this pipeline.\n=========================\n\n[NOTE]\n=========================\nThe `monitor_ml` Elasticsearch cluster permission is required in order to manage ML models and ML inference pipelines which use those models.\n=========================\n\n[discrete#ingest-pipeline-search-details-specific-custom-reference]\n===== `<index-name>@custom` Reference\n\nThis pipeline is empty to start (no processors), but can be added to via the Kibana UI either through the Pipelines\ntab of your index, or from the *Stack Management > Ingest Pipelines* page.\nUnlike the `ent-search-generic-ingestion` pipeline and the `<index-name>` pipeline, this pipeline is NOT \"managed\".\n\nYou are encouraged to make additions and edits to this pipeline, provided its name remains the same.\nThis provides a convenient hook from which to add custom processing and transformations for your data.\nBe sure to read the <<ingest, docs for ingest pipelines>> to see what options are available.\n\n[WARNING]\n=========================\nYou should not rename this pipeline.\n=========================\n\n[discrete#ingest-pipeline-search-upgrading-notes]\n=== Upgrading notes\n\n.Expand to see upgrading notes\n[%collapsible%closed]\n=============\n\n* `app_search_crawler` - Since 8.3, {app-search-crawler} has utilized this pipeline to power its binary content\nextraction.\n  You can read more about this pipeline and its usage in the {app-search-ref}/web-crawler-reference.html#web-crawler-reference-binary-content-extraction[App Search Guide].\n  When upgrading from 8.3 to 8.5+, be sure to note any changes that you made to the `app_search_crawler` pipeline.\n  These changes should be re-applied to each index's `<index-name>@custom` pipeline in order to ensure a consistent data processing experience.\n  In 8.5+, the <<ingest-pipeline-search-pipeline-settings, index setting to enable binary content>> is required *in addition* to the configurations mentioned in the {app-search-ref}/web-crawler-reference.html#web-crawler-reference-binary-content-extraction[App Search Guide].\n\n* `ent_search_crawler` - Since 8.4, the Elastic web crawler has utilized this pipeline to power its binary content extraction.\n  You can read more about this pipeline and its usage in the {enterprise-search-ref}/crawler-managing.html#crawler-managing-binary-content[Elastic web crawler Guide].\n  When upgrading from 8.4 to 8.5+, be sure to note any changes that you made to the `ent_search_crawler` pipeline.\nThese changes should be re-applied to each index's `<index-name>@custom` pipeline in order to ensure a consistent data processing experience.\n  In 8.5+, the <<ingest-pipeline-search-pipeline-settings, index setting to enable binary content>> is required *in addition* to the configurations mentioned in the {enterprise-search-ref}/crawler-managing.html#crawler-managing-binary-content[Elastic web crawler Guide].\n\n* `ent-search-generic-ingestion` - Since 8.5, Native Connectors, Connector Clients, and new (>8.4) Elastic web crawler indices will all make use of this pipeline by default.\n  You can <<ingest-pipeline-search-details-generic-reference, read more about this pipeline>> above.\n  As this pipeline is \"managed\", any modifications that were made to `app_search_crawler` and/or `ent_search_crawler` should NOT be made to `ent-search-generic-ingestion`.\n  Instead, if such customizations are desired, you should utilize <<ingest-pipeline-search-details-specific>>, placing all modifications in the `<index-name>@custom` pipeline(s).\n=============\n\ninclude::search-inference-processing.asciidoc[]\ninclude::search-nlp-tutorial.asciidoc[]\n"
}