{
    "meta": {
        "timestamp": "2024-11-01T03:07:09.170273",
        "size": 17302,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/esql-cross-clusters.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "esql-cross-clusters",
        "version": "8.15"
    },
    "doc": "[[esql-cross-clusters]]\n=== Using {esql} across clusters\n++++\n<titleabbrev>Using {esql} across clusters</titleabbrev>\n++++\n\n[partintro]\n\npreview::[\"{ccs-cap} for {esql} is in technical preview and may be changed or removed in a future release. Elastic will work to fix any issues, but features in technical preview are not subject to the support SLA of official GA features.\"]\n\n[NOTE]\n====\nFor {ccs-cap} with {esql} on version 8.16 or later, remote clusters must also be on version 8.16 or later.\n====\n\nWith {esql}, you can execute a single query across multiple clusters.\n\n[discrete]\n[[esql-ccs-prerequisites]]\n==== Prerequisites\n\ninclude::{es-ref-dir}/search/search-your-data/search-across-clusters.asciidoc[tag=ccs-prereqs]\n\ninclude::{es-ref-dir}/search/search-your-data/search-across-clusters.asciidoc[tag=ccs-gateway-seed-nodes]\n\ninclude::{es-ref-dir}/search/search-your-data/search-across-clusters.asciidoc[tag=ccs-proxy-mode]\n\n[discrete]\n[[esql-ccs-security-model]]\n==== Security model\n\n{es} supports two security models for cross-cluster search (CCS):\n\n* <<esql-ccs-security-model-certificate, TLS certificate authentication>>\n* <<esql-ccs-security-model-api-key, API key authentication>>\n\n[TIP]\n====\nTo check which security model is being used to connect your clusters, run `GET _remote/info`.\nIf you're using the API key authentication method, you'll see the `\"cluster_credentials\"` key in the response.\n====\n\n[discrete]\n[[esql-ccs-security-model-certificate]]\n===== TLS certificate authentication\n\nTLS certificate authentication secures remote clusters with mutual TLS.\nThis could be the preferred model when a single administrator has full control over both clusters.\nWe generally recommend that roles and their privileges be identical in both clusters.\n\nRefer to <<remote-clusters-cert, TLS certificate authentication>> for prerequisites and detailed setup instructions.\n\n[discrete]\n[[esql-ccs-security-model-api-key]]\n===== API key authentication\n\nThe following information pertains to using {esql} across clusters with the <<remote-clusters-api-key, *API key based security model*>>. You'll need to follow the steps on that page for the *full setup instructions*. This page only contains additional information specific to {esql}.\n\nAPI key based cross-cluster search (CCS) enables more granular control over allowed actions between clusters.\nThis may be the preferred model when you have different administrators for different clusters and want more control over who can access what data. In this model, cluster administrators must explicitly define the access given to clusters and users.\n\nYou will need to:\n\n* Create an API key on the *remote cluster* using the <<security-api-create-cross-cluster-api-key,Create cross-cluster API key>> API or using the {kibana-ref}/api-keys.html[Kibana API keys UI].\n* Add the API key to the keystore on the *local cluster*, as part of the steps in <<remote-clusters-security-api-key-local-actions,configuring the local cluster>>. All cross-cluster requests from the local cluster are bound by the API key\u2019s privileges.\n\nUsing {esql} with the API key based security model requires some additional permissions that may not be needed when using the traditional query DSL based search.\nThe following example API call creates a role that can query remote indices using {esql} when using the API key based security model.\nThe final privilege, `remote_cluster`, is required to allow remote enrich operations.\n\n[source,console]\n----\nPOST /_security/role/remote1\n{\n  \"cluster\": [\"cross_cluster_search\"], <1>\n  \"indices\": [\n    {\n      \"names\" : [\"\"], <2>\n      \"privileges\": [\"read\"]\n    }\n  ],\n  \"remote_indices\": [ <3>\n    {\n      \"names\": [ \"logs-*\" ],\n      \"privileges\": [ \"read\",\"read_cross_cluster\" ], <4>\n      \"clusters\" : [\"my_remote_cluster\"] <5>\n    }\n  ],\n   \"remote_cluster\": [ <6>\n        {\n            \"privileges\": [\n                \"monitor_enrich\"\n            ],\n            \"clusters\": [\n                \"my_remote_cluster\"\n            ]\n        }\n    ]\n}\n----\n\n<1> The `cross_cluster_search` cluster privilege is required for the _local_ cluster.\n<2> Typically, users will have permissions to read both local and remote indices. However, for cases where the role\nis intended to ONLY search the remote cluster, the `read` permission is still required for the local cluster.\nTo provide read access to the local cluster, but disallow reading any indices in the local cluster, the `names`\nfield may be an empty string.\n<3> The indices allowed read access to the remote cluster. The configured\n<<security-api-create-cross-cluster-api-key,cross-cluster API key>> must also allow this index to be read.\n<4> The `read_cross_cluster` privilege is always required when using {esql} across clusters with the API key based\nsecurity model.\n<5> The remote clusters to which these privileges apply.\nThis remote cluster must be configured with a <<security-api-create-cross-cluster-api-key,cross-cluster API key>>\nand connected to the remote cluster before the remote index can be queried.\nVerify connection using the <<cluster-remote-info, Remote cluster info>> API.\n<6> Required to allow remote enrichment. Without this, the user cannot read from the `.enrich` indices on the\nremote cluster. The `remote_cluster` security privilege was introduced in version *8.15.0*.\n\nYou will then need a user or API key with the permissions you created above. The following example API call creates\na user with the `remote1` role.\n\n[source,console]\n----\nPOST /_security/user/remote_user\n{\n  \"password\" : \"<PASSWORD>\",\n  \"roles\" : [ \"remote1\" ]\n}\n----\n\nRemember that all cross-cluster requests from the local cluster are bound by the cross cluster API key\u2019s privileges,\nwhich are controlled by the remote cluster's administrator.\n\n[TIP]\n====\nCross cluster API keys created in versions prior to 8.15.0 will need to replaced or updated to add the new permissions\nrequired for {esql} with ENRICH.\n====\n\n[discrete]\n[[ccq-remote-cluster-setup]]\n==== Remote cluster setup\n\nOnce the security model is configured, you can add remote clusters.\n\ninclude::{es-ref-dir}/search/search-your-data/search-across-clusters.asciidoc[tag=ccs-remote-cluster-setup]\n\n<1> Since `skip_unavailable` was not set on `cluster_three`, it uses\nthe default of `false`. See the <<ccq-skip-unavailable-clusters>>\nsection for details.\n\n[discrete]\n[[ccq-from]]\n==== Query across multiple clusters\n\nIn the `FROM` command, specify data streams and indices on remote clusters\nusing the format `<remote_cluster_name>:<target>`. For instance, the following\n{esql} request queries the `my-index-000001` index on a single remote cluster\nnamed `cluster_one`:\n\n[source,esql]\n----\nFROM cluster_one:my-index-000001\n| LIMIT 10\n----\n\nSimilarly, this {esql} request queries the `my-index-000001` index from\nthree clusters:\n\n* The local (\"querying\") cluster\n* Two remote clusters, `cluster_one` and `cluster_two`\n\n[source,esql]\n----\nFROM my-index-000001,cluster_one:my-index-000001,cluster_two:my-index-000001\n| LIMIT 10\n----\n\nLikewise, this {esql} request queries the `my-index-000001` index from all\nremote clusters (`cluster_one`, `cluster_two`, and `cluster_three`):\n\n[source,esql]\n----\nFROM *:my-index-000001\n| LIMIT 10\n----\n\n[discrete]\n[[ccq-cluster-details]]\n==== Cross-cluster metadata\n\nUsing the `\"include_ccs_metadata\": true` option, users can request that\nES|QL {ccs} responses include metadata about the search on each cluster (when the response format is JSON).\nHere we show an example using the async search endpoint. {ccs-cap} metadata is also present in the synchronous\nsearch endpoint response when requested.\n\n[source,console]\n----\nPOST /_query/async?format=json\n{\n  \"query\": \"\"\"\n    FROM my-index-000001,cluster_one:my-index-000001,cluster_two:my-index*\n    | STATS COUNT(http.response.status_code) BY user.id\n    | LIMIT 2\n  \"\"\",\n  \"include_ccs_metadata\": true\n}\n----\n// TEST[setup:my_index]\n// TEST[s/cluster_one:my-index-000001,cluster_two:my-index//]\n\nWhich returns:\n\n[source,console-result]\n----\n{\n  \"is_running\": false,\n  \"took\": 42,  <1>\n  \"columns\" : [\n    {\n      \"name\" : \"COUNT(http.response.status_code)\",\n      \"type\" : \"long\"\n    },\n    {\n      \"name\" : \"user.id\",\n      \"type\" : \"keyword\"\n    }\n  ],\n  \"values\" : [\n    [4, \"elkbee\"],\n    [1, \"kimchy\"]\n  ],\n  \"_clusters\": {  <2>\n    \"total\": 3,\n    \"successful\": 3,\n    \"running\": 0,\n    \"skipped\": 0,\n    \"partial\": 0,\n    \"failed\": 0,\n    \"details\": { <3>\n      \"(local)\": { <4>\n        \"status\": \"successful\",\n        \"indices\": \"blogs\",\n        \"took\": 41,  <5>\n        \"_shards\": { <6>\n          \"total\": 13,\n          \"successful\": 13,\n          \"skipped\": 0,\n          \"failed\": 0\n        }\n      },\n      \"cluster_one\": {\n        \"status\": \"successful\",\n        \"indices\": \"cluster_one:my-index-000001\",\n        \"took\": 38,\n        \"_shards\": {\n          \"total\": 4,\n          \"successful\": 4,\n          \"skipped\": 0,\n          \"failed\": 0\n        }\n      },\n      \"cluster_two\": {\n        \"status\": \"successful\",\n        \"indices\": \"cluster_two:my-index*\",\n        \"took\": 40,\n        \"_shards\": {\n          \"total\": 18,\n          \"successful\": 18,\n          \"skipped\": 1,\n          \"failed\": 0\n        }\n      }\n    }\n  }\n}\n----\n// TEST[skip: cross-cluster testing env not set up]\n\n<1> How long the entire search (across all clusters) took, in milliseconds.\n<2> This section of counters shows all possible cluster search states and how many cluster\nsearches are currently in that state. The clusters can have one of the following statuses: *running*,\n*successful* (searches on all shards were successful), *skipped* (the search\nfailed on a cluster marked with `skip_unavailable`=`true`) or *failed* (the search\nfailed on a cluster marked with `skip_unavailable`=`false`).\n<3> The `_clusters/details` section shows metadata about the search on each cluster.\n<4> If you included indices from the local cluster you sent the request to in your {ccs},\nit is identified as \"(local)\".\n<5> How long (in milliseconds) the search took on each cluster. This can be useful to determine\nwhich clusters have slower response times than others.\n<6> The shard details for the search on that cluster, including a count of shards that were\nskipped due to the can-match phase results. Shards are skipped when they cannot have any matching data\nand therefore are not included in the full ES|QL query.\n\n\nThe cross-cluster metadata can be used to determine whether any data came back from a cluster.\nFor instance, in the query below, the wildcard expression for `cluster-two` did not resolve\nto a concrete index (or indices). The cluster is, therefore, marked as 'skipped' and the total\nnumber of shards searched is set to zero.\n\n[source,console]\n----\nPOST /_query/async?format=json\n{\n  \"query\": \"\"\"\n    FROM cluster_one:my-index*,cluster_two:logs*\n    | STATS COUNT(http.response.status_code) BY user.id\n    | LIMIT 2\n  \"\"\",\n  \"include_ccs_metadata\": true\n}\n----\n// TEST[continued]\n// TEST[s/cluster_one:my-index\\*,cluster_two:logs\\*/my-index-000001/]\n\nWhich returns:\n\n[source,console-result]\n----\n{\n  \"is_running\": false,\n  \"took\": 55,\n  \"columns\": [\n     ... // not shown\n  ],\n  \"values\": [\n     ... // not shown\n  ],\n  \"_clusters\": {\n    \"total\": 2,\n    \"successful\": 2,\n    \"running\": 0,\n    \"skipped\": 0,\n    \"partial\": 0,\n    \"failed\": 0,\n    \"details\": {\n      \"cluster_one\": {\n        \"status\": \"successful\",\n        \"indices\": \"cluster_one:my-index*\",\n        \"took\": 38,\n        \"_shards\": {\n          \"total\": 4,\n          \"successful\": 4,\n          \"skipped\": 0,\n          \"failed\": 0\n        }\n      },\n      \"cluster_two\": {\n        \"status\": \"skipped\", <1>\n        \"indices\": \"cluster_two:logs*\",\n        \"took\": 0,\n        \"_shards\": {\n          \"total\": 0, <2>\n          \"successful\": 0,\n          \"skipped\": 0,\n          \"failed\": 0\n        }\n      }\n    }\n  }\n}\n----\n// TEST[skip: cross-cluster testing env not set up]\n\n<1> This cluster is marked as 'skipped', since there were no matching indices on that cluster.\n<2> Indicates that no shards were searched (due to not having any matching indices).\n\n\n\n\n[discrete]\n[[ccq-enrich]]\n==== Enrich across clusters\n\nEnrich in {esql} across clusters operates similarly to <<esql-enrich,local enrich>>.\nIf the enrich policy and its enrich indices are consistent across all clusters, simply\nwrite the enrich command as you would without remote clusters. In this default mode,\n{esql} can execute the enrich command on either the local cluster or the remote\nclusters, aiming to minimize computation or inter-cluster data transfer. Ensuring that\nthe policy exists with consistent data on both the local cluster and the remote\nclusters is critical for ES|QL to produce a consistent query result.\n\n[TIP]\n====\nEnrich in {esql} across clusters using the API key based security model was introduced in version *8.15.0*.\nCross cluster API keys created in versions prior to 8.15.0 will need to replaced or updated to use the new required permissions.\nRefer to the example in the <<esql-ccs-security-model-api-key,API key authentication>> section.\n====\n\nIn the following example, the enrich with `hosts` policy can be executed on\neither the local cluster or the remote cluster `cluster_one`.\n\n[source,esql]\n----\nFROM my-index-000001,cluster_one:my-index-000001\n| ENRICH hosts ON ip\n| LIMIT 10\n----\n\nEnrich with an {esql} query against remote clusters only can also happen on\nthe local cluster. This means the below query requires the `hosts` enrich\npolicy to exist on the local cluster as well.\n\n[source,esql]\n----\nFROM cluster_one:my-index-000001,cluster_two:my-index-000001\n| LIMIT 10\n| ENRICH hosts ON ip\n----\n\n[discrete]\n[[esql-enrich-coordinator]]\n===== Enrich with coordinator mode\n\n{esql} provides the enrich `_coordinator` mode to force {esql} to execute the enrich\ncommand on the local cluster. This mode should be used when the enrich policy is\nnot available on the remote clusters or maintaining consistency of enrich indices\nacross clusters is challenging.\n\n[source,esql]\n----\nFROM my-index-000001,cluster_one:my-index-000001\n| ENRICH _coordinator:hosts ON ip\n| SORT host_name\n| LIMIT 10\n----\n\n[discrete]\n[IMPORTANT]\n====\nEnrich with the `_coordinator` mode usually increases inter-cluster data transfer and\nworkload on the local cluster.\n====\n\n[discrete]\n[[esql-enrich-remote]]\n===== Enrich with remote mode\n\n{esql} also provides the enrich `_remote` mode to force {esql} to execute the enrich\ncommand independently on each remote cluster where the target indices reside.\nThis mode is useful for managing different enrich data on each cluster, such as detailed\ninformation of hosts for each region where the target (main) indices contain\nlog events from these hosts.\n\nIn the below example, the `hosts` enrich policy is required to exist on all\nremote clusters: the `querying` cluster (as local indices are included),\nthe remote cluster `cluster_one`, and `cluster_two`.\n\n[source,esql]\n----\nFROM my-index-000001,cluster_one:my-index-000001,cluster_two:my-index-000001\n| ENRICH _remote:hosts ON ip\n| SORT host_name\n| LIMIT 10\n----\n\nA `_remote` enrich cannot be executed after a <<esql-stats-by,stats>>\ncommand. The following example would result in an error:\n\n[source,esql]\n----\nFROM my-index-000001,cluster_one:my-index-000001,cluster_two:my-index-000001\n| STATS COUNT(*) BY ip\n| ENRICH _remote:hosts ON ip\n| SORT host_name\n| LIMIT 10\n----\n\n[discrete]\n[[esql-multi-enrich]]\n===== Multiple enrich commands\n\nYou can include multiple enrich commands in the same query with different\nmodes. {esql} will attempt to execute them accordingly. For example, this\nquery performs two enriches, first with the `hosts` policy on any cluster\nand then with the `vendors` policy on the local cluster.\n\n[source,esql]\n----\nFROM my-index-000001,cluster_one:my-index-000001,cluster_two:my-index-000001\n| ENRICH hosts ON ip\n| ENRICH _coordinator:vendors ON os\n| LIMIT 10\n----\n\nA `_remote` enrich command can't be executed after a `_coordinator` enrich\ncommand. The following example would result in an error.\n\n[source,esql]\n----\nFROM my-index-000001,cluster_one:my-index-000001,cluster_two:my-index-000001\n| ENRICH _coordinator:hosts ON ip\n| ENRICH _remote:vendors ON os\n| LIMIT 10\n----\n\n[discrete]\n[[ccq-exclude]]\n==== Excluding clusters or indices from {esql} query\n\nTo exclude an entire cluster, prefix the cluster alias with a minus sign in\nthe `FROM` command, for example: `-my_cluster:*`:\n\n[source,esql]\n----\nFROM my-index-000001,cluster*:my-index-000001,-cluster_three:*\n| LIMIT 10\n----\n\nTo exclude a specific remote index, prefix the index with a minus sign in\nthe `FROM` command, such as `my_cluster:-my_index`:\n\n[source,esql]\n----\nFROM my-index-000001,cluster*:my-index-*,cluster_three:-my-index-000001\n| LIMIT 10\n----\n\n[discrete]\n[[ccq-skip-unavailable-clusters]]\n==== Optional remote clusters\n\n{ccs-cap} for {esql} currently does not respect the `skip_unavailable`\nsetting. As a result, if a remote cluster specified in the request is\nunavailable or failed, {ccs} for {esql} queries will fail regardless of the setting.\n\nWe are actively working to align the behavior of {ccs} for {esql} with other\n{ccs} APIs.\n\n[discrete]\n[[ccq-during-upgrade]]\n==== Query across clusters during an upgrade\n\ninclude::{es-ref-dir}/search/search-your-data/search-across-clusters.asciidoc[tag=ccs-during-upgrade]\n"
}