{
    "meta": {
        "timestamp": "2024-11-01T03:07:08.743284",
        "size": 16334,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-icu.html",
        "type": "documentation",
        "role": [],
        "has_code": true,
        "title": "analysis-icu",
        "version": "8.15"
    },
    "doc": "[[analysis-icu]]\n=== ICU analysis plugin\n\nThe ICU Analysis plugin integrates the Lucene ICU module into {es},\nadding extended Unicode support using the https://icu.unicode.org/[ICU]\nlibraries, including better analysis of Asian languages, Unicode\nnormalization, Unicode-aware case folding, collation support, and\ntransliteration.\n\n[IMPORTANT]\n.ICU analysis and backwards compatibility\n================================================\n\nFrom time to time, the ICU library receives updates such as adding new\ncharacters and emojis, and improving collation (sort) orders. These changes\nmay or may not affect search and sort orders, depending on which characters\nsets you are using.\n\nWhile we restrict ICU upgrades to major versions, you may find that an index\ncreated in the previous major version will need to be reindexed in order to\nreturn correct (and correctly ordered) results, and to take advantage of new\ncharacters.\n\n================================================\n\n:plugin_name: analysis-icu\ninclude::install_remove.asciidoc[]\n\n[[analysis-icu-analyzer]]\n==== ICU analyzer\n\nThe `icu_analyzer` analyzer performs basic normalization, tokenization and character folding, using the\n`icu_normalizer` char filter, `icu_tokenizer` and `icu_folding` token filter\n\nThe following parameters are accepted:\n\n[horizontal]\n\n`method`::\n\n    Normalization method. Accepts `nfkc`, `nfc` or `nfkc_cf` (default)\n\n`mode`::\n\n    Normalization mode. Accepts `compose` (default) or `decompose`.\n\n[[analysis-icu-normalization-charfilter]]\n==== ICU normalization character filter\n\nNormalizes characters as explained\nhttps://unicode-org.github.io/icu/userguide/transforms/normalization/[here].\nIt registers itself as the `icu_normalizer` character filter, which is\navailable to all indices without any further configuration. The type of\nnormalization can be specified with the `name` parameter, which accepts `nfc`,\n`nfkc`, and `nfkc_cf` (default). Set the `mode` parameter to `decompose` to\nconvert `nfc` to `nfd` or `nfkc` to `nfkd` respectively:\n\nWhich letters are normalized can be controlled by specifying the\n`unicode_set_filter` parameter, which accepts a\nhttps://icu-project.org/apiref/icu4j/com/ibm/icu/text/UnicodeSet.html[UnicodeSet].\n\nHere are two examples, the default usage and a customised character filter:\n\n\n[source,console]\n--------------------------------------------------\nPUT icu_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"nfkc_cf_normalized\": { <1>\n            \"tokenizer\": \"icu_tokenizer\",\n            \"char_filter\": [\n              \"icu_normalizer\"\n            ]\n          },\n          \"nfd_normalized\": { <2>\n            \"tokenizer\": \"icu_tokenizer\",\n            \"char_filter\": [\n              \"nfd_normalizer\"\n            ]\n          }\n        },\n        \"char_filter\": {\n          \"nfd_normalizer\": {\n            \"type\": \"icu_normalizer\",\n            \"name\": \"nfc\",\n            \"mode\": \"decompose\"\n          }\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n<1> Uses the default `nfkc_cf` normalization.\n<2> Uses the customized `nfd_normalizer` token filter, which is set to use `nfc` normalization with decomposition.\n\n[[analysis-icu-tokenizer]]\n==== ICU tokenizer\n\nTokenizes text into words on word boundaries, as defined in\nhttps://www.unicode.org/reports/tr29/[UAX #29: Unicode Text Segmentation].\nIt behaves much like the {ref}/analysis-standard-tokenizer.html[`standard` tokenizer],\nbut adds better support for some Asian languages by using a dictionary-based\napproach to identify words in Thai, Lao, Chinese, Japanese, and Korean, and\nusing custom rules to break Myanmar and Khmer text into syllables.\n\n[source,console]\n--------------------------------------------------\nPUT icu_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"my_icu_analyzer\": {\n            \"tokenizer\": \"icu_tokenizer\"\n          }\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n===== Rules customization\n\nexperimental[This functionality is marked as experimental in Lucene]\n\nYou can customize the `icu-tokenizer` behavior by specifying per-script rule files, see the\nhttp://userguide.icu-project.org/boundaryanalysis#TOC-RBBI-Rules[RBBI rules syntax reference]\nfor a more detailed explanation.\n\nTo add icu tokenizer rules, set the `rule_files` settings, which should contain a comma-separated list of\n`code:rulefile` pairs in the following format:\nhttps://unicode.org/iso15924/iso15924-codes.html[four-letter ISO 15924 script code],\nfollowed by a colon, then a rule file name. Rule files are placed `ES_HOME/config` directory.\n\nAs a demonstration of how the rule files can be used, save the following user file to `$ES_HOME/config/KeywordTokenizer.rbbi`:\n\n[source,text]\n-----------------------\n.+ {200};\n-----------------------\n\nThen create an analyzer to use this rule file as follows:\n\n[source,console]\n--------------------------------------------------\nPUT icu_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"tokenizer\": {\n          \"icu_user_file\": {\n            \"type\": \"icu_tokenizer\",\n            \"rule_files\": \"Latn:KeywordTokenizer.rbbi\"\n          }\n        },\n        \"analyzer\": {\n          \"my_analyzer\": {\n            \"type\": \"custom\",\n            \"tokenizer\": \"icu_user_file\"\n          }\n        }\n      }\n    }\n  }\n}\n\nGET icu_sample/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"Elasticsearch. Wow!\"\n}\n--------------------------------------------------\n\nThe above `analyze` request returns the following:\n\n[source,console-result]\n--------------------------------------------------\n{\n   \"tokens\": [\n      {\n         \"token\": \"Elasticsearch. Wow!\",\n         \"start_offset\": 0,\n         \"end_offset\": 19,\n         \"type\": \"<ALPHANUM>\",\n         \"position\": 0\n      }\n   ]\n}\n--------------------------------------------------\n\n\n[[analysis-icu-normalization]]\n==== ICU normalization token filter\n\nNormalizes characters as explained\nhttps://unicode-org.github.io/icu/userguide/transforms/normalization/[here]. It registers\nitself as the `icu_normalizer` token filter, which is available to all indices\nwithout any further configuration. The type of normalization can be specified\nwith the `name` parameter, which accepts `nfc`, `nfkc`, and `nfkc_cf`\n(default).\n\nWhich letters are normalized can be controlled by specifying the\n`unicode_set_filter` parameter, which accepts a\nhttps://icu-project.org/apiref/icu4j/com/ibm/icu/text/UnicodeSet.html[UnicodeSet].\n\nYou should probably prefer the <<analysis-icu-normalization-charfilter,Normalization character filter>>.\n\nHere are two examples, the default usage and a customised token filter:\n\n[source,console]\n--------------------------------------------------\nPUT icu_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"nfkc_cf_normalized\": { <1>\n            \"tokenizer\": \"icu_tokenizer\",\n            \"filter\": [\n              \"icu_normalizer\"\n            ]\n          },\n          \"nfc_normalized\": { <2>\n            \"tokenizer\": \"icu_tokenizer\",\n            \"filter\": [\n              \"nfc_normalizer\"\n            ]\n          }\n        },\n        \"filter\": {\n          \"nfc_normalizer\": {\n            \"type\": \"icu_normalizer\",\n            \"name\": \"nfc\"\n          }\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n<1> Uses the default `nfkc_cf` normalization.\n<2> Uses the customized `nfc_normalizer` token filter, which is set to use `nfc` normalization.\n\n\n[[analysis-icu-folding]]\n==== ICU folding token filter\n\nCase folding of Unicode characters based on `UTR#30`, like the\n{ref}/analysis-asciifolding-tokenfilter.html[ASCII-folding token filter]\non steroids. It registers itself as the `icu_folding` token filter and is\navailable to all indices:\n\n[source,console]\n--------------------------------------------------\nPUT icu_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"folded\": {\n            \"tokenizer\": \"icu_tokenizer\",\n            \"filter\": [\n              \"icu_folding\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\nThe ICU folding token filter already does Unicode normalization, so there is\nno need to use Normalize character or token filter as well.\n\nWhich letters are folded can be controlled by specifying the\n`unicode_set_filter` parameter, which accepts a\nhttps://icu-project.org/apiref/icu4j/com/ibm/icu/text/UnicodeSet.html[UnicodeSet].\n\nThe following example exempts Swedish characters from folding. It is important\nto note that both upper and lowercase forms should be specified, and that\nthese filtered character are not lowercased which is why we add the\n`lowercase` filter as well:\n\n[source,console]\n--------------------------------------------------\nPUT icu_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"swedish_analyzer\": {\n            \"tokenizer\": \"icu_tokenizer\",\n            \"filter\": [\n              \"swedish_folding\",\n              \"lowercase\"\n            ]\n          }\n        },\n        \"filter\": {\n          \"swedish_folding\": {\n            \"type\": \"icu_folding\",\n            \"unicode_set_filter\": \"[^\u00e5\u00e4\u00f6\u00c5\u00c4\u00d6]\"\n          }\n        }\n      }\n    }\n  }\n}\n--------------------------------------------------\n\n\n[[analysis-icu-collation]]\n==== ICU collation token filter\n\n[WARNING]\n======\nThis token filter has been deprecated since Lucene 5.0. Please use\n<<analysis-icu-collation-keyword-field, ICU Collation Keyword Field>>.\n======\n\n[[analysis-icu-collation-keyword-field]]\n==== ICU collation keyword field\n\nCollations are used for sorting documents in a language-specific word order.\nThe `icu_collation_keyword` field type is available to all indices and will encode\nthe terms directly as bytes in a doc values field and a single indexed token just\nlike a standard {ref}/keyword.html[Keyword Field].\n\nDefaults to using {defguide}/sorting-collations.html#uca[DUCET collation],\nwhich is a best-effort attempt at language-neutral sorting.\n\nBelow is an example of how to set up a field for sorting German names in\n``phonebook'' order:\n\n[source,console]\n--------------------------\nPUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {   <1>\n        \"type\": \"text\",\n        \"fields\": {\n          \"sort\": {  <2>\n            \"type\": \"icu_collation_keyword\",\n            \"index\": false,\n            \"language\": \"de\",\n            \"country\": \"DE\",\n            \"variant\": \"@collation=phonebook\"\n          }\n        }\n      }\n    }\n  }\n}\n\nGET /my-index-000001/_search <3>\n{\n  \"query\": {\n    \"match\": {\n      \"name\": \"Fritz\"\n    }\n  },\n  \"sort\": \"name.sort\"\n}\n\n--------------------------\n\n<1> The `name` field uses the `standard` analyzer, and so supports full text queries.\n<2> The `name.sort` field is an `icu_collation_keyword` field that will preserve the name as\n    a single token doc values, and applies the German ``phonebook'' order.\n<3> An example query which searches the `name` field and sorts on the `name.sort` field.\n\n===== Parameters for ICU collation keyword fields\n\nThe following parameters are accepted by `icu_collation_keyword` fields:\n\n[horizontal]\n\n`doc_values`::\n\n    Should the field be stored on disk in a column-stride fashion, so that it\n    can later be used for sorting, aggregations, or scripting? Accepts `true`\n    (default) or `false`.\n\n`index`::\n\n    Should the field be searchable? Accepts `true` (default) or `false`.\n\n`null_value`::\n\n    Accepts a string value which is substituted for any explicit `null`\n    values. Defaults to `null`, which means the field is treated as missing.\n\n{ref}/ignore-above.html[`ignore_above`]::\n\n    Strings longer than the `ignore_above` setting will be ignored.\n    Checking is performed on the original string before the collation.\n    The `ignore_above` setting can be updated on existing fields\n    using the {ref}/indices-put-mapping.html[PUT mapping API].\n    By default, there is no limit and all values will be indexed.\n\n`store`::\n\n    Whether the field value should be stored and retrievable separately from\n    the {ref}/mapping-source-field.html[`_source`] field. Accepts `true` or `false`\n    (default).\n\n`fields`::\n\n    Multi-fields allow the same string value to be indexed in multiple ways for\n    different purposes, such as one field for search and a multi-field for\n    sorting and aggregations.\n\n===== Collation options\n\n`strength`::\n\nThe strength property determines the minimum level of difference considered\nsignificant during comparison. Possible values are : `primary`, `secondary`,\n`tertiary`, `quaternary` or `identical`. See the\nhttps://icu-project.org/apiref/icu4j/com/ibm/icu/text/Collator.html[ICU Collation documentation]\nfor a more detailed explanation for each value. Defaults to `tertiary`\nunless otherwise specified in the collation.\n\n`decomposition`::\n\nPossible values: `no` (default, but collation-dependent) or `canonical`.\nSetting this decomposition property to `canonical` allows the Collator to\nhandle unnormalized text properly, producing the same results as if the text\nwere normalized. If `no` is set, it is the user's responsibility to ensure\nthat all text is already in the appropriate form before a comparison or before\ngetting a CollationKey. Adjusting decomposition mode allows the user to select\nbetween faster and more complete collation behavior. Since a great many of the\nworld's languages do not require text normalization, most locales set `no` as\nthe default decomposition mode.\n\nThe following options are expert only:\n\n`alternate`::\n\nPossible values: `shifted` or `non-ignorable`. Sets the alternate handling for\nstrength `quaternary` to be either shifted or non-ignorable. Which boils down\nto ignoring punctuation and whitespace.\n\n`case_level`::\n\nPossible values: `true` or `false` (default). Whether case level sorting is\nrequired. When strength is set to `primary` this will ignore accent\ndifferences.\n\n\n`case_first`::\n\nPossible values: `lower` or `upper`. Useful to control which case is sorted\nfirst when the case is not ignored for strength `tertiary`. The default depends on\nthe collation.\n\n`numeric`::\n\nPossible values: `true` or `false` (default) . Whether digits are sorted\naccording to their numeric representation. For example the value `egg-9` is\nsorted before the value `egg-21`.\n\n\n`variable_top`::\n\nSingle character or contraction. Controls what is variable for `alternate`.\n\n`hiragana_quaternary_mode`::\n\nPossible values: `true` or `false`. Distinguishing between Katakana and\nHiragana characters in `quaternary` strength.\n\n\n[[analysis-icu-transform]]\n==== ICU transform token filter\n\nTransforms are used to process Unicode text in many different ways, such as\ncase mapping, normalization, transliteration and bidirectional text handling.\n\nYou can define which transformation you want to apply with the `id` parameter\n(defaults to `Null`), and specify text direction with the `dir` parameter\nwhich accepts `forward` (default) for LTR and `reverse` for RTL. Custom\nrulesets are not yet supported.\n\nFor example:\n\n[source,console]\n--------------------------------------------------\nPUT icu_sample\n{\n  \"settings\": {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"latin\": {\n            \"tokenizer\": \"keyword\",\n            \"filter\": [\n              \"myLatinTransform\"\n            ]\n          }\n        },\n        \"filter\": {\n          \"myLatinTransform\": {\n            \"type\": \"icu_transform\",\n            \"id\": \"Any-Latin; NFD; [:Nonspacing Mark:] Remove; NFC\" <1>\n          }\n        }\n      }\n    }\n  }\n}\n\nGET icu_sample/_analyze\n{\n  \"analyzer\": \"latin\",\n  \"text\": \"\u4f60\u597d\" <2>\n}\n\nGET icu_sample/_analyze\n{\n  \"analyzer\": \"latin\",\n  \"text\": \"\u0437\u0434\u0440\u0430\u0432\u0441\u0442\u0432\u0443\u0439\u0442\u0435\" <3>\n}\n\nGET icu_sample/_analyze\n{\n  \"analyzer\": \"latin\",\n  \"text\": \"\u3053\u3093\u306b\u3061\u306f\" <4>\n}\n\n--------------------------------------------------\n\n<1> This transforms transliterates characters to Latin, and separates accents\n    from their base characters, removes the accents, and then puts the\n    remaining text into an unaccented form.\n\n<2> Returns `ni hao`.\n<3> Returns `zdravstvujte`.\n<4> Returns `kon'nichiha`.\n\nFor more documentation, Please see the https://unicode-org.github.io/icu/userguide/transforms/[user guide of ICU Transform].\n"
}