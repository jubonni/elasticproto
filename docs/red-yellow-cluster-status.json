{
    "meta": {
        "size": 8638,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/red-yellow-cluster-status.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "red-yellow-cluster-status",
        "version": "8.15"
    },
    "doc": "[[red-yellow-cluster-status]]\n=== Red or yellow cluster health status\n\nA red or yellow cluster health status indicates one or more shards are not assigned to\na node.\n\n* **Red health status**: The cluster has some unassigned primary shards, which\nmeans that some operations such as searches and indexing may fail.\n* **Yellow health status**: The cluster has no unassigned primary shards but some\nunassigned replica shards. This increases your risk of data loss and can degrade\ncluster performance.\n\nWhen your cluster has a red or yellow health status, it will continue to process\nsearches and indexing where possible, but may delay certain management and\ncleanup activities until the cluster returns to green health status. For instance,\nsome <<index-lifecycle-management,{ilm-init}>> actions require the index on which they\noperate to have a green health status.\n\nIn many cases, your cluster will recover to green health status automatically.\nIf the cluster doesn't automatically recover, then you must <<fix-red-yellow-cluster-status,manually address>>\nthe remaining problems so management and cleanup activities can proceed.\n\n[discrete]\n[[diagnose-cluster-status]]\n==== Diagnose your cluster status\n\n**Check your cluster status**\n\nUse the <<cluster-health,cluster health API>>.\n\n[source,console]\n----\nGET _cluster/health?filter_path=status,*_shards\n----\n\nA healthy cluster has a green `status` and zero `unassigned_shards`. A yellow\nstatus means only replicas are unassigned. A red status means one or\nmore primary shards are unassigned.\n\n**View unassigned shards**\n\nTo view unassigned shards, use the <<cat-shards,cat shards API>>.\n\n[source,console]\n----\nGET _cat/shards?v=true&h=index,shard,prirep,state,node,unassigned.reason&s=state\n----\n\nUnassigned shards have a `state` of `UNASSIGNED`. The `prirep` value is `p` for\nprimary shards and `r` for replicas.\n\nTo understand why an unassigned shard is not being assigned and what action\nyou must take to allow {es} to assign it, use the\n<<cluster-allocation-explain,cluster allocation explanation API>>.\n\n[source,console]\n----\nGET _cluster/allocation/explain?filter_path=index,node_allocation_decisions.node_name,node_allocation_decisions.deciders.*\n{\n  \"index\": \"my-index\",\n  \"shard\": 0,\n  \"primary\": false\n}\n----\n// TEST[s/^/PUT my-index\\n/]\n\n[discrete]\n[[fix-red-yellow-cluster-status]]\n==== Fix a red or yellow cluster status\n\nA shard can become unassigned for several reasons. The following tips outline the\nmost common causes and their solutions.\n\n[discrete]\n[[fix-cluster-status-reenable-allocation]]\n===== Re-enable shard allocation\n\nYou typically disable allocation during a <<restart-cluster,restart>> or other\ncluster maintenance. If you forgot to re-enable allocation afterward, {es} will\nbe unable to assign shards. To re-enable allocation, reset the\n`cluster.routing.allocation.enable` cluster setting.\n\n[source,console]\n----\nPUT _cluster/settings\n{\n  \"persistent\" : {\n    \"cluster.routing.allocation.enable\" : null\n  }\n}\n----\n\n[discrete]\n[[fix-cluster-status-recover-nodes]]\n===== Recover lost nodes\n\nShards often become unassigned when a data node leaves the cluster. This can\noccur for several reasons, ranging from connectivity issues to hardware failure.\nAfter you resolve the issue and recover the node, it will rejoin the cluster.\n{es} will then automatically allocate any unassigned shards.\n\nTo avoid wasting resources on temporary issues, {es} <<delayed-allocation,delays\nallocation>> by one minute by default. If you've recovered a node and don\u2019t want\nto wait for the delay period, you can call the <<cluster-reroute,cluster reroute\nAPI>> with no arguments to start the allocation process. The process runs\nasynchronously in the background.\n\n[source,console]\n----\nPOST _cluster/reroute\n----\n\n[discrete]\n[[fix-cluster-status-allocation-settings]]\n===== Fix allocation settings\n\nMisconfigured allocation settings can result in an unassigned primary shard.\nThese settings include:\n\n* <<shard-allocation-filtering,Shard allocation>> index settings\n* <<cluster-shard-allocation-filtering,Allocation filtering>> cluster settings\n* <<shard-allocation-awareness,Allocation awareness>> cluster settings\n\nTo review your allocation settings, use the <<indices-get-settings,get index\nsettings>> and <<cluster-get-settings,cluster get settings>> APIs.\n\n[source,console]\n----\nGET my-index/_settings?flat_settings=true&include_defaults=true\n\nGET _cluster/settings?flat_settings=true&include_defaults=true\n----\n// TEST[s/^/PUT my-index\\n/]\n\nYou can change the settings using the <<indices-update-settings,update index\nsettings>> and <<cluster-update-settings,cluster update settings>> APIs.\n\n[discrete]\n[[fix-cluster-status-allocation-replicas]]\n===== Allocate or reduce replicas\n\nTo protect against hardware failure, {es} will not assign a replica to the same\nnode as its primary shard. If no other data nodes are available to host the\nreplica, it remains unassigned. To fix this, you can:\n\n* Add a data node to the same tier to host the replica.\n\n* Change the `index.number_of_replicas` index setting to reduce the number of\nreplicas for each primary shard. We recommend keeping at least one replica per\nprimary.\n\n[source,console]\n----\nPUT _settings\n{\n  \"index.number_of_replicas\": 1\n}\n----\n// TEST[s/^/PUT my-index\\n/]\n\n\n[discrete]\n[[fix-cluster-status-disk-space]]\n===== Free up or increase disk space\n\n{es} uses a <<disk-based-shard-allocation,low disk watermark>> to ensure data\nnodes have enough disk space for incoming shards. By default, {es} does not\nallocate shards to nodes using more than 85% of disk space.\n\nTo check the current disk space of your nodes, use the <<cat-allocation,cat\nallocation API>>.\n\n[source,console]\n----\nGET _cat/allocation?v=true&h=node,shards,disk.*\n----\n\nIf your nodes are running low on disk space, you have a few options:\n\n* Upgrade your nodes to increase disk space.\n\n* Delete unneeded indices to free up space. If you use {ilm-init}, you can\nupdate your lifecycle policy to use <<ilm-searchable-snapshot,searchable\nsnapshots>> or add a delete phase. If you no longer need to search the data, you\ncan use a <<snapshot-restore,snapshot>> to store it off-cluster.\n\n* If you no longer write to an index, use the <<indices-forcemerge,force merge\nAPI>> or {ilm-init}'s <<ilm-forcemerge,force merge action>> to merge its\nsegments into larger ones.\n+\n[source,console]\n----\nPOST my-index/_forcemerge\n----\n// TEST[s/^/PUT my-index\\n/]\n\n* If an index is read-only, use the <<indices-shrink-index,shrink index API>> or\n{ilm-init}'s <<ilm-shrink,shrink action>> to reduce its primary shard count.\n+\n[source,console]\n----\nPOST my-index/_shrink/my-shrunken-index\n----\n// TEST[s/^/PUT my-index\\n{\"settings\":{\"index.number_of_shards\":2,\"blocks.write\":true}}\\n/]\n\n* If your node has a large disk capacity, you can increase the low disk\nwatermark or set it to an explicit byte value.\n+\n[source,console]\n----\nPUT _cluster/settings\n{\n  \"persistent\": {\n    \"cluster.routing.allocation.disk.watermark.low\": \"30gb\"\n  }\n}\n----\n// TEST[s/\"30gb\"/null/]\n\n[discrete]\n[[fix-cluster-status-jvm]]\n===== Reduce JVM memory pressure\n\nShard allocation requires JVM heap memory. High JVM memory pressure can trigger\n<<circuit-breaker,circuit breakers>> that stop allocation and leave shards\nunassigned. See <<high-jvm-memory-pressure>>.\n\n[discrete]\n[[fix-cluster-status-restore]]\n===== Recover data for a lost primary shard\n\nIf a node containing a primary shard is lost, {es} can typically replace it\nusing a replica on another node. If you can't recover the node and replicas\ndon't exist or are irrecoverable, <<cluster-allocation-explain,Allocation\nExplain>> will report `no_valid_shard_copy` and you'll need to do one of the following:\n\n* restore the missing data from <<snapshot-restore,snapshot>>\n* index the missing data from its original data source\n* accept data loss on the index-level by running <<indices-delete-index,Delete Index>>\n* accept data loss on the shard-level by executing <<cluster-reroute,Cluster Reroute>> allocate_stale_primary or allocate_empty_primary command with `accept_data_loss: true`\n+\nWARNING: Only use this option if node recovery is no longer possible. This\nprocess allocates an empty primary shard. If the node later rejoins the cluster,\n{es} will overwrite its primary shard with data from this newer empty shard,\nresulting in data loss.\n+\n[source,console]\n----\nPOST _cluster/reroute\n{\n  \"commands\": [\n    {\n      \"allocate_empty_primary\": {\n        \"index\": \"my-index\",\n        \"shard\": 0,\n        \"node\": \"my-node\",\n        \"accept_data_loss\": \"true\"\n      }\n    }\n  ]\n}\n----\n// TEST[s/^/PUT my-index\\n/]\n// TEST[catch:bad_request]\n"
}