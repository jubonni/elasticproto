{
    "meta": {
        "size": 6485,
        "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-random-sampler-aggregation.html",
        "type": "documentation",
        "role": [],
        "has_code": false,
        "title": "search-aggregations-random-sampler-aggregation",
        "version": "8.15"
    },
    "doc": "[[search-aggregations-random-sampler-aggregation]]\n=== Random sampler aggregation\n++++\n<titleabbrev>Random sampler</titleabbrev>\n++++\n\nexperimental::[]\n\nThe `random_sampler` aggregation is a single bucket aggregation that randomly\nincludes documents in the aggregated results. Sampling provides significant\nspeed improvement at the cost of accuracy.\n\nThe sampling is accomplished by providing a random subset of the entire set of\ndocuments in a shard. If a filter query is provided in the search request, that\nfilter is applied over the sampled subset. Consequently, if a filter is\nrestrictive, very few documents might match; therefore, the statistics might not\nbe as accurate.\n\nNOTE: This aggregation is not to be confused with the\n<<search-aggregations-bucket-sampler-aggregation,sampler aggregation>>. The\nsampler aggregation is not over all documents; rather, it samples the first `n`\ndocuments matched by the query.\n\n[source,console]\n----\nGET kibana_sample_data_ecommerce/_search?size=0&track_total_hits=false\n{\n  \"aggregations\": {\n    \"sampling\": {\n      \"random_sampler\": {\n        \"probability\": 0.1\n      },\n      \"aggs\": {\n        \"price_percentiles\": {\n          \"percentiles\": {\n            \"field\": \"taxful_total_price\"\n          }\n        }\n      }\n    }\n  }\n}\n----\n// TEST[setup:kibana_sample_data_ecommerce]\n\n[[random-sampler-top-level-params]]\n==== Top-level parameters for random_sampler\n\n`probability`::\n(Required, float) The probability that a document will be included in the\naggregated data. Must be greater than 0, less than `0.5`, or exactly `1`. The\nlower the probability, the fewer documents are matched.\n\n`seed`::\n(Optional, integer) The seed to generate the random sampling of documents. When\na seed is provided, the random subset of documents is the same between calls.\n\n[[random-sampler-inner-workings]]\n==== How does the sampling work?\n\nThe aggregation is a random sample of all the documents in the index. In other\nwords, the sampling is over the background set of documents. If a query is\nprovided, a document is returned if it is matched by the query and if the\ndocument is in the random sampling. The sampling is not done over the matched\ndocuments.\n\nConsider the set of documents `[1, 2, 3, 4, 5]`. Your query matches `[1, 3, 5]`\nand the randomly sampled set is `[2, 4, 5]`. In this case, the document returned\nwould be `[5]`.\n\nThis type of sampling provides almost linear improvement in query latency in relation to the amount\nby which sampling reduces the document set size:\n\nimage::images/aggregations/random-sampler-agg-graph.png[Graph of the median speedup by sampling factor,align=\"center\"]\n\nThis graph is typical of the speed up for the majority of aggregations for a test data set of 63 million documents. The exact constants will depend on the data set size and the number of shards, but the form of the relationship between speed up and probability holds widely. For certain aggregations, the speed up may not\nbe as dramatic. These aggregations have some constant overhead unrelated to the number of documents seen. Even for\nthose aggregations, the speed improvements can be significant.\n\nThe sample set is generated by skipping documents using a geometric distribution\n(`(1-p)^(k-1)*p`) with success probability being the provided `probability` (`p` in the distribution equation).\nThe values returned from the distribution indicate how many documents to skip in\nthe background. This is equivalent to selecting documents uniformly at random. It follows that the expected number of failures before a success is\n`(1-p)/p`. For example, with the `\"probability\": 0.01`, the expected number of failures (or\naverage number of documents skipped) would be `99` with a variance of `9900`.\nConsequently, if you had only 80 documents in your index or matched by your\nfilter, you would most likely receive no results.\n\nimage::images/aggregations/relative-error-vs-doc-count.png[Graph of the relative error by sampling probability and doc count,align=\"center\"]\n\nIn the above image `p` is the probability provided to the aggregation, and `n` is the number of documents matched by whatever\nquery is provided. You can see the impact of outliers on `sum` and `mean`, but when many documents are still matched at\nhigher sampling rates, the relative error is still low.\n\nNOTE: This represents the result of aggregations against a typical positively skewed APM data set which also has outliers in the upper tail. The linear dependence of the relative error on the sample size is found to hold widely, but the slope depends on the variation in the quantity being aggregated. As such, the variance in your own data may\n      cause relative error rates to increase or decrease at a different rate.\n[[random-sampler-consistency]]\n==== Random sampler consistency\n\nFor a given `probability` and `seed`, the random sampler aggregation is consistent when sampling unchanged data from the same shard.\nHowever, this is background random sampling if a particular document is included in the sampled set or not is dependent on current number of segments.\n\nMeaning, replica vs. primary shards could return different values as different particular documents are sampled.\n\nIf the shard changes in via doc addition, update, deletion, or segment merging, the particular documents sampled could change, and thus the resulting statistics could change.\n\nThe resulting statistics used from the random sampler aggregation are approximate and should be treated as such.\n\n[[random-sampler-special-cases]]\n==== Random sampling special cases\n\nAll counts returned by the random sampler aggregation are scaled to ease visualizations and calculations. For example,\nwhen randomly sampling a <<search-aggregations-bucket-datehistogram-aggregation, date histogram aggregation>> every\n`doc_count` value for every bucket is scaled by the inverse of the random_sampler `probability` value. So, if `doc_count`\nfor a bucket is `10,000` with `probability: 0.1`, the actual number of documents aggregated is `1,000`.\n\nAn exception to this is <<search-aggregations-metrics-cardinality-aggregation, cardinality aggregation>>. Unique item\ncounts are not suitable for automatic scaling. When interpreting the cardinality count, compare it\nto the number of sampled docs provided in the top level `doc_count` within the random_sampler aggregation. It gives\nyou an idea of unique values as a percentage of total values. It may not reflect, however, the exact number of unique values\nfor the given field.\n"
}